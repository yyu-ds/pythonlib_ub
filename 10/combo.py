# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:11:29 2019

@author: ub71894 (4e8e6d0b), CSG
"""

import glob
import os, sys, pandas as pd, numpy as np
dir_path = r'C:\Users\ub71894\Documents\Projects\GlobalCorporation'
os.chdir(os.path.join(dir_path,'src'))

#%% consolidate GCAR data
if 1:
    dt_list=[os.path.join(dir_path,r'Data\GCAR_BR_Approved',e) for e in os.listdir(os.path.join(dir_path,r'Data\GCAR_BR_Approved')) if e.find('.tsv')>0]
    dt0=pd.DataFrame()
    for e in dt_list:
        tmp=pd.read_csv(e, sep='\t', header=0,encoding = "ISO-8859-1")
        dt0=dt0.append(tmp,ignore_index=True)

dt0.to_pickle(os.path.join(dir_path,'Data\GCAR_2008-2019.pkl'))




#%%
model_names = [
 'Academic Corporation', 
 'Bank',
 'Corporation (Major Company)', 
 'Corporation (Other Company)',
 'Finance Company',
 'Fund', 
 'Health Care Company', 
 'Individual',
 'Life Insurance Company', 
 'Municipality',
 'Non-Life Insurance Company', 
 'Securities Company']


#%%
data = pd.read_pickle(r'..\Data\GCAR BR Approved All.pkl')
regex_str = '^(Current) (Financial Data #|Score #|Total Score)|^Scoring Name #|\
^Borrower (CIF|Name)$|^(Primary|Second|Third|Final).*Evaluation|\
^Application No.$|^Approval Date$|Borrower Type|Data From|Statement Date|Applying Office Name|\
^External Rating|Country$'
all_model_data = data.filter(regex=regex_str)
model_name_inuse = 'Corporation (Other Company)'
# 'Corporation (Major Company)'

#%%  non-filtered data
is_model = all_model_data['Borrower Type']==model_name_inuse
model_data_inuse = all_model_data[is_model]
print(model_data_inuse.shape)

model_data_inuse.to_pickle('model_date_inuse.pkl')
 
#%% filterd by CD
is_model_filtered = (all_model_data['Borrower Type']==model_name_inuse) & (all_model_data['Data From']=='CD')
model_data_filtered = all_model_data[is_model_filtered]
print(model_data_filtered.shape)

model_data_filtered['Approval year'] = pd.DatetimeIndex(model_data_filtered['Approval Date']).year
obs_count = model_data_filtered['Approval year'].value_counts().sort_index()

model_data_filtered.to_pickle('model_data_filtered.pkl')




#%% filter by replicated score
Corp_Others_ModelData = all_model_data[is_model_filtered]
isempty_cols = Corp_Others_ModelData.filter(regex='^Current Financial Data').isnull().all() 
# Corp_Others_ModelData[isempty_cols.index[~isempty_cols]].apply(lambda x:pd.to_numeric(x.str.replace('\,|\%',''), errors='coerce'))

factor_bins = \
{'Corporation (Other Company)':
    {

     '1. Capability for Debt Repayment - (1) Cash Flow Amount': {'bins': [-pd.np.Inf, 10*10**6, 30*10**6, 50*10**6, 70*10**6, 100*10**6, 300*10**6, 500*10**6, 700*10**6, 1000*10**6, pd.np.Inf],
                                                                 'labels': [2, 3, 4, 6, 8, 9, 11, 12, 13, 15]},
     '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA': {'bins': [-pd.np.Inf, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2, 8.2, 9.2, 10.2, pd.np.Inf],
                                                                                  'labels': [10, 8, 6, 4, 2, 0, -2, -4, -6, -8, -10]},#EBITA is negative then set it to -10
     '1. Capability for Debt Repayment - (3) EBITDA / Net Interest Payment': {'bins': [-pd.np.Inf, 0.85, 0.95, 2, 3, 4, 6, 8, pd.np.Inf],
                                                                              'labels': [0, 2, 4, 6, 8, 10, 11, 12]},
     '2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio': {'bins': [-pd.np.Inf, -80, -70, -60, -50, -40, -30, -20, -10, 0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, pd.np.Inf],
                                                                                  'labels': [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
     '2. Financial Condition - 2-1. Safety - (2) Gearing Ratio': {'bins': [-pd.np.Inf, 50, 75, 100, 150, 200, pd.np.Inf],
                                                                  'labels': [10, 8, 6, 4, 2, 0]},
     '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales': {'bins': [-pd.np.Inf, 1, 2, 3, 6, 8, 10, 12, 14, 16, 20, pd.np.Inf],
                                                                                    'labels': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
     '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets': {'bins': [-pd.np.Inf, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, pd.np.Inf],
                                                                                                             'labels': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]},
     '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth': {'bins': [-pd.np.Inf, -40, -30, -20, -10, -2.5, 0, 0.5, 5, 25, 35, pd.np.Inf],
                                                                                                                    'labels': [-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10]},
     '2. Financial Condition - 2-4. Size - (1) Net Worth': {'bins': [-pd.np.Inf, 50*10**6, 100*10**6, 150*10**6, 200*10**6, 350*10**6, 500*10**6, 750*10**6, 1000*10**6, 1500*10**6, 2000*10**6, pd.np.Inf],
                                                            'labels': [0, 1, 3, 4, 6, 7, 8, 10, 11, 13, 14]},
     '2. Financial Condition - 2-4. Size - (2) Sales': {'bins': [-pd.np.Inf, 100*10**6, 500*10**6, 1000*10**6, 3000*10**6, pd.np.Inf],
                                                        'labels': [0, 1, 2, 3, 4]}

    }
}



score_df = pd.DataFrame()
for idx in isempty_cols.index[~isempty_cols].str.replace('Current Financial Data #',''):
    factor_num='{}'.format(idx)
    scr_str=Corp_Others_ModelData['Scoring Name #'+factor_num].dropna().unique()[0]    
    print(scr_str)
    df=Corp_Others_ModelData['Current Financial Data #'+factor_num]    
    df_clean = pd.to_numeric(df.astype(str).apply(lambda x:x.replace('%','').replace(',','')), errors='coerce')
    
    is_right = True
    if scr_str in ['2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio',
                   '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales',
                   '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets',
                   '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth',
                   '2. Financial Condition - 2-4. Size - (1) Net Worth']:
        is_right = False
    
    binned_df=pd.cut(df_clean,
                     bins=factor_bins[model_names[3]][scr_str]['bins'], 
                     labels=factor_bins[model_names[3]][scr_str]['labels'], right=is_right)
    df_2 = pd.concat([Corp_Others_ModelData.filter(regex='^Borrower Name$|^Application No.$|^Approval Date'),
                      df,df_clean, binned_df, Corp_Others_ModelData['Current Score #'+factor_num]], axis=1)
#     display(df_2.head())
    bin_counts = binned_df.value_counts(dropna=False).sort_index()
#     display(bin_counts.to_frame())
    nan_diff = bin_counts[bin_counts.index[-1:]].values[0]-df.isna().value_counts()[True]
    if nan_diff>0:
        print('Please check Current Financial Data #{} as there are {} strings which are not NaNs:'.format(factor_num,nan_diff))
#         non_numbered_df = df[~df.isna()].astype(str).apply(lambda x:x.replace('%','').replace(',',''))   
#         print(non_numbered_df[pd.to_numeric(non_numbered_df, errors='coerce').isna()].value_counts(),'\n') 
  
    if scr_str == '1. Capability for Debt Repayment - (1) Cash Flow Amount':
        binned_df = binned_df.astype(float)
        binned_df[df_clean<0]=0
        binned_df = binned_df.astype('category')

    #Deal with negative EBITA for the factor - '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA'
    if scr_str == '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA':
        binned_df = binned_df.astype(float)
        binned_df[(df == 'Negative EBITDA')|(df == '#DIV/0!')]=-10
        binned_df = binned_df.astype('category')
#         print(df_2.iloc[[13781,12594]])        
#         print(df_2[(df == 'Negative EBITDA')|(df == '#DIV/0!')].shape)
#         print(df.isna().value_counts()) 

    if scr_str == '1. Capability for Debt Repayment - (3) EBITDA / Net Interest Payment':
        binned_df = binned_df.astype(float)
        binned_df[(df == 'EBITDA is 0 or less')|(df == 'Negative EBITDA')]=0
        binned_df[df=='#DIV/0!']=12
        binned_df = binned_df.astype('category')
        
    if scr_str in ['2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio',
                   '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets',
                   '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth']:
        binned_df = binned_df.astype(float)
        binned_df[df=='#DIV/0!']=0
        binned_df = binned_df.astype('category') #use this option right = False for binning

    if scr_str == '2. Financial Condition - 2-1. Safety - (2) Gearing Ratio':
        binned_df[(df=='#DIV/0!')|(df == 'Negative EQuity')]=0
        binned_df = binned_df.astype(float)
        binned_df[df_clean.le(0)]=0
        binned_df = binned_df.astype('category')
           
    if scr_str == '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales':
        binned_df[(df=='#DIV/0!')|(df == 'Negative Sales')]=0
    
    diff_score = binned_df.fillna(0).sub(Corp_Others_ModelData['Current Score #'+factor_num], fill_value=0.0)
#     print(df_2[(diff_score.abs()>0)].head(20))
    vCts = diff_score.value_counts(dropna=False).sort_index()
#     print(vCts)
    print('Number of mismatches:',vCts[(vCts.index!=0.0) & (~vCts.index.isna())].sum(),'\n')

    bin_counts = binned_df.value_counts(dropna=False).sort_index()
    nan_diff = bin_counts[bin_counts.index[-1:]].values[0]-df.isna().value_counts()[True]
#     print('Current Financial Data #{} after migtigating for the NaNs:{}\n'.format(factor_num,nan_diff))
    score_df=pd.concat([score_df, binned_df], axis=1)




score_mismatches = score_df.sum(axis=1).sub(Corp_Others_ModelData['Current Total Score'], fill_value=0.0).value_counts(dropna=False).sort_index()
print('Total matches out of {} are {} ({:.0%})'.format(score_df.shape[0],score_mismatches.loc[0.0],score_mismatches.loc[0.0]/score_df.shape[0]))



cols_need = [
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',
'Current Total Score',
'Primary Evaluation',
'Final Evaluation']

final_df = score_df.sum(axis=1).to_frame().join(Corp_Others_ModelData[cols_need])
final_df.rename({0:'Computed Total Score'}, axis='columns',inplace=True)
final_df['Score Difference'] = final_df['Computed Total Score'].fillna(0.0).eq(final_df['Current Total Score'].fillna(0.0))
final_df['Approval year'] = pd.DatetimeIndex(Corp_Others_ModelData['Approval Date']).year


final_df_same_score = final_df[final_df['Score Difference']]

final_df_same_score.to_pickle('final_df_same_score.pkl')

#%%

#Model Document
# bins=[-pd.np.Inf,15, 25, 30, 35, 40, 45, 55, 60, 65, 70, 75, 80, 85, pd.np.Inf]
# labels = [80, 70, 62, 61, 52, 51, 40, 33, 32, 31, 23, 22, 21, 10]
#Application
bins=[-pd.np.Inf, 15, 25, 30, 35, 40, 45, 55, 60, 65, 70, 75, 85, pd.np.Inf]
labels = [80, 70, 62, 61, 52, 51, 40, 33, 32, 31, 22, 21, 10]

final_df = final_df.join(pd.DataFrame({'Primary Evaluation Mapped':pd.cut(final_df['Computed Total Score'], bins=bins, labels=labels, right=True)}))

final_df['Primary Evaluation Difference'] = final_df['Primary Evaluation Mapped'].astype(float).fillna(0.0).eq(final_df['Primary Evaluation'].astype(float).fillna(0.0))
final_df_same_rating = final_df[(final_df['Score Difference']==True) & (final_df['Primary Evaluation Difference']==True)]


final_df_same_rating.to_pickle('final_df_same_rating.pkl')
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:11:29 2019

@author: ub71894 (4e8e6d0b), CSG
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


def  gcar_sfa(dat):
    nz_col = dat.columns[~dat.isnull().all()]
    dat = dat[nz_col]
    pl_x = [
    'Current Financial Data #1',
    'Current Financial Data #2',
    'Current Financial Data #3',
    'Current Financial Data #4',
    'Current Financial Data #5',
    'Current Financial Data #6',
    'Current Financial Data #7',
    'Current Financial Data #8',
    'Current Financial Data #9',
    'Current Financial Data #10',
    ]
    pl_y = [
    'Primary Evaluation',
    'Final Evaluation']   

    dat[pl_x] = dat[pl_x].astype(str).apply(lambda x:pd.to_numeric(x.replace('\\%|\\,','', regex=True), errors='coerce')) 
      

    #get the SomersD matrix 
    array_sd = np.zeros((len(pl_x), len(pl_y)))
    for i,x in enumerate(pl_x):
        for j,y in enumerate(pl_y):
            df = dat[[x,y]].dropna(how='any')
            print('x ='+x+'  y='+y+str(df.shape))
            array_sd[i,j] = np.abs(SomersD(df[y], df[x]))   

    df_sd = pd.DataFrame(array_sd)
    df_sd.index = pl_x
    df_sd.columns = pl_y  

    return(df_sd)

#%%
dat_filter1 = pd.read_pickle('model_data_filtered.pkl')
dat_filter2 = pd.read_pickle('final_df_same_score.pkl')
dat_filter3 = pd.read_pickle('final_df_same_rating.pkl')

a1 = gcar_sfa(dat_filter1)
a2 = gcar_sfa(dat_filter2)
a3 = gcar_sfa(dat_filter3)



#%%
raw = pd.read_pickle(r'..\Data\GCAR BR Approved All.pkl')
model_name_inuse = 'Corporation (Other Company)'
is_model = raw['Borrower Type']==model_name_inuse
dat = raw[is_model]

dat.dropna(subset=['Primary Evaluation'], inplace=True)
dat['Approval year'] = pd.DatetimeIndex(dat['Approval Date']).year
dat['Approvalyear'] = dat['Approval year']
for i in range(9):
    year = 2010+i
    temp = dat.query('Approvalyear=={}'.format(year))
    print('In {}, unique customer# : {}'.format(year, len(temp.drop_duplicates(subset=['Borrower Name']))))

dat_uni = dat.drop_duplicates(subset=['Borrower Name'])

obs_count = dat['Approval year'].value_counts().sort_index()
obs_count_uni = dat_uni['Approval year'].value_counts().sort_index()


#pl_todelete = raw.columns[raw.isna().all()].tolist()
#raw.drop(columns=pl_todelete, inplace=True)

pl_cols = [
'Applying Office Name',
'Borrower Country',
'Borrower Office Name',
'Credit Division',
'Group Core Office Name',
'Parent Office Name',
'Parent / Group Core Office Name',
'in the Americas',
]


for name in pl_cols:
    print(dat_uni[name].unique())


Unit
book1 = dat_uni['Borrower Office Name'].value_counts()

book2 = dat_uni['Parent / Group Core Office Name'].value_counts()

book3 = dat_uni['in the Americas'].value_counts()
with pd.ExcelWriter('book1.xlsx', engine='openpyxl', mode='a') as writer:
    book1.to_excel(writer, sheet_name='book1')
    book2.to_excel(writer, sheet_name='book2')
    book3.to_excel(writer, sheet_name='book3')# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:11:29 2019

@author: ub71894 (4e8e6d0b), CSG
"""

import os, sys, pandas as pd, numpy as np
import difflib
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


gdat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR BR Approved All.pkl')
gdat = gdat[gdat['Borrower Type']=='Corporation (Other Company)']
gdat_uni = gdat.drop_duplicates(subset=['Borrower Name'])


sp  = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\CNI\data\Compustat\compustat_data.pickle')
sp_uni = sp.drop_duplicates(subset=['CONM'], keep='last')
pl_name_base = sp_uni['CONM'].tolist()




#%% name match
data_gcar = pd.DataFrame()
data_gcar['name_gcar'] = gdat_uni['Borrower Name'].tolist()
data_gcar['diffmatchname_70'] = 'NAN'
data_gcar['diffmatchname_80'] = 'NAN'
data_gcar['diffmatchname_90'] = 'NAN'

for i in range(len(data_gcar)):
    temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.9)
    if temp:
        data_gcar.diffmatchname_90[i] = temp[0]
    else:
        temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.8)
        if temp:
            data_gcar.diffmatchname_80[i] = temp[0]
        else:
            temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.7)
            if temp:
                data_gcar.diffmatchname_70[i] = temp[0]    


data_gcar.to_excel(r'output\03\data_gcar.xlsx')





#%% after manual 
#############GCAR
data_gcar = pd.read_excel(r'output\data_gcar.xlsx')
data_gcar = data_gcar.query('correct==1')

pl_nameinRA = []
for i, row in data_gcar.iterrows():
    if row.loc['diffmatchname_90'] != "NAN":
        pl_nameinRA.append(row.loc['diffmatchname_90'])
        continue
    elif row.loc['diffmatchname_80'] != "NAN":
        pl_nameinRA.append(row.loc['diffmatchname_80'])
        continue
    else:
        pl_nameinRA.append(row.loc['diffmatchname_70'])
data_gcar['Customer_Name_lower'] = pl_nameinRA

data_gcar = pd.merge(data_gcar[['name_gcar','Customer_Name_lower']], dat[[
    'Customer_Name','Customer_Name_lower', 'CUSTOMERID', 
    'Borrow_US_entity', 'Country']], 
    on=['Customer_Name_lower'], how='left')

data_gcar.rename(columns={'name_gcar':'ForeignCompany'}, inplace=True)
data_gcar.drop(['Customer_Name_lower'], axis=1, inplace=True)
data_gcar['FC_source'] = 'GCAR'
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:11:29 2019

@author: ub71894 (4e8e6d0b), CSG
"""

import os, sys, pandas as pd, numpy as np
import difflib
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


gdat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR BR Approved All.pkl')
gdat = gdat[gdat['Borrower Type']=='Corporation (Major Company)']
gdat_uni = gdat.drop_duplicates(subset=['Borrower Name'])


sp  = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\CNI\data\Compustat\compustat_data.pickle')
sp_uni = sp.drop_duplicates(subset=['CONM'], keep='last')
pl_name_base = sp_uni['CONM'].tolist()




#%% name match
data_gcar = pd.DataFrame()
data_gcar['name_gcar'] = gdat_uni['Borrower Name'].tolist()
data_gcar['diffmatchname_70'] = 'NAN'
data_gcar['diffmatchname_80'] = 'NAN'
data_gcar['diffmatchname_90'] = 'NAN'

for i in range(len(data_gcar)):
    temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.9)
    if temp:
        data_gcar.diffmatchname_90[i] = temp[0]
    else:
        temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.8)
        if temp:
            data_gcar.diffmatchname_80[i] = temp[0]
        else:
            temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.7)
            if temp:
                data_gcar.diffmatchname_70[i] = temp[0]    


data_gcar.to_excel(r'output\03\data_gcar_major.xlsx')





#%% after manual 
#############GCAR
data_gcar = pd.read_excel(r'output\data_gcar.xlsx')
data_gcar = data_gcar.query('correct==1')

pl_nameinRA = []
for i, row in data_gcar.iterrows():
    if row.loc['diffmatchname_90'] != "NAN":
        pl_nameinRA.append(row.loc['diffmatchname_90'])
        continue
    elif row.loc['diffmatchname_80'] != "NAN":
        pl_nameinRA.append(row.loc['diffmatchname_80'])
        continue
    else:
        pl_nameinRA.append(row.loc['diffmatchname_70'])
data_gcar['Customer_Name_lower'] = pl_nameinRA

data_gcar = pd.merge(data_gcar[['name_gcar','Customer_Name_lower']], dat[[
    'Customer_Name','Customer_Name_lower', 'CUSTOMERID', 
    'Borrow_US_entity', 'Country']], 
    on=['Customer_Name_lower'], how='left')

data_gcar.rename(columns={'name_gcar':'ForeignCompany'}, inplace=True)
data_gcar.drop(['Customer_Name_lower'], axis=1, inplace=True)
data_gcar['FC_source'] = 'GCAR'
# -*- coding: utf-8 -*-
"""
Created on Fri Jan  3 12:01:22 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np
import difflib, time
from heapq import nlargest
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR BR Approved All.pkl')
gdat_unim = data[data['Borrower Type']=='Corporation (Major Company)'].drop_duplicates(subset=['Borrower Name'])
gdat_unio = data[data['Borrower Type']=='Corporation (Other Company)'].drop_duplicates(subset=['Borrower Name'])

list_cols = ['Borrower Name', 'Borrower Country','Borrower Industry','Borrower Type']
gdat_unim = gdat_unim[list_cols]
gdat_unio = gdat_unio[list_cols]

list_overlap = list(set(gdat_unim['Borrower Name'])&set(gdat_unio['Borrower Name']))
gdat_unim.loc[gdat_unim['Borrower Name'].isin(list_overlap), 'Borrower Type'] = 'Both'
gdat_unio.loc[gdat_unio['Borrower Name'].isin(list_overlap), 'Borrower Type'] = 'Both'

dat = pd.concat([gdat_unim, gdat_unio], axis=0)
dat_uni = dat.drop_duplicates(subset=['Borrower Name'])
dat_uni['Borrower Type'].value_counts()
'''
Out[15]: 
Corporation (Major Company)    3900
Corporation (Other Company)    3617
Both                            734
Name: Borrower Type, dtype: int64

'''


country_count = dat_uni['Borrower Country'].value_counts()
# 71 countries

(country_count.cumsum()/len(dat_uni)).head(20)
'''
# top 20 cum pct:
UNITED STATES     0.563810
#BRAZIL            0.656163
#CANADA            0.741122
#MEXICO            0.795782
UNITED KINGDOM    0.818204
#NETHERLANDS       0.838201
#CHILE             0.853351
#LUXEMBOURG        0.864138
#IRELAND           0.874197
#SWITZERLAND       0.884256
#ARGENTINA         0.893710
GERMANY           0.901709
#PERU              0.908375
FRANCE            0.913950
JAPAN             0.919404
#AUSTRALIA         0.924494
#COLOMBIA          0.929463
#SINGAPORE         0.934311
#BERMUDA           0.938553
#CAYMAN ISLANDS    0.942310


0.005575+0.007999
Batch 1:  33.70%
#BRAZIL          
#CANADA          
#MEXICO          
#NETHERLANDS     
#CHILE           
#LUXEMBOURG      
#IRELAND         
#SWITZERLAND     
#ARGENTINA               
#PERU                      
#AUSTRALIA       
#COLOMBIA        
#SINGAPORE       
#BERMUDA         
#CAYMAN ISLANDS  

Batch 2:  1.35%
GERMANY 
FRANCE 

Batch 3:  0.54%
JAPAN  

Batch 4:
Mid Atlantic 
Midwest

Batch 5:
Northeast
Southeast
Southwest


Batch 6:
west

Batch 7:
UNITED KINGDOM 
    Industry: Financials; Energy and Utilities; Materials; Healthcare

    
'''




ind_count = dat_uni['Borrower Industry'].value_counts()
#434 industries

(ind_count.cumsum()/len(dat_uni)).head(20)
'''
# top 20 cum pct:
Transportation Equipment             0.058054
Electric Utility Services            0.108835
Oil and Gas Extraction               0.146285
Machinery Wholesale                  0.180099
Other Finance Services               0.211126
Industrial & Commercial Machinery    0.234396
Food and Kindred Products            0.256333
Food and Kindred Products            0.256333
Food and Kindred Products            0.256333
Gas Utility Services                 0.277300
Chemicals and Related Products       0.296813
Prescription Pharmaceuticals         0.315477
Metal Mining                         0.330990
Other Products Wholesale             0.346261
Aircraft and Components              0.360441
Special Social Services              0.374379
Fabricated Metal Products            0.387226
Other Financial Institutions         0.399709
General Machinery Wholesale          0.410980
Iron Metal Industries                0.421403
Other Health Care Services           0.431463
Chemical Product Wholesale           0.441401
'''



sp_batch  = pd.read_excel(r'..\data\SNL_batch6.xlsx',sheet_name='batch6', skiprows=1)
sp_batch = sp_batch.drop([0])
sp_batch_uni = sp_batch.drop_duplicates(subset=[ 'Entity Name '], keep='last')
pl_name_base = sp_batch_uni['Entity Name '].tolist()

pl_name_base_str=[]
for name in pl_name_base:
    if isinstance(name,str):
        pass
    else:
        name = str(name)
    pl_name_base_str.append(name )



data_tokyo = pd.DataFrame()
data_tokyo['name_tokyo'] = dat_uni['Borrower Name'].tolist()
data_tokyo['diffmatchname_80'] = np.nan
data_tokyo['diffmatchname_90'] = np.nan

start_time = time.time()
for i in range(len(data_tokyo)):
    if i%82==0:
        print(f'{i/82:.0f}% complete{20*"."}')
    s = difflib.SequenceMatcher()
    s.set_seq2(data_tokyo.name_tokyo[i])
    max_score=0.8; 
    for x in pl_name_base_str:
        s.set_seq1(x)
        if s.real_quick_ratio() >= max_score and \
           s.quick_ratio() >= max_score and \
           s.ratio() >= max_score:
            max_score = s.ratio()
            bb = x
        else:
            continue
    if max_score>=0.9:
        data_tokyo.diffmatchname_90[i] = bb
    elif max_score>0.8:
        data_tokyo.diffmatchname_80[i] = bb
    else:
        continue
print("--- %s seconds ---" % (time.time() - start_time))


data_tokyo.to_excel(r'output\tokyonamematching\data_tokyo_batch6.xlsx')



#%%
'''
EBITDA  132709
Net sales
    sales revenue  132527
    gross profit 132529   all NA
    total  revenue  141780

total debt 132319

capitalization  133846
interest expense  227248

Net Operating Profit
    Net Operating Income 138302
    Net Operating Profit after tax 138304
Ending Cash & Equivalents 132167

Tangible Net Worth 
    Adjust net worth 255357
    Intangible Assets 266826
    Tangible Assets  132265

Compustat

Total Asset  268552
Total Liabilities  268420
Total debt 276026
EBITDA  271528
Net Sales  275450
Capitalization 
    Total Capitalization, at Book Value 278237
Interest Expense  271486
Net Operating Profit
    Net Operating Income 291625
    Net Operating Profit after tax 291628
    Net Operating Income(reported) 275461
Ending cash & Equivalents 
    cash & cash Equivalents(SP) 275616
    cash & cash Equivalents(SNL) 273676
Tangible Net Worth 
    Tangible Assets  273697

we can pull sp rating
'''



#%% after manual work

combined_data = pd.DataFrame()
for i in range(6):

    batch_No = i+1

    sp_batch  = pd.read_excel(r'..\data\SNL_batch{}.xlsx'.format(batch_No), sheet_name='batch{}'.format(batch_No),skiprows=1)
    sp_batch = sp_batch.drop([0])
    sp_batch_uni = sp_batch.drop_duplicates(subset=[ 'Entity Name '], keep='last')
    sp_batch_uni = sp_batch_uni[[
    'Entity Name ',
    'Entity ID ',
    'Company Type ',
    'Company Status ',
    'Geography ']]
    
    matched_data_batch = pd.read_excel(r'output\tokyonamematching\data_tokyo_batch{}_corrected.xlsx'.format(batch_No))
    matched = matched_data_batch.query('correct==1')
    matched.reset_index(drop=True, inplace=True)
    matched['diffmatchname_90'].replace({'NAN':np.nan}, inplace=True) # for batch 1
    matched['Entity Name '] = matched['diffmatchname_90'].fillna(matched['diffmatchname_80'])
    matched = matched[['name_tokyo', 'Entity Name ']]
    matched['source'] = f'batch{batch_No}'
    mergered_data = pd.merge(matched, sp_batch_uni, on=['Entity Name '], how='left')
    combined_data = pd.concat([combined_data, mergered_data], axis=0)
                        

combined_data.reset_index(drop=True, inplace=True)
combined_data.to_pickle(r'output\tokyonamematching\combined_data_raw.pkl')

combined_data.drop_duplicates(subset=['name_tokyo'], keep='last', inplace=True)
combined_data.reset_index(drop=True, inplace=True)
combined_data.to_pickle(r'output\tokyonamematching\combined_data_cleaned.pkl')

dat_uni = pd.read_csv('TokyoDataUniqueName.csv')
print( f'The raw availability pct is {100*len(combined_data)/len(dat_uni):.2f}%')

# The raw availability pct is 9.89%


#%%
data_forpull = pd.concat([combined_data]*4, axis=0)
data_forpull.sort_values(by=['name_tokyo'], inplace=True)
data_forpull['year'] = ['FY2018','FY2017','FY2016','FY2015']*len(combined_data)
data_forpull.reset_index(drop=True, inplace=True)
data_forpull = data_forpull[[
'Entity ID ',
'year',
'name_tokyo',
'Entity Name ',
'Entity ID ',
'Company Type ',
'Company Status ',
'Geography ',
]]

data_forpull.to_excel(r'output\tokyonamematching\data_forpull.xlsx')


#%% after pulling data


dat_2018['ValidFieldsNo'] = 10-dat_2018[[
 'Total Assets (Reported)',
 'Total Liabilities (Reported)',
 'Total Debt (Reported)',
 'EBITDA (Reported)',
 'Net Sales - (Reported) (Reported)',
 'Total Capitalization, at Book Value (Reported)',
 'Interest Expense (Reported)',
 'Net Operating Profit After Tax (Reported)',
 'Cash and Cash Equivalents (Reported)',
 'Tangible Assets (Reported)']].isnull().sum(axis=1)

temp = dat_2018.groupby(by=['Borrower Country'])['ValidFieldsNo'].mean()

temp[[
'UNITED STATES',
'BRAZIL',
'CANADA',
'MEXICO',
'UNITED KINGDOM',
'NETHERLANDS',
'CHILE',
'LUXEMBOURG',
'SWITZERLAND',
'IRELAND',
'ARGENTINA',
'GERMANY',
'PERU',
'FRANCE',
'JAPAN',
'AUSTRALIA',
'COLOMBIA',
'SINGAPORE',
'BERMUDA',
]]
UNITED STATES     1.965587
BRAZIL            3.090909
CANADA            2.641509
MEXICO            0.384615
UNITED KINGDOM    5.000000
NETHERLANDS       0.095238
CHILE             1.600000
LUXEMBOURG        0.000000
SWITZERLAND       0.555556
IRELAND           0.500000
ARGENTINA         5.000000
GERMANY           0.933333
PERU              1.666667
FRANCE            2.333333
JAPAN                  NaN
AUSTRALIA         2.714286
COLOMBIA          5.000000
SINGAPORE         0.000000
BERMUDA           3.000000# -*- coding: utf-8 -*-
"""
Created on Mon Jan 13 10:40:56 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
import missingno as msno
from pandas_profiling import ProfileReport

dat_tokyo = pd.read_csv('TokyoDataUniqueName.csv')
dat_tokyo = dat_tokyo.drop(['Unnamed: 0'],axis=1)

data = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\output\tokyonamematching\data_pulled.xlsx',
 sheet_name='CapitalIQ',
 skiprows=1)
data = data.drop([0])
data.rename(columns={'name_tokyo':'Borrower Name'}, inplace=True)
dat_foranalysis = pd.merge(data[['Borrower Name','year']].drop_duplicates(subset=['Borrower Name']),
							 dat_tokyo, on=['Borrower Name'], how='right')

dat_last4 = pd.merge(data, dat_tokyo, on=['Borrower Name'], how='left')
dat_2018 = dat_last4.query('year=="FY2018"')

cols_forqc = [
 'Borrower Country', 'Borrower Industry', 'Borrower Type',
 'Entity ID ',
 'Company Type ',
 'Company Status ',
 'Geography ',
 'Total Assets (Reported)',
 'Total Liabilities (Reported)',
 'Total Debt (Reported)',
 'EBITDA (Reported)',
 'Net Sales - (Reported) (Reported)',
 'Total Capitalization, at Book Value (Reported)',
 'Interest Expense (Reported)',
 'Net Operating Income (Reported)',
 'Net Operating Profit After Tax (Reported)',
 'Net Operating Income - (Reported) (Reported)',
 'Cash and Cash Equivalents (Reported)',
 'Cash and Cash Equivalents (Reported).1',
 'Tangible Assets (Reported)']

cols_formsno = [
 'Entity ID ',
 'Total Assets (Reported)',
 'Total Liabilities (Reported)',
 'Total Debt (Reported)',
 'EBITDA (Reported)',
 'Net Sales - (Reported) (Reported)',
 'Total Capitalization, at Book Value (Reported)',
 'Interest Expense (Reported)',
 'Net Operating Profit After Tax (Reported)',
 'Cash and Cash Equivalents (Reported)',
 'Tangible Assets (Reported)']


cols_factors = [
 'Total Assets (Reported)',
 'Total Liabilities (Reported)',
 'Total Debt (Reported)',
 'EBITDA (Reported)',
 'Net Sales - (Reported) (Reported)',
 'Total Capitalization, at Book Value (Reported)',
 'Interest Expense (Reported)',
 'Net Operating Income (Reported)',
 'Net Operating Profit After Tax (Reported)',
 'Net Operating Income - (Reported) (Reported)',
 'Cash and Cash Equivalents (Reported)',
 'Cash and Cash Equivalents (Reported).1',
 'Tangible Assets (Reported)'
       ]
#%%
dat_plot1 = dat_last4[cols_forqc]
profile = ProfileReport(dat_plot1, correlation_threshold=1)
profile.to_file()

msno.matrix(dat_plot1[cols_formsno])

len(dat_last4[cols_factors].dropna(how='all'))
1266


#%%

dat_plot2 = dat_2018[cols_forqc]

profile = ProfileReport(dat_plot2, correlation_threshold=1)
profile.to_file()

msno.matrix(dat_plot2[cols_formsno])


len(dat_2018[cols_factors].dropna(how='all'))
294
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_cif_cleaner, gcar_americas_office_code

data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019.pkl')
# 482712
data = data[data['Data From']=='CD']
# 222034
data1 = data[data['Borrower Type']=='Corporation (Major Company)']
# 36858
data2 = data[data['Borrower Type']=='Corporation (Other Company)']
# 125359
data = pd.concat([data1, data2], axis=0)

data.dropna(subset=['Borrower CIF'], inplace=True)
# 162217
data['Borrower CIF_cleaned'] =gcar_cif_cleaner(data)
# 162217



data['timestamp'] = data['Approval Date'].mask(
                        pd.isnull(data['Approval Date']),
                        data['Approval/Agreed Date'])
data['timestamp'] = pd.to_datetime(data['timestamp'])
data.dropna(subset=['timestamp'], inplace=True)
# 162217

data['year'] = [int(x.year) for x in data['timestamp'] ]
gdat_unim = data[data['Borrower Type']=='Corporation (Major Company)'].drop_duplicates(subset=['Borrower CIF_cleaned','year'])
gdat_unio = data[data['Borrower Type']=='Corporation (Other Company)'].drop_duplicates(subset=['Borrower CIF_cleaned','year'])
#gdat_unifff = data[data['Borrower Type']=='MUB Model'].drop_duplicates(subset=['Borrower CIF_cleaned'])

list_overlap = list(set(gdat_unim['Borrower CIF_cleaned'])&set(gdat_unio['Borrower CIF_cleaned']))
gdat_unim['Borrower Type 2'] = gdat_unim['Borrower Type'].copy()
gdat_unio['Borrower Type 2'] = gdat_unio['Borrower Type'].copy()

gdat_unim.loc[gdat_unim['Borrower CIF_cleaned'].isin(list_overlap), 'Borrower Type 2'] = 'Both'
gdat_unio.loc[gdat_unio['Borrower CIF_cleaned'].isin(list_overlap), 'Borrower Type 2'] = 'Both'

dat = pd.concat([gdat_unim, gdat_unio], axis=0)
# 105060
dat = dat.drop_duplicates(subset=['Borrower CIF_cleaned','year'])
# 104062
dat.shape
#Out[26]: (104062, 904)


dat['Borrower Type'].value_counts()
'''
Out[27]: 
Corporation (Other Company)    78039
Corporation (Major Company)    16405
Both                            9618
Name: Borrower Type, dtype: int64
'''
#dat.to_pickle(r'..\data\GCAR_2008-2019_filtered2.pkl')
#country_str = 'Applying Office Name'



country_str = 'Borrower Country'
country_count = dat[country_str].value_counts()
# 124

dat['year'].value_counts().sort_index()

'''
2008      195
2009     3585
2010     9393
2011    10151
2012    10946
2013    11249
2014    11698
2015     9981
2016     9889
2017     9722
2018     8957
2019     1309
Name: year, dtype: int64
'''
#%%
def _top5_country(data):
    return(data[country_str].value_counts().head(10))

df_1 = dat.groupby(by=['year']).apply(_top5_country).reset_index()
df_1.to_excel(r'top10county_by_year.xlsx')




dat_GCAR = dat.loc[(dat['Borrower Office Code']).isin([3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770])] 
dat_GCAR['year'].value_counts()
'''
2016    2588
2014    2482
2015    2385
2011    2323
2013    2251
2012    2133
2010    2048
2017    1607
2018    1124
2019     227
'''
df_2 = dat_GCAR.groupby(by=['year']).apply(_top5_country).reset_index()

# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_cif_cleaner, gcar_converter

data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019.pkl')
# 482712



#%% add part of BL besides CD # modified on 08/28/2020

data_BL = data.loc[(data['Data From'].isin(['BL']))]
data_BL = data_BL[data_BL['Ringi/Sairyo']=='Sairyo'] 
data_CD = data.loc[(data['Data From'].isin(['CD']))]

data = pd.concat([data_CD, data_BL])
# 260879

#  add on 2020/09/01 , but no impact

data = data.sort_values(by=['Data From'])
data = data.drop_duplicates(subset=['Application No.'],keep='last')





data1 = data[data['Borrower Type']=='Corporation (Major Company)']
# 43831
data2 = data[data['Borrower Type']=='Corporation (Other Company)']
# 154135
data = pd.concat([data1, data2], axis=0)

data.dropna(subset=['Borrower CIF'], inplace=True)
# 197966
data['Borrower_CIF_cleaned'] = gcar_cif_cleaner(data, col='Borrower CIF')



data['timestamp'] = data['Approval Date'].mask(
                        pd.isnull(data['Approval Date']),
                        data['Approval/Agreed Date'])
data['timestamp'] = pd.to_datetime(data['timestamp'])
data.dropna(subset=['timestamp'], inplace=True)
# 197966

data['year'] = [int(x.year) for x in data['timestamp'] ]
gdat_unim = data[data['Borrower Type']=='Corporation (Major Company)'].drop_duplicates(subset=['Borrower_CIF_cleaned','year'])
gdat_unio = data[data['Borrower Type']=='Corporation (Other Company)'].drop_duplicates(subset=['Borrower_CIF_cleaned','year'])
#gdat_unifff = data[data['Borrower Type']=='MUB Model'].drop_duplicates(subset=['Borrower_CIF_cleaned'])

list_overlap = list(set(gdat_unim['Borrower_CIF_cleaned'])&set(gdat_unio['Borrower_CIF_cleaned']))
gdat_unim['Borrower Type 2'] = gdat_unim['Borrower Type'].copy()
gdat_unio['Borrower Type 2'] = gdat_unio['Borrower Type'].copy()

gdat_unim.loc[gdat_unim['Borrower_CIF_cleaned'].isin(list_overlap), 'Borrower Type 2'] = 'Both'
gdat_unio.loc[gdat_unio['Borrower_CIF_cleaned'].isin(list_overlap), 'Borrower Type 2'] = 'Both'

dat = pd.concat([gdat_unim, gdat_unio], axis=0)
# 121645

# keep just one record per year per customer, keep one from "CD"
dat = dat.drop_duplicates(subset=['Borrower_CIF_cleaned','year'], keep='last')
# 120384

dat.reset_index(drop=True, inplace=True)



dat.to_pickle(r'..\data\GCAR_2008-2019_filtered3.pkl')

# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered.pkl')
dat = data.query('year>=2017')

#%% Drop defaulters
def_mask = (dat['Applied Rating']==101) | (dat['Applied Rating']==102)
dat = dat[~def_mask]

country_str = 'Borrower Country'
country_count = dat[country_str].value_counts()
# 104
dat['year'].value_counts().sort_index()

'''
2017    9714
2018    8948
2019    8283
Name: year, dtype: int64
'''

#%% split to Americas and Non-Americas
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]

dat_americas = dat.loc[(dat['Borrower Office Code']).isin(list_americas_office_code)] 
dat_nonamericas = dat.loc[~(dat['Borrower Office Code']).isin(list_americas_office_code)] 

# americas
dat_americas['year'].value_counts()
'''
2017    1606
2018    1124
2019     972
'''
dat_nonamericas['year'].value_counts()
'''
2017    8108
2018    7824
2019    7311
'''


'''
list_officename = []
list_countryname = []
for code in list_americas_office_code:
    mask = dat_americas['Borrower Office Code']==code
    officename = dat_americas[mask]['Borrower Office Name'].unique().tolist()
    countryname = dat_americas[mask]['Borrower Country'].unique().tolist()
    list_officename.append(officename)
    list_countryname.append(countryname)
df_am = pd.DataFrame()
df_am['code'] = list_americas_office_code
df_am['OfficeName'] = list_officename
df_am['CountryName'] = list_countryname

df_am.to_excel('americas_code.xlsx')
'''

#%% cleaning rating 
# Americas: 2-3-->2-2 and 8-3 --> 8-2
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].replace({23:22,83:82})
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].replace({23:22,83:82})
# Non- Americas: 8-3 --> 8-2 and remove outliers (21)
dat_nonamericas['Applied Rating'] = dat_nonamericas['Applied Rating'].replace({83:82})
dat_nonamericas = dat_nonamericas[dat_nonamericas['Applied Rating']!=21]


# Americas: replace 2-0 to 50% 2-1 and 50% 2-2
mask_20 = dat_americas['Applied Rating']==20
mask_21_filter = mask_20.cumsum() < (mask_20.sum()/2)
mask_22_filter = mask_20.cumsum() >= (mask_20.sum()/2)
mask_21 = mask_20 * mask_21_filter
mask_22 = mask_20 * mask_22_filter
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_21.astype('bool'), 21)
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_22.astype('bool'), 22)


# Americas: replace 3-0 to 33.3% to 3-1, 33.3% to 3-2, and 33.3% to 3-3
mask_30 = dat_americas['Applied Rating']==30
mask_31_filter = mask_30.cumsum() < (mask_30.sum()/3)
mask_32_filter = (mask_30.cumsum() >= (mask_30.sum()/3)) & ( mask_30.cumsum() < (mask_30.sum()*2/3))
mask_33_filter = mask_30.cumsum() >= (mask_30.sum()*2/3)

mask_31 = mask_30 * mask_31_filter
mask_32 = mask_30 * mask_32_filter
mask_33 = mask_30 * mask_33_filter

dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_31.astype('bool'), 31)
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_32.astype('bool'), 32)
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_33.astype('bool'), 33)



#%% cast integer rating to string
dat_americas['BTMU Rating'] = dat_americas['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat_nonamericas['BTMU Rating'] = dat_nonamericas['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])

dat_americas['BTMU Rating'].value_counts().sort_index() 
dat_nonamericas['BTMU Rating'].value_counts().sort_index() 



'''
#%% find some missed office code for Americas:
list_cty = ['BRAZIL','Br. Virgin Is.','BERMUDA','CAYMAN ISLANDS','UNITED STATES','CANADA']
list_df_tmp = {}
for cty in list_cty:
    tmp = dat_nonamericas[dat_nonamericas['Borrower Country'] ==cty]
    list_code= tmp['Borrower Office Code'].unique().tolist()
    list_officename=[]
    for code in list_code:
        list_officename.append(tmp[tmp['Borrower Office Code']==code]['Borrower Office Name'].unique().tolist())
    
    df_tmp = pd.DataFrame()
    df_tmp['code'] = list_code
    df_tmp['officename'] = list_officename
    list_df_tmp.update({cty:df_tmp})
# manual check find code below:

additional_BorrowerOfficeCode = [3281,3282,3286]  # add this to previous code


addtional = dat_nonamericas.loc[(dat_nonamericas['Borrower Office Code']).isin(additional_BorrowerOfficeCode)] 
dat_americas = pd.concat([dat_americas, addtional], axis=0)

dat_nonamericas = dat_nonamericas.loc[~(dat_nonamericas['Borrower Office Code']).isin(additional_BorrowerOfficeCode)] 
'''

dat_americas.to_pickle(r'..\data\dat_americas.pkl')
dat_nonamericas.to_pickle(r'..\data\dat_nonamericas.pkl')


'''
dat_americas.to_pickle(r'..\data\dat_americas_2008-2019.pkl')
dat_nonamericas.to_pickle(r'..\data\dat_nonamericas_2008-2019.pkl')

'''

# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCARS 2008-2019_matches.pkl')


#%% Drop defaulters
def_mask = (dat['Applied Rating']==101) | (dat['Applied Rating']==102)
dat = dat[~def_mask]

country_str = 'Borrower Country'
country_count = dat[country_str].value_counts()
dat['year'].value_counts().sort_index()


#%% split to Americas and Non-Americas
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286
]

dat_americas = dat.loc[(dat['Borrower Office Code']).isin(list_americas_office_code)] 
dat_nonamericas = dat.loc[~(dat['Borrower Office Code']).isin(list_americas_office_code)] 



#%% Drop rating 2-0, 2-3, 3-0, 8-3, 9-0

mask = dat_americas['Applied Rating'].isin([20,23,30,83,90])
dat_americas = dat_americas[~mask]
mask = dat_nonamericas['Applied Rating'].isin([20,23,30,83,90])
dat_nonamericas = dat_nonamericas[~mask]


# cast integer rating to string
dat_americas['BTMU Rating'] = dat_americas['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat_nonamericas['BTMU Rating'] = dat_nonamericas['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])

dat_americas['BTMU Rating'].value_counts().sort_index() 
dat_nonamericas['BTMU Rating'].value_counts().sort_index() 
'''
1-0      333
2-1        2
4-0    19301
5-1     7942
5-2     3979
6-1     3513
6-2     5341
7-0     8749
8-1     4641
8-2     3935
Name: BTMU Rating, dtype: int64
'''

# map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat_americas['Applied Rating PDRR'] = dat_americas['Applied Rating'].transform(lambda x: pd_mapping[x])
dat_nonamericas['Applied Rating PDRR'] = dat_nonamericas['Applied Rating'].transform(lambda x: pd_mapping[x])


cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Match'
]


dat_americas[cols].to_pickle(r'..\data\dat_americas_2008-2019_dropratings.pkl')
dat_nonamericas[cols].to_pickle(r'..\data\dat_nonamericas_2008-2019_dropratings.pkl')



#%% EU 
# load data and then 
def_mask = (dat['Applied Rating']==101) | (dat['Applied Rating']==102)
dat = dat[~def_mask]


list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286
]

list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

dat_eu = dat.loc[(dat['Borrower Office Code']).isin(list_eu_office_code)] 
dat_row = dat.loc[~(dat['Borrower Office Code']).isin(list_code)] 





#%% Drop rating 2-0, 2-3, 3-0, 8-3, 9-0
mask = dat_eu['Applied Rating'].isin([20,23,30,83,90])
dat_eu = dat_eu[~mask]
mask = dat_row['Applied Rating'].isin([20,23,30,83,90])
dat_row = dat_row[~mask]


# cast integer rating to string
dat_eu['BTMU Rating'] = dat_eu['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat_row['BTMU Rating'] = dat_row['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])

dat_eu['BTMU Rating'].value_counts().sort_index() 
dat_row['BTMU Rating'].value_counts().sort_index() 


# map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat_eu['Applied Rating PDRR'] = dat_eu['Applied Rating'].transform(lambda x: pd_mapping[x])
dat_row['Applied Rating PDRR'] = dat_row['Applied Rating'].transform(lambda x: pd_mapping[x])


cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Match'
]


dat_eu[cols].to_pickle(r'..\data\dat_eu_2008-2019_dropratings.pkl')
dat_row[cols].to_pickle(r'..\data\dat_row_2008-2019_dropratings.pkl')

# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner


df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
df1 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_2008-2019_dropratings.pkl')
df2 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_2008-2019_dropratings.pkl')
df3 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_2008-2019_dropratings.pkl')
df4 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_row_2008-2019_dropratings.pkl')



bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df1, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df2, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df3, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df4, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


df_bvd['Fiscal year'].value_counts().sort_index()
'''
Out[20]: 
1984       1
1985       1
1986       1
1987       1
1988       1
1989       2
1990       4
1991       5
1992       7
1993      12
1994      22
1995      29
1996      32
1997      35
1998      47
1999      82
2000     132
2001     209
2002     336
2003     433
2004     471
2005     617
2006    1078
2007    1692
2008    2962
2009    6279
2010    6813
2011    7214
2012    7572
2013    8556
2014    9367
2015    8940
2016    9075
2017    8597
2018    6637
2019     250
2020       3
Name: BvD_Fiscal year, dtype: int64
'''

len(set(df1['Borrower CIF'].unique()))
# 5761

len(set(df2['Borrower CIF'].unique()))
# 14080

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(df1['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 1061

len(set(df2['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 8187


len(set(df3['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 2386

len(set(df4['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 5873


#%%

#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% merge
df_am = pd.merge(df1, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_am['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_am['BvD_CIF']]
df_am['inBvD'].value_counts()
'''
Out[11]: 
No     16584
Yes     2976
Name: inBvD, dtype: int64

'''

df_nam = pd.merge(df2, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_nam['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_nam['BvD_CIF']]
df_nam['inBvD'].value_counts()

'''
No     30826
Yes    29054
Name: inBvD, dtype: int64
'''

df_eu = pd.merge(df3, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_eu['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_eu['BvD_CIF']]
df_eu['inBvD'].value_counts()
'''
Yes    9016
No     6874
Name: inBvD, dtype: int64
'''

df_row = pd.merge(df4, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_row['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_row['BvD_CIF']]
df_row['inBvD'].value_counts()

'''
No     23952
Yes    20038
Name: inBvD, dtype: int64
'''


df_am.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_SFA_BvD_2008-2019.pkl')
df_nam.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_SFA_BvD_2008-2019.pkl')

df_eu.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_SFA_BvD_2008-2019.pkl')
df_row.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_row_SFA_BvD_2008-2019.pkl')

# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']


df_am = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_SFA_BvD_2008-2019.pkl')
df_nam = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_SFA_BvD_2008-2019.pkl')

df_eu = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_SFA_BvD_2008-2019.pkl')
df_row = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_row_SFA_BvD_2008-2019.pkl')


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

df_am = build_cni_factors(df_am)
df_nam = build_cni_factors(df_nam)
df_eu = build_cni_factors(df_eu)
df_row = build_cni_factors(df_row)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df_am[factor] = df_am[factor].clip(
            np.nanmin(df_am[factor][df_am[factor] != -np.inf]), 
            np.nanmax(df_am[factor][df_am[factor] != np.inf]))
        df_nam[factor] = df_nam[factor].clip(
            np.nanmin(df_nam[factor][df_nam[factor] != -np.inf]),
             np.nanmax(df_nam[factor][df_nam[factor] != np.inf]))
        df_eu[factor] = df_eu[factor].clip(
            np.nanmin(df_eu[factor][df_eu[factor] != -np.inf]),
             np.nanmax(df_eu[factor][df_eu[factor] != np.inf]))
        df_row[factor] = df_row[factor].clip(
            np.nanmin(df_row[factor][df_row[factor] != -np.inf]),
             np.nanmax(df_row[factor][df_row[factor] != np.inf]))
    
 
   


#%% factor level SFA
def sfa_result(data, filename):
    dat = data.query('2016<=year<=2018')
    dat_LC = dat.query('BvD_Proxy_Net_Sales_inUSD>1e9')
    dat_MM = dat.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
    list_dataset = [dat, dat_LC, dat_MM]
    list_tabname = ['Full', 'LC', 'MM']
    list_factor = model_LC.quant_factor + model_MM.quant_factor
    list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier)
    list_year = ['==2016','==2017','==2018','<=2018']
    list_colname = ['2016_SD','2017_SD','2018_SD','ALL_SD']
    list_colname2 = ['2016_obs','2017_obs','2018_obs','ALL_obs']
    
    writer = pd.ExcelWriter(filename)
    for i, df in enumerate(list_dataset):
        result = pd.DataFrame()
        for p,cond in enumerate(list_year):
            df_tmp = df.query(f'year{cond}')
            list_sd = []
            list_obs = []
            for q, factor in enumerate(list_factor):
                df_tmp_dropna = df_tmp.dropna(subset=[factor])
                list_obs.append(len(df_tmp_dropna))
                list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
            result[list_colname[p]] = list_sd
            result[list_colname2[p]] = list_obs
    
        result.index = list_factor
        result.to_excel(writer, list_tabname[i])
    
    writer.save()



sfa_result(df_am, r'SFA/SFA_results_AM_3year.xlsx')

sfa_result(df_nam, r'SFA/SFA_results_NAM_3year.xlsx')

sfa_result(df_eu, r'SFA/SFA_results_EU_3year.xlsx')

sfa_result(df_row, r'SFA/SFA_results_RoW_3year.xlsx')

sfa_result(pd.concat([df_am, df_eu]), r'SFA/SFA_results_AM+EU_3year.xlsx')

# -*- coding: utf-8 -*-
"""
Created on Mon May 18 12:42:58 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
df_am = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_SFA_BvD_2008-2019.pkl')
df_nam = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_SFA_BvD_2008-2019.pkl')

df_eu = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_SFA_BvD_2008-2019.pkl')
df_row = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_row_SFA_BvD_2008-2019.pkl')


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    


def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 



df_am = build_cni_factors(df_am)
df_nam = build_cni_factors(df_nam)
df_eu = build_cni_factors(df_eu)
df_row = build_cni_factors(df_row)

df_am = other_processing(df_am)
df_nam = other_processing(df_nam)
df_eu = other_processing(df_eu)
df_row = other_processing(df_row)






# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df_am[factor] = df_am[factor].clip(
            np.nanmin(df_am[factor][df_am[factor] != -np.inf]), 
            np.nanmax(df_am[factor][df_am[factor] != np.inf]))
        df_nam[factor] = df_nam[factor].clip(
            np.nanmin(df_nam[factor][df_nam[factor] != -np.inf]),
             np.nanmax(df_nam[factor][df_nam[factor] != np.inf]))
        df_eu[factor] = df_eu[factor].clip(
            np.nanmin(df_eu[factor][df_eu[factor] != -np.inf]),
             np.nanmax(df_eu[factor][df_eu[factor] != np.inf]))
        df_row[factor] = df_row[factor].clip(
            np.nanmin(df_row[factor][df_row[factor] != -np.inf]),
             np.nanmax(df_row[factor][df_row[factor] != np.inf]))
    
 
   



def get_tokyo_rating(dat, model):

    pdrr_tokyo_mapping = dict(zip(MS.PDRR, MS.BTMU))
    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
        rating_tokyo = pdrr_tokyo_mapping[rating_PDRR]
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)
        list_Ratings_Tokyo.append(rating_tokyo)
    
    dat['PD'] = list_PD
    dat['Ratings_PDRR'] = list_Ratings_PDRR
    dat['Ratings_Tokyo'] = list_Ratings_Tokyo

    return (dat)


#%% 
def sfa_quant_result(data, filename):
    dat = data.query('2016<=year<=2018')
    dat_LC = dat.query('BvD_Proxy_Net_Sales_inUSD>1e9')
    dat_MM = dat.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
    list_dataset = [dat, dat_LC, dat_MM]
    list_tabname = ['Full', 'LC', 'MM']
    
    list_factor = model_LC.quant_factor + model_MM.quant_factor
    list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier)
    list_year = ['==2016','==2017','==2018','<=2018']
    list_colname = ['2016_SD','2017_SD','2018_SD','ALL_SD']
    list_colname2 = ['2016_obs','2017_obs','2018_obs','ALL_obs']
    
    writer = pd.ExcelWriter(filename)
    for i, df in enumerate(list_dataset):
        result = pd.DataFrame()
        list_result=[]
        # by LC model
        dat_LC = df.dropna(subset=model_LC.quant_factor)
        dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC_norm['quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
        dat_LC_norm['quant_score'] = 50*(dat_LC_norm['quant_score']-model_LC.quantmean) / model_LC.quantstd
        dat_LC_norm = get_tokyo_rating(dat_LC_norm, model_LC)
        # by MM model
        dat_MM = df.dropna(subset=model_MM.quant_factor)
        dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM_norm['quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
        dat_MM_norm['quant_score'] = 50*(dat_MM_norm['quant_score']-model_MM.quantmean) / model_MM.quantstd
        dat_MM_norm = get_tokyo_rating(dat_MM_norm, model_MM)
    
    
        list_result.append(len(dat_LC_norm))
        list_result.append(SomersD(dat_LC_norm['Applied Rating'], dat_LC_norm['quant_score']))
        list_result.append(SomersD(dat_LC_norm['Applied Rating'], dat_LC_norm['Ratings_Tokyo']))
        list_result.append(len(dat_MM_norm))
        list_result.append(SomersD(dat_MM_norm['Applied Rating'], dat_MM_norm['quant_score']))
        list_result.append(SomersD(dat_MM_norm['Applied Rating'], dat_MM_norm['Ratings_Tokyo']))
        result[' '] = list_result
        result.index = ['valid_obs_by_LC_model', 'SD_LC_quant_score', 'SD_LC_ratings',
        'valid_obs_by_MM_model', 'SD_MM_quant_score', 'SD_MM_ratings',]
    
        result.to_excel(writer, list_tabname[i])
    
        TM_table_LC = pd.DataFrame(TMstats(dat_LC_norm, 'Ratings_PDRR', 'Applied Rating PDRR', PDRR=range(1,16)), index=['LC_model_result'])
        TM_table_LC.to_excel(writer, list_tabname[i], startrow=9)
        TM_table_MM = pd.DataFrame(TMstats(dat_MM_norm, 'Ratings_PDRR', 'Applied Rating PDRR', PDRR=range(1,16)), index=['MM_model_result'])
        TM_table_MM.to_excel(writer, list_tabname[i], startrow=13)
    
    
    
    writer.save()



sfa_quant_result(df_am, r'SFA/SFA_quant_results_AM_3year.xlsx')
sfa_quant_result(df_nam, r'SFA/SFA_quant_results_NAM_3year.xlsx')
sfa_quant_result(df_eu, r'SFA/SFA_quant_results_EU_3year.xlsx')
sfa_quant_result(df_row, r'SFA/SFA_quant_results_RoW_3year.xlsx')
sfa_quant_result(pd.concat([df_am, df_eu]), r'SFA/SFA_quant_results_AM+EU_3year.xlsx')






#%% add benchmark
def cal_benchmark(data):
    dat = data.query('2016<=year<=2018')
    dat = dat.query('BvD_Proxy_Net_Sales_inUSD==BvD_Proxy_Net_Sales_inUSD')
    dat_LC = dat.query('BvD_Proxy_Net_Sales_inUSD>1e9')
    dat_MM = dat.query('BvD_Proxy_Net_Sales_inUSD<=1e9')

    list_dataset = [dat, dat_LC, dat_MM]

    list_size=[]
    list_sd=[]
    for df in list_dataset:
        tmp = df.dropna(subset=['Current Total Score', 'Applied Rating'])
        list_size.append(len(tmp))
        # since the larger the score is, the better rating we get. We need to flip the sign
        list_sd.append(-SomersD(tmp['Applied Rating'], tmp['Current Total Score']))
    for i in range(3):
        print(list_size[i])
        print(list_sd[i])


cal_benchmark(df_am)
336
0.3249063257659246
82
0.02690100430416069
254
0.4403415829841069

cal_benchmark(df_eu)
881
0.2608789194043254
193
0.06724045185583648
688
0.38227702009923387

cal_benchmark(df_row)
5618
0.55597180490371
218
0.3545129054667878
5400
0.5630587249570274


cal_benchmark(pd.concat([df_am, df_eu]))
1217
0.2931508784146656
275
0.00841040183030967
942
0.41950608872899686

cal_benchmark(df_nam)
6499
0.5110856208954311
411
0.12183034853183376
6088
0.5439936648041924




#%% add benchmark
def cal_benchmark2(data):
    dat = data.query('2016<=year<=2018')
    dat = dat.query('BvD_Proxy_Net_Sales_inUSD==BvD_Proxy_Net_Sales_inUSD')
    dat_LC = dat.query('BvD_Proxy_Net_Sales_inUSD>1e9')
    dat_MM = dat.query('BvD_Proxy_Net_Sales_inUSD<=1e9')

    list_dataset = [dat, dat_LC, dat_MM]

    list_size=[]
    list_sd=[]
    for df in list_dataset:
        tmp = df.dropna(subset=['Primary Evaluation', 'Applied Rating'])
        list_size.append(len(tmp))
        # since the larger the score is, the better rating we get. We need to flip the sign
        list_sd.append(SomersD(tmp['Applied Rating'], tmp['Primary Evaluation']))
    for i in range(3):
        print(list_size[i])
        print(list_sd[i])


cal_benchmark2(df_am)
512
0.3520805220252509
217
0.25250037694124744
295
0.4382639503985828

cal_benchmark2(df_eu)
1171
0.4998271606224997
303
0.4357264818277909
868
0.4771044303797468

cal_benchmark2(df_row)
5623
0.5260360260749327
220
0.3704269095120282
5403
0.5321673248399976


cal_benchmark2(pd.concat([df_am, df_eu]))
1683
0.4945046634736577
520
0.320753389726184
1163
0.493681523071406

cal_benchmark2(df_nam)
6794
0.5239868935911581
523
0.41288723894517393
6271
0.5292719557412381# -*- coding: utf-8 -*-
"""
Created on Thu May 21 11:39:13 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
#dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_SFA_BvD_2008-2019.pkl')
#dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_SFA_BvD_2008-2019.pkl')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_SFA_BvD_2008-2019.pkl')

cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']



#%% build CnI ratios
dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
# for invalid_neg
dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']


# MM new transformation
dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb



#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios

dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']





factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))


dat.to_pickle(r'EXP\dat_eu_withRatios.pkl')
#dat.to_pickle(r'EXP\dat_am_withRatios.pkl')
#dat.to_pickle(r'EXP\dat_nam_withRatios.pkl')
# -*- coding: utf-8 -*-
"""
Created on Thu May 21 11:39:13 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')

df_am = pd.read_pickle(r'EXP\dat_am_withRatios.pkl')
df_eu = pd.read_pickle(r'EXP\dat_eu_withRatios.pkl')
# MM porfolio
dat = pd.concat([df_am, df_eu]).query('BvD_Proxy_Net_Sales_inUSD<=1e9').query('2016<=year<=2018')


#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    list_quality.append(len(df_tmp_dropna)/ len(dat))
    list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor])))

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'EXP/SFA_AM+EU_MM_3year.xlsx')



#%% pick SD > 0.17
list_short = result.query('SomersD>=0.15 and DataQuality>=0.7').index.tolist()





# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(np.abs(dat[list_short].corr()), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)



# after manual selection
list_final=[
'MM_NOP_to_NS',
'MM_TangNW_to_TA', 
'prof@NOP_to_TA',
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'BvD_Shareholders funds',
'prof@NOP_to_TangNW',
'bs@TL_to_Capt',
'BvD_Net current assets']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')
    print(f'5% of {factor} is {tmp[factor].quantile(0.05)}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,3,p)
    sns.distplot(tmp[factor])
    plt.title(factor)


dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1
dat['BvD_Shareholders funds'] = dat['BvD_Shareholders funds'] + np.abs(dat['BvD_Shareholders funds'].min()) +1 
dat['BvD_Net current assets'] = dat['BvD_Net current assets'] + np.abs(dat['BvD_Net current assets'].min()) +1 


list_transform = [0, 0, 0, 1, 1, 1, 0, 0, 1]



#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'MM_NOP_to_NS',
'MM_TangNW_to_TA', 
'prof@NOP_to_TA',
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'BvD_Shareholders funds',
'prof@NOP_to_TangNW',
'bs@TL_to_Capt',
'BvD_Net current assets']

for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)


list_neg = [0, 0, 0, 0, 0, 0, 'BvD_Proxy_Tangible_Net_Worth_inUSD', 
'BvD_Proxy_Capitalization_inUSD', 0]



#%% MFA
dat_norm = dat[list_final+['Applied Rating PDRR', 'year','BvD_Proxy_Tangible_Net_Worth_inUSD', 
'BvD_Proxy_Capitalization_inUSD']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection

writer = pd.ExcelWriter(f'EXP\\MM_AM+EU_MFA_modelselection_tran_ineg.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+['Applied Rating PDRR', 'year','BvD_Proxy_Tangible_Net_Worth_inUSD', 
'BvD_Proxy_Capitalization_inUSD']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(f'EXP\\MM_AM+EU_MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[2:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()






#%%
dicretization output

# -*- coding: utf-8 -*-
"""
Created on Thu May 21 11:39:13 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat_full = pd.read_pickle(r'EXP\dat_am_withRatios.pkl')

dat = dat_full.query('BvD_Proxy_Net_Sales_inUSD<=1e9')


#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    list_quality.append(len(df_tmp_dropna)/ len(dat))
    list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor])))

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'EXP/SFA_am_MM.xlsx')


#%% pick SD > 0.20
list_short = result.query('SomersD>=0.2 and DataQuality>=0.7').index.tolist()





# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)



# after manual selection
list_final=[
'MM_NOP_to_NS',
'new@COGSRatio',
'BvD_P/L for period [=Net income] ',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'BvD_Gross profit',
'new@GrossMarkup',
'bs@CL_to_Capt',
'BvD_Shareholders funds',
'liq@CA_to_TA']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(2,5,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [0, 0, 0, 0, 0, 'BvD_Gross profit', 'new@GrossMarkup',0,
 'BvD_Shareholders funds', 0]



#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'new@COGSRatio',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'bs@CL_to_Capt',
'liq@CA_to_TA'
]
for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)


list_neg = [0, 0, 0, 0, 0, 0, 0,
'BvD_Proxy_Capitalization_inUSD', 0, 0]



#%% MFA
dat_norm = dat[list_final+['Applied Rating PDRR','BvD_Proxy_Capitalization_inUSD']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection

writer = pd.ExcelWriter(f'EXP\\MM_am_MFA_modelselection_tran_ineg.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model



dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+['Applied Rating PDRR','BvD_Proxy_Capitalization_inUSD', 'year']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(f'EXP\\MM_am_MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[1:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()



#%% test model performance in specific time window
pl_model = [
['MM_quantscore', 'new@COGSRatio', 'BvD_P/L for period [=Net income] '],
['MM_NOP_to_NS', 'new@COGSRatio', 'BvD_P/L for period [=Net income] '],
['MM_quantscore', 'new@COGSRatio', 'BvD_P/L for period [=Net income] ', 'BvD_Gross profit'],
['MM_NOP_to_NS', 'new@COGSRatio', 'BvD_P/L for period [=Net income] ', 'BvD_Shareholders funds']
]


pl_wts = [
[0.2760324498451464, 0.32910693847868816, 0.39486061167616543],
[0.5490265572037817, 0.22192574077591296, 0.2290477020203053],
[0.24017217165783636, 0.1873908152898595, 0.29393683458372544, 0.2785001784685788],
[0.5149208809202723, 0.11615951364216957, 0.15103506868066563, 0.21788453675689246]
]

pl_desc = [
'3-factor_forced MM quant score',
'3-factor_no restriction',
'4-factor_forced MM quant score',
'4-factor_no restriction',
]

def sd_fun(data):
    return (SomersD(data['Applied Rating PDRR'], data['score']))

writer = pd.ExcelWriter(f'EXP\\SD_3year_MM_am.xlsx')
dat_20161718 = dat_norm.query('2016<=year<=2018')
for i, model_setting in enumerate(pl_model):
    pl_result=[]
    pl_result.append(model_setting)
    pl_result.append(pl_wts[i])

    df = dat_20161718.dropna(subset=model_setting+['Applied Rating PDRR', 'year'], how='any')
    df_all = dat_norm.dropna(subset=model_setting+['Applied Rating PDRR', 'year'], how='any')

    df['score'] = (pl_wts[i]*df[model_setting]).sum(axis=1)
    df_all['score'] = (pl_wts[i]*df_all[model_setting]).sum(axis=1)
    pl_result.append(SomersD(df_all['Applied Rating PDRR'], df_all['score']))
    pl_result.append(len(df_all))

    pl_result.append(SomersD(df['Applied Rating PDRR'], df['score']))
    pl_result.append(len(df))
    pl_result =pl_result + df.groupby(by='year').apply(sd_fun).sort_index().tolist()
    pl_result =pl_result + df['year'].value_counts().sort_index().tolist()

    df_result = pd.DataFrame()
    df_result['Stats'] = pl_result
    df_result.index = ['Model', 'Weights', 'SomersD_allyears', 'Size_allyears', 'SomersD_3years', 'Size_3years',
    'SomersD_2016', 'SomersD_2017','SomersD_2018', 'Size_Year 2016', 'Size_Year 2017', 'Size_Year 2018']


    df_result.loc[['Model', 'Weights', 'SomersD_allyears', 'Size_allyears', 'SomersD_3years', 'Size_3years',
    'SomersD_2016', 'Size_Year 2016', 'SomersD_2017','Size_Year 2017', 'SomersD_2018', 'Size_Year 2018']].to_excel(writer, pl_desc[i])
writer.save()


#%%
dicretization output

# -*- coding: utf-8 -*-
"""
Created on Thu May 21 11:39:13 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat_full = pd.read_pickle(r'EXP\dat_nam_withRatios.pkl')

dat = dat_full.query('BvD_Proxy_Net_Sales_inUSD<=1e9')


#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    list_quality.append(len(df_tmp_dropna)/ len(dat))
    list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor])))

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'EXP/SFA_nonam_MM.xlsx')


#%% pick SD > 0.17
list_short = result.query('SomersD>=0.17 and DataQuality>=0.7').index.tolist()





# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(np.abs(dat[list_short].corr()), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)



# after manual selection
list_final=[
'MM_TangNW_to_TA', 
'MM_NOP_to_NS',
'MM_TD_to_Capt',
'liq@CA_to_TL',
'BvD_P/L for period [=Net income] ',
'act@NS_to_CL',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_TD',
'size@TangNW_to_TA_exc_CA'] 


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,3,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [0, 0, 0, 0, 0, 0, 0, 0, 0]



#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'MM_TangNW_to_TA', 
'MM_NOP_to_NS',
'MM_TD_to_Capt',
'liq@CA_to_TL',
'act@NS_to_CL',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_TD',
'size@TangNW_to_TA_exc_CA'] 

for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)


list_neg = [0, 0, 0, 0, 0, 0, 0, 0, 0]



#%% MFA
dat_norm = dat[list_final+['Applied Rating PDRR']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection

writer = pd.ExcelWriter(f'EXP\\MM_nonam_MFA_modelselection_tran_ineg.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+['Applied Rating PDRR', 'year']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(f'EXP\\MM_nonam_MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[3:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()




#%% test model performance in specific time window
pl_model = [
['MM_quantscore', 'BvD_P/L for period [=Net income] ', 'act@NS_to_CL'],
['MM_TangNW_to_TA', 'MM_NOP_to_NS', 'MM_TD_to_Capt'],
['MM_quantscore', 'BvD_P/L for period [=Net income] ', 'act@NS_to_CL', 'liq@CA_exc_CL_to_CA'],
['MM_TangNW_to_TA', 'MM_NOP_to_NS', 'MM_TD_to_Capt', 'act@NS_to_CL']
]


pl_wts = [
[0.6688393678686652, 0.25122288271927945, 0.07993774941205531],
[0.3140555408651869, 0.49440358720272076, 0.19154087193209246],
[0.6673877348286364, 0.25101356864725605, 0.07884720162529875, 0.002751494898808766],
[0.26750934594307624, 0.4739852217430573, 0.1719935833624545, 0.08651184895141205]
]

pl_desc = [
'3-factor_forced MM quant score',
'3-factor_no restriction',
'4-factor_forced MM quant score',
'4-factor_no restriction',
]

def sd_fun(data):
    return (SomersD(data['Applied Rating PDRR'], data['score']))


writer = pd.ExcelWriter(f'EXP\\SD_3year_MM_nam.xlsx')
dat_20161718 = dat_norm.query('2016<=year<=2018')
for i, model_setting in enumerate(pl_model):
    pl_result=[]
    pl_result.append(model_setting)
    pl_result.append(pl_wts[i])

    df = dat_20161718.dropna(subset=model_setting+['Applied Rating PDRR', 'year'], how='any')
    df_all = dat_norm.dropna(subset=model_setting+['Applied Rating PDRR', 'year'], how='any')

    df['score'] = (pl_wts[i]*df[model_setting]).sum(axis=1)
    df_all['score'] = (pl_wts[i]*df_all[model_setting]).sum(axis=1)
    pl_result.append(SomersD(df_all['Applied Rating PDRR'], df_all['score']))
    pl_result.append(len(df_all))

    pl_result.append(SomersD(df['Applied Rating PDRR'], df['score']))
    pl_result.append(len(df))
    pl_result =pl_result + df.groupby(by='year').apply(sd_fun).sort_index().tolist()
    pl_result =pl_result + df['year'].value_counts().sort_index().tolist()

    df_result = pd.DataFrame()
    df_result['Stats'] = pl_result
    df_result.index = ['Model', 'Weights', 'SomersD_allyears', 'Size_allyears', 'SomersD_3years', 'Size_3years',
    'SomersD_2016', 'SomersD_2017','SomersD_2018', 'Size_Year 2016', 'Size_Year 2017', 'Size_Year 2018']


    df_result.loc[['Model', 'Weights', 'SomersD_allyears', 'Size_allyears', 'SomersD_3years', 'Size_3years',
    'SomersD_2016', 'Size_Year 2016', 'SomersD_2017','Size_Year 2017', 'SomersD_2018', 'Size_Year 2018']].to_excel(writer, pl_desc[i])
writer.save()




#%%
dicretization output

# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter

os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCARS 2008-2019_matches2.pkl')


#%% Drop defaulters
mask = dat['Applied Rating'].isin([83, 90, 101, 102])
dat = dat[~mask]

#%% Drop rating 2-3
mask = dat['Applied Rating']==23
dat = dat[~mask]


#%% tag Geo info
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286
]

list_EMEA_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

dat['Geo'] = 'Asia'
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(list_americas_office_code), 'Americas')
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(list_EMEA_office_code), 'EMEA')



#%% Cast integer rating to string
dat['BTMU Rating'] = dat['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat['BTMU Rating'].value_counts().sort_index() 
'''
1-0      533
2-0     7231
2-1      134
2-2      369
3-0    18852
3-1      837
3-2     1540
3-3     1474
4-0    23073
5-1    10455
5-2     5634
6-1     4883
6-2     6239
7-0    11167
8-1     5979
8-2     4955
Name: BTMU Rating, dtype: int64

'''
#dat.query('Geo=="Americas"')['BTMU Rating'].value_counts().sort_index() 
#dat.query('Geo=="EMEA"')['BTMU Rating'].value_counts().sort_index() 
#dat.query('Geo=="Asia"')['BTMU Rating'].value_counts().sort_index() 



#%% Map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['Applied Rating PDRR'] = dat['Applied Rating'].transform(lambda x: pd_mapping[x])



#%% convert to numeric value
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

gcar_converter(dat, curr, inplace=True)


#%% Choose the columns we need 
cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'Secondary Evaluation',
'Third Evaluation',
'BTMU Rating',
'Final Result of Evaluation based on Financial Substance Score',
'Applied Rating',
'Applied Rating PDRR',
'Match',
'Borrower Industry',
'Industry Model',
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

dat[cols].to_pickle(r'..\data\dat_2008-2019_cleaned.pkl')



# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner


df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned.pkl')



bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(dat, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


df_bvd['Fiscal year'].value_counts().sort_index()
'''
Out[20]: 
1984       1
1985       1
1986       1
1987       1
1988       1
1989       2
1990       4
1991       5
1992       7
1993      12
1994      22
1995      29
1996      32
1997      35
1998      47
1999      82
2000     132
2001     209
2002     336
2003     433
2004     471
2005     617
2006    1078
2007    1692
2008    2962
2009    6279
2010    6813
2011    7214
2012    7572
2013    8556
2014    9367
2015    8940
2016    9075
2017    8597
2018    6637
2019     250
2020       3
Name: BvD_Fiscal year, dtype: int64
'''

len(set(dat['Borrower CIF'].unique()))
# 23102

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(dat['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 10727




#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% merge
df_merged = pd.merge(dat, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_merged['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_merged['BvD_CIF']]
df_merged['inBvD'].value_counts()

'''
Out[11]: 
No     61244
Yes    45052
Name: inBvD, dtype: int64

'''
df_merged.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_BvD.pkl')


df_merged_3yr = df_merged.query('2016<=year<=2018')
df_merged_3yr.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2016-2018_cleaned_BvD.pkl')

# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np
import pickle
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
lmdb = 0.3
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']

dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2016-2018_cleaned_BvD.pkl')


cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']



#%% build CnI ratios
dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
# for invalid_neg
dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']


# MM new transformation
dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb



#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios
dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']



factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))



#%% build current tokyo model factors:
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']
# fill inf
for factor in curr:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))




dat['Other#1'] = dat['BvD_Cash flow']
dat.groupby('Borrower Type')['Other#1'].mean()
'''Borrower Type
Corporation (Major Company)    1.609592e+09
Corporation (Other Company)    5.704757e+07 ****
Name: Other#1, dtype: float64
'''
dat['Other#1'] = dat['Other#1'].mask(dat['Borrower Type']=="Corporation (Other Company)",  dat['Current Financial Data #1'])
dat.groupby('Borrower Type')['Other#1'].mean()
'''
Borrower Type
Corporation (Major Company)    1.609592e+09
Corporation (Other Company)    3.716528e+08 ****
Name: Other#1, dtype: float64
'''



dat['Other#3'] = dat['LC_UBEBITDA_to_IE']
dat.groupby('Borrower Type')['Other#3'].mean()
'''
Borrower Type
Corporation (Major Company)     425.914331
Corporation (Other Company)    4939.195581  ****
Name: Other#3, dtype: float64
'''
dat['Other#3'] = dat['Other#3'].mask(dat['Borrower Type']=="Corporation (Other Company)",  dat['Current Financial Data #3'])
dat.groupby('Borrower Type')['Other#3'].mean()
'''
Borrower Type
Corporation (Major Company)    425.914331
Corporation (Other Company)    469.930354   ****
Name: Other#3, dtype: float64
'''

dat['SP#3'] = dat['LC_UBEBITDA_to_IE']
dat.groupby('Borrower Type')['SP#3'].mean()
'''
Borrower Type
Corporation (Major Company)     425.914331  ****
Corporation (Other Company)    4939.195581  
Name: SP#3, dtype: float64
'''
dat['SP#3'] = dat['SP#3'].mask(dat['Borrower Type']=="Corporation (Major Company)",  dat['Current Financial Data #3'])
dat.groupby('Borrower Type')['SP#3'].mean()
'''
Borrower Type
Corporation (Major Company)    86309.951587 ****
Corporation (Other Company)     4939.195581
Name: SP#3, dtype: float64
'''



dat['Other#6'] = dat['BvD_Operating revenue (Turnover)'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['Other#6'] = dat['Other#6'].clip(
            np.nanmin(dat['Other#6'][dat['Other#6'] != -np.inf]), 
            np.nanmax(dat['Other#6'][dat['Other#6'] != np.inf]))

dat.groupby('Borrower Type')['Other#6'].mean()
'''
Borrower Type
Corporation (Major Company)    3.327465
Corporation (Other Company)    2.392267 ****
Name: Other#6, dtype: float64
'''
dat['Other#6'] = dat['Other#6'].mask(dat['Borrower Type']=="Corporation (Other Company)",  dat['Current Financial Data #6'])
dat.groupby('Borrower Type')['Other#6'].mean()
'''
Borrower Type
Corporation (Major Company)    3.327465
Corporation (Other Company)   -3.562748 ****  
Name: Other#6, dtype: float64
'''


dat['Other#10'] = dat['BvD_Proxy_Net_Sales_inUSD']
dat.groupby('Borrower Type')['Other#10'].mean()
'''Borrower Type
Corporation (Major Company)    1.077222e+10
Corporation (Other Company)    5.117881e+08 ****
Name: Other#10, dtype: float64
'''
dat['Other#10'] = dat['Other#10'].mask(dat['Borrower Type']=="Corporation (Other Company)",  dat['Current Financial Data #10'])
dat.groupby('Borrower Type')['Other#10'].mean()
'''
Borrower Type
Corporation (Major Company)    1.077222e+10
Corporation (Other Company)    2.885040e+09 ****
Name: Other#10, dtype: float64
'''

list_toremove = [
 'Other#1',
 'Other#6',
 'Other#10',
 'Other#3',
 'SP#3']

dat.drop(columns=list_toremove, inplace=True)


#%% rename current factor
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

for name in curr:
    dat['Other_'+name] = np.nan
    dat['SP_'+name] = np.nan

    dat['Other_'+name] = dat['Other_'+name].mask(dat['Borrower Type']=="Corporation (Other Company)", dat[name])
    dat['SP_'+name] = dat['SP_'+name].mask(dat['Borrower Type']!="Corporation (Other Company)", dat[name])


dat.drop(columns=curr, inplace=True)



#%% label geo info
dat = pd.concat([dat, pd.get_dummies(dat['Geo'], prefix='Geo')], axis=1)

#%% label tokyo model info
dat = pd.concat([dat, pd.get_dummies(dat['Borrower Type'], prefix='Tokyo')], axis=1)



#%% add portfolio info
dat['portfolio'] = 'unknown'
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']>1e9, 'LC')
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']<=1e9, 'MM')

dat['portfolio'].value_counts()
'''
unknown    17438
MM          9932
LC          1781

'''



#%% split
dat_LC = dat.query('portfolio=="LC"')
dat_MM = dat.query('portfolio=="MM"')



dat_LC.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready.pkl')
dat_MM.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready.pkl')# -*- coding: utf-8 -*-
"""
Created on Wed Jul  1 17:33:42 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1428
1     353
Name: Gua_override, dtype: int64
'''


dat = dat.query('Gua_override==0')

#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
    else:
        list_sd.append(np.nan)

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.23
list_short = result.query('SomersD>=0.23 and DataQuality>=0.6').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'LC_UBEBITDA_to_IE', 
'LC_Total Assets',
'LC_TD_to_Capt', 
'BvD_P/L for period [=Net income] ',
'BvD_Taxation',
'new@CashFlowRatio',
'new@GrossMarkup',
'MM_NOP_to_NS',
'bs@TL_to_TA',]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,4,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,4,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
0, 
0,
0, 
'BvD_P/L for period [=Net income] ',
'BvD_Taxation',
0,
'new@GrossMarkup',
0,
0,]



dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1
dat['new@GrossMarkup'] = dat['new@GrossMarkup'] + np.abs(dat['new@GrossMarkup'].min()) +1






#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'LC_UBEBITDA_to_IE', 
'LC_Total Assets',
'LC_TD_to_Capt', 
'new@CashFlowRatio',
'MM_NOP_to_NS',
'bs@TL_to_TA',]


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0, 0, 0, 0]

# add 'LC_UBEBITDA_to_IE'
list_neg = ['BvD_Proxy_Interest_expense_inUSD', 0, 0, 0, 0, 0, 0, 0, 0]





#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_noGua.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['LC_quantscore'] = dat_LC_norm['LC_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_noGua_keepLC.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[3:], factor_num):
        model = ['LC_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()




#%% test model performance in specific time window
pl_model = [
['LC_UBEBITDA_to_IE', 'BvD_P/L for period [=Net income] ', 'new@GrossProfitRatio', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'LC_Total Assets', 'MM_NOP_to_NS', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'new@GrossMarkup', 'MM_NOP_to_NS', 'bs@TL_to_TA', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'MM_NOP_to_NS', 'BvD_Current liabilities', 'bs@TL_to_TA', 'BvD_Number of employees'],

['LC_quantscore', 'MM_NOP_to_NS', 'ds@TL_to_IE', 'BvD_Number of employees'],
['LC_quantscore', 'BvD_P/L for period [=Net income] ', 'bs@TL_to_TA', 'new@GrossProfitRatio', 'BvD_Number of employees'],
['LC_quantscore', 'MM_NOP_to_NS', 'ds@TL_to_IE', 'bs@TL_to_TA', 'BvD_Number of employees'],
]


pl_wts = [
[0.3470943676883373, 0.3094237495903403, 0.2241067886496746, 0.11937509407164787],
[0.3028042522301452, 0.28754401464417806, 0.21049849683271868, 0.19915323629295809],
[0.2017635865470767, 0.11851394096645368, 0.25492115111394326, 0.1873349258684585, 0.2374663955040679],
[0.21452340344731483, 0.2579658156565938, 0.21283526809987038, 0.17506720142423865, 0.13960831137198226],

[0.3000239419148258, 0.27318194963235026, 0.1612281695398581, 0.2655659389129658],
[0.3082001561292678, 0.22668572659732553, 0.19389857177200756, 0.10350008941273288, 0.1677154560886661],
[0.16978881028899634, 0.255742355367358, 0.17453387707690923, 0.1480144791140035, 0.25192047815273294]
]

pl_desc = [
'4-factor_1',
'4-factor_2',
'5-factor_1',
'5-factor_2',
'LC+3-factor',
'LC+4-factor_1',
'LC+4-factor_2',
]


def calc_fun(list_dataset, list_dataset_name):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_new = dat.dropna(subset=model_setting, how='any')

        dat_new['score'] = (pl_wts[i]*dat_new[model_setting]).sum(axis=1)
        list_obs_newmodel.append(len(dat_new))
        if len(dat_new)>5:
            list_sd_newmodel.append(SomersD(dat_new['Applied Rating PDRR'], dat_new['score']))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)



#%% Size
df1 = dat_norm.query('Geo_Americas ==1')
df2 = dat_norm.query('Geo_EMEA ==1')
df3 = dat_norm.query('Geo_Asia ==1')


#%% Year
df4 = dat_norm.query('year==2016')
df5 = dat_norm.query('year==2017')
df6 = dat_norm.query('year==2018')

#%% model
df7 = dat_norm[dat_norm['Borrower Type']=='Corporation (Major Company)']
df8 = dat_norm[dat_norm['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(dat_norm['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(dat_norm[dat_norm['Industry Model']==ind])


list_dataset = [dat_norm, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name


writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelperf_original.xlsx')

for i, model_setting in enumerate(pl_model):

    result = calc_fun(list_dataset, list_dataset_name)
    result.to_excel(writer, pl_desc[i])

writer.save()







#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['LC_UBEBITDA_to_IE', 'LC_Total Assets', 'LC_TD_to_Capt', 'BvD_P/L for period [=Net income] ', 'MM_NOP_to_NS']
wts = [0.23174731768774995, 0.13753553928236661, 0.1613268684050183, 0.2142622172846372, 0.25512805734022803]

df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5325123445773098

bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun

optSD_wts = res1.x / res1.x.sum()
#  array([0.26935967, 0.19810441, 0.14292322, 0.18743158, 0.20218111])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5336082487138782






#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5644000237967756
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5333452317211018


score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5644000237967756 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(15,45), (15,45), (15,45)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5653268455808449

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5653268455808449



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= df, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()

df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5653268455808449
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5365233537171503






CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'round', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))




y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5736087496986264

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5524421440894759

df['iso'] = y_
df['iso_round'] = y_round






#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name





def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:

   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\LC\result_noGua.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1428
1     353
Name: Gua_override, dtype: int64
'''
dat = dat.query('Gua_override==0')
dat.reset_index(drop=True, inplace=True)


#%%
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])

dat = dat.iloc[df_temp.query('diff<=4').index,]
dat.reset_index(drop=True, inplace=True)

SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7243048491819798
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7301408184890371

from minepy import MINE
mine = MINE(alpha=0.6, c=15) 



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
list_mic=[]

for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        mine.compute_score(df_tmp_dropna[factor],df_tmp_dropna['Applied Rating PDRR']) 
        list_mic.append(mine.mic() )
    else:
        list_sd.append(np.nan)
        list_mic.append(np.nan)

result['SomersD'] = list_sd
result['MIC'] = list_mic
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)

result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.23
list_short = result.query('SomersD>=0.23 and DataQuality>=0.7').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'LC_Total Assets',
'LC_TD_to_Capt', 
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'new@CashFlowRatio',
'bs@TL_to_TA',
'MM_NOP_to_NS',
'bs@TD_to_TA_exc_TL',
'BvD_Net current assets', # from MIC
'BvD_Working capital'  # from MIC
]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')



for i, factor in enumerate(list_final): 
    tmp = dat_norm[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')




for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(2,5,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
0, 
0,
'BvD_P/L for period [=Net income] ',
'BvD_Taxation',
0,
0,
0,
0,
'BvD_Net current assets', # from MIC
'BvD_Working capital']



dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1
dat['BvD_Net current assets'] = dat['BvD_Net current assets'] + np.abs(dat['BvD_Net current assets'].min()) +1
dat['BvD_Working capital'] = dat['BvD_Working capital'] + np.abs(dat['BvD_Working capital'].min()) +1





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'LC_TD_to_Capt', 
'new@CashFlowRatio',
'bs@TL_to_TA',
'MM_NOP_to_NS',
'bs@TD_to_TA_exc_TL']


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0, 0, 0, 0]

# add 'LC_TD_to_Capt', 
list_neg = [0,'BvD_Proxy_Capitalization_inUSD', 0, 0, 0, 0, 0, 0,0,0]





#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_noGua_nooutliers.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['LC_quantscore'] = dat_LC_norm['LC_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_noGua_nooutliers_keepLC.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[2:], factor_num):
        model = ['LC_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = [
'LC_Total Assets',
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'new@CashFlowRatio',
'MM_NOP_to_NS',
]

wts = [0.23174731768774995, 0.13753553928236661, 0.1613268684050183, 0.2142622172846372, 0.25512805734022803]

df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5325123445773098

bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun

optSD_wts = res1.x / res1.x.sum()
#  array([0.26935967, 0.19810441, 0.14292322, 0.18743158, 0.20218111])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5336082487138782






#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5644000237967756
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5333452317211018


score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5644000237967756 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(15,45), (15,45), (15,45)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5653268455808449

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5653268455808449



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= df, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()

df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5653268455808449
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5365233537171503






CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'round', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))




y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5736087496986264

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5524421440894759

df['iso'] = y_
df['iso_round'] = y_round






#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name





def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:

   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\LC\result_noGua.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Wed Jul  1 17:33:42 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1428
1     353
Name: Gua_override, dtype: int64
'''




#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
    else:
        list_sd.append(np.nan)

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.20
list_short = result.query('SomersD>=0.2 and DataQuality>=0.6').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'LC_UBEBITDA_to_IE', 
'LC_UBEBITDA_to_NS',
'LC_Total Assets',
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'new@GrossMarkup',
'MM_NOP_to_NS',
'BvD_Current liabilities',
'ds@TL_to_IE',
'bs@TL_to_TA',
'new@GrossProfitRatio',
'BvD_Number of employees']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,4,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,4,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [0, 0, 0,'BvD_P/L for period [=Net income] ', 'BvD_Taxation',  'new@GrossMarkup', 0,
 'BvD_Current liabilities', 0,0,0,0]


dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'LC_UBEBITDA_to_IE', 
'LC_UBEBITDA_to_NS',
'MM_NOP_to_NS',
'ds@TL_to_IE',
'bs@TL_to_TA',
'new@GrossProfitRatio',
]
for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0, 'ds@TL_to_IE', 0, 0]

# add 'LC_UBEBITDA_to_IE'
list_neg = ['BvD_Proxy_Interest_expense_inUSD', 0, 0, 0, 0, 0, 0,
 0, 'BvD_Proxy_Interest_expense_inUSD', 0, 0, 0]





#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'Cty_Rating_Label'
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['LC_quantscore'] = dat_LC_norm['LC_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_keepLC.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[3:], factor_num):
        model = ['LC_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()




#%% test model performance in specific time window
pl_model = [
['LC_UBEBITDA_to_IE', 'BvD_P/L for period [=Net income] ', 'new@GrossProfitRatio', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'LC_Total Assets', 'MM_NOP_to_NS', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'new@GrossMarkup', 'MM_NOP_to_NS', 'bs@TL_to_TA', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'MM_NOP_to_NS', 'BvD_Current liabilities', 'bs@TL_to_TA', 'BvD_Number of employees'],

['LC_quantscore', 'MM_NOP_to_NS', 'ds@TL_to_IE', 'BvD_Number of employees'],
['LC_quantscore', 'BvD_P/L for period [=Net income] ', 'bs@TL_to_TA', 'new@GrossProfitRatio', 'BvD_Number of employees'],
['LC_quantscore', 'MM_NOP_to_NS', 'ds@TL_to_IE', 'bs@TL_to_TA', 'BvD_Number of employees'],
]


pl_wts = [
[0.3470943676883373, 0.3094237495903403, 0.2241067886496746, 0.11937509407164787],
[0.3028042522301452, 0.28754401464417806, 0.21049849683271868, 0.19915323629295809],
[0.2017635865470767, 0.11851394096645368, 0.25492115111394326, 0.1873349258684585, 0.2374663955040679],
[0.21452340344731483, 0.2579658156565938, 0.21283526809987038, 0.17506720142423865, 0.13960831137198226],

[0.3000239419148258, 0.27318194963235026, 0.1612281695398581, 0.2655659389129658],
[0.3082001561292678, 0.22668572659732553, 0.19389857177200756, 0.10350008941273288, 0.1677154560886661],
[0.16978881028899634, 0.255742355367358, 0.17453387707690923, 0.1480144791140035, 0.25192047815273294]
]

pl_desc = [
'4-factor_1',
'4-factor_2',
'5-factor_1',
'5-factor_2',
'LC+3-factor',
'LC+4-factor_1',
'LC+4-factor_2',
]


def calc_fun(list_dataset, list_dataset_name):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_new = dat.dropna(subset=model_setting, how='any')

        dat_new['score'] = (pl_wts[i]*dat_new[model_setting]).sum(axis=1)
        list_obs_newmodel.append(len(dat_new))
        if len(dat_new)>5:
            list_sd_newmodel.append(SomersD(dat_new['Applied Rating PDRR'], dat_new['score']))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)



#%% Size
df1 = dat_norm.query('Geo_Americas ==1')
df2 = dat_norm.query('Geo_EMEA ==1')
df3 = dat_norm.query('Geo_Asia ==1')


#%% Year
df4 = dat_norm.query('year==2016')
df5 = dat_norm.query('year==2017')
df6 = dat_norm.query('year==2018')

#%% model
df7 = dat_norm[dat_norm['Borrower Type']=='Corporation (Major Company)']
df8 = dat_norm[dat_norm['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(dat_norm['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(dat_norm[dat_norm['Industry Model']==ind])


list_dataset = [dat_norm, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name


writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelperf_original.xlsx')

for i, model_setting in enumerate(pl_model):

    result = calc_fun(list_dataset, list_dataset_name)
    result.to_excel(writer, pl_desc[i])

writer.save()







#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['LC_UBEBITDA_to_IE', 'new@GrossMarkup', 'MM_NOP_to_NS', 'bs@TL_to_TA', 'BvD_Number of employees']
wts = [0.2017635865470767, 0.11851394096645368, 0.25492115111394326, 0.1873349258684585, 0.2374663955040679]
df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5500012719733395

bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun

optSD_wts = res1.x / res1.x.sum()
#  array([0.21192067, 0.16478885, 0.22968535, 0.1883884 , 0.20521674])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5516548373146098






#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5545549365285304
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5325370780228447


score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5545549365285304 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(15,45), (15,45), (15,45)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5580401434785927

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5580401434785927



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= df, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()
df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5545549365285304
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5325370780228447







CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'round', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))




y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5704418835381211

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5513241242463558

df['iso'] = y_
df['iso_round'] = y_round






#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name





def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:

   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\LC\result.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Wed Jul  1 17:33:42 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
Out[201]: 
0    6027
1    3905
Name: Gua_override, dtype: int64
'''



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
    else:
        list_sd.append(np.nan)

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'SFAMFA\MM\SFA.xlsx')


#%% pick SD > 0.20
list_short = result.query('SomersD>=0.2 and DataQuality>=0.6').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'MM_NOP_to_NS', 
'MM_TangNW_to_TA', 
'Other_Current Financial Data #1',
'Other_Current Financial Data #6',
'Other_Current Financial Data #7',
'Other_Current Financial Data #8',
'Other_Current Financial Data #9',
'Other_Current Financial Data #10',
'BvD_P/L for period [=Net income] ',
'BvD_Taxation',
'prof@NOP_to_TA',
'BvD_Gross profit',]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,4,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,4,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [0, 0, 'Other_Current Financial Data #1',0,0,0,
'Other_Current Financial Data #9','Other_Current Financial Data #10','BvD_P/L for period [=Net income] ',
'BvD_Taxation',0,'BvD_Gross profit']


dat['Other_Current Financial Data #1'] = dat['Other_Current Financial Data #1'] + np.abs(dat['Other_Current Financial Data #1'].min()) +1
dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'MM_NOP_to_NS', 
'MM_TangNW_to_TA',
'prof@NOP_to_TA'
]
for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0]


list_neg = [0,]*12





#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'Gua_override',
'Cty_Rating_Label',
'Cty_Rating_Label2',
'Cty_Rating_Label3',
'Cty_Rating_Label4'
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\MM\MFA_modelselection.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'Gua_override',
'Cty_Rating_Label',
'Cty_Rating_Label2',
'Cty_Rating_Label3',
'Cty_Rating_Label4'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\MM\MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[3:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()




#%% test model performance in specific time window
pl_model = [
['MM_TangNW_to_TA', 'Other_Current Financial Data #7', 'Other_Current Financial Data #9', 'prof@NOP_to_TA'],
['MM_TangNW_to_TA', 'Other_Current Financial Data #7', 'Other_Current Financial Data #8', 'Other_Current Financial Data #9', 'prof@NOP_to_TA'],
['MM_quantscore', 'Other_Current Financial Data #7', 'Other_Current Financial Data #8', 'Other_Current Financial Data #9'],
['MM_quantscore', 'Other_Current Financial Data #6', 'Other_Current Financial Data #7', 'Other_Current Financial Data #9', 'Other_Current Financial Data #10']
]


pl_wts = [
[0.08863298057963558, 0.45066161689225986, 0.429129491329704, 0.031575911198400475],
[0.09033936630467564, 0.3772702389878451, 0.08327461539905458, 0.41327286706749405, 0.035842912240930704],
[0.13411697294300234, 0.37033066086384886, 0.06991838722806433, 0.4256339789650845],
[0.16274477658990186, 0.1327840853072716, 0.30059737188942964, 0.21778194951020405, 0.18609181670319294]
]

pl_desc = [
'4-factor',
'5-factor',
'MM+3-factor',
'MM+4-factor',
]


def calc_fun(list_dataset, list_dataset_name):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_new = dat.dropna(subset=model_setting, how='any')

        dat_new['score'] = (pl_wts[i]*dat_new[model_setting]).sum(axis=1)
        list_obs_newmodel.append(len(dat_new))
        if len(dat_new)>5:
            list_sd_newmodel.append(SomersD(dat_new['Applied Rating PDRR'], dat_new['score']))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)



#%% Size
df1 = dat_norm.query('Geo_Americas ==1')
df2 = dat_norm.query('Geo_EMEA ==1')
df3 = dat_norm.query('Geo_Asia ==1')


#%% Year
df4 = dat_norm.query('year==2016')
df5 = dat_norm.query('year==2017')
df6 = dat_norm.query('year==2018')

#%% model
df7 = dat_norm[dat_norm['Borrower Type']=='Corporation (Major Company)']
df8 = dat_norm[dat_norm['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(dat_norm['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(dat_norm[dat_norm['Industry Model']==ind])


list_dataset = [dat_norm, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name


writer = pd.ExcelWriter(r'SFAMFA\MM\MFA_modelperf_original.xlsx')

for i, model_setting in enumerate(pl_model):

    result = calc_fun(list_dataset, list_dataset_name)
    result.to_excel(writer, pl_desc[i])

writer.save()







#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['MM_quantscore', 'Other_Current Financial Data #6', 'Other_Current Financial Data #7', 'Other_Current Financial Data #9', 'Other_Current Financial Data #10']

wts = [0.16274477658990186, 0.1327840853072716, 0.30059737188942964, 0.21778194951020405, 0.18609181670319294]
df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5813386858932407

bounds1 = [(0.1,0.5), (0.1,0.5), (0.1,0.5), (0.1,0.5),(0.1,0.5)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun

optSD_wts = res1.x / res1.x.sum()
#  array([0.19127431, 0.11132909, 0.25876922, 0.23658854, 0.20203884])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
#  0.5825335980383117






#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.481
Model:                             OLS   Adj. R-squared:                  0.481
Method:                  Least Squares   F-statistic:                     1340.
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):               0.00
Time:                         15:53:37   Log-Likelihood:                -9871.1
No. Observations:                 4343   AIC:                         1.975e+04
Df Residuals:                     4339   BIC:                         1.978e+04
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            7.0752      0.079     89.383      0.000       6.920       7.230
score            0.0645      0.001     60.417      0.000       0.062       0.067
Geo_Americas     3.4234      0.224     15.268      0.000       2.984       3.863
Geo_Asia         1.7993      0.084     21.345      0.000       1.634       1.965
Geo_EMEA         1.8525      0.111     16.645      0.000       1.634       2.071
==============================================================================
Omnibus:                       59.991   Durbin-Watson:                   1.798
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               98.353
Skew:                           0.108   Prob(JB):                     4.39e-22
Kurtosis:                       3.705   Cond. No.                     8.58e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 7.45e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""

'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# .5870772559834265
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5713107603347456


score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5870772559834265 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(10,100), (10,100), (10,100)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5879435866320977

score_Americas, score_EMEA, score_Asia = res1.x
# array([66.51779736, 24.97172697, 23.25744774])
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5879435866320977


sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= df, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()
df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj'].mean()

df_plot.plot()
df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.587901288857051
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# .5723921906443253









y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5918184754876011

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5791968452049734

df['iso'] = y_
df['iso_round'] = y_round






#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry

list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name





def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:

   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)



result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\MM\result.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))





#%% DEMO
df_demo = pd.concat([df.query('Geo=="Asia"').sample(63),
    df.query('Geo=="Americas"'),
    df.query('Geo=="EMEA"').sample(63)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\MM\demo_MM.png")



# -*- coding: utf-8 -*-
"""
Created on Tue Sep  1 14:12:27 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
#dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered3.pkl')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCARS 2008-2019_matches3.pkl')


#%% Drop defaulters
mask = dat['Applied Rating'].isin([83, 90, 101, 102])
dat = dat[~mask].reset_index(drop=True)

# Drop rating 2-3
mask = dat['Applied Rating']==23
dat = dat[~mask].reset_index(drop=True)



#%% Drop Borrowers who have unique "CIF" but multiple(>=3) "Names"
tmp = dat.groupby('Borrower_CIF_cleaned')['Borrower Name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts(normalize=True)
'''
1    0.657148
2    0.255707
3    0.063053
4    0.017100
5    0.005476
6    0.001011
7    0.000421
9    0.000042
8    0.000042
Name: Num_CusName, dtype: float64

'''

list_toremove = tmp[tmp.Num_CusName>=3].index.tolist()
mask = dat.Borrower_CIF_cleaned.isin(list_toremove)
dat = dat[~mask].reset_index(drop=True)
# 102200



#%% tag Geo info
dat['Geo'] = 'Asia'
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(gcar_Americas_office_code), 'Americas')
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(gcar_EMEA_office_code), 'EMEA')

dat['Geo'].value_counts()
'''
Asia        57698
Americas    23118
EMEA        21384
Name: Geo, dtype: int64

'''



#%% Cast integer rating to string
dat['BTMU Rating'] = dat['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat['BTMU Rating'].value_counts().sort_index() 
'''
1-0      564
2-0     7238
2-1      135
2-2      338
3-0    19273
3-1      776
3-2     1440
3-3     1442
4-0    24528
5-1    10509
5-2     5434
6-1     4782
6-2     5764
7-0     9947
8-1     5499
8-2     4531
Name: BTMU Rating, dtype: int64
'''

#dat.query('Geo=="Americas"')['BTMU Rating'].value_counts().sort_index() 
#dat.query('Geo=="EMEA"')['BTMU Rating'].value_counts().sort_index() 
#dat.query('Geo=="Asia"')['BTMU Rating'].value_counts().sort_index() 



#%% Map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['Applied Rating PDRR'] = dat['Applied Rating'].transform(lambda x: pd_mapping[x])



#%% convert to numeric value
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

gcar_converter(dat, curr, inplace=True)


#%% Choose the columns we need 
'''
cols=[
'Data From',
'Application No.',
'Borrower CIF',
'Borrower_CIF_cleaned',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'Secondary Evaluation',
'Third Evaluation',
'BTMU Rating',
'Final Result of Evaluation based on Financial Substance Score',
'Applied Rating',
'Applied Rating PDRR',
'Borrower Industry',
'Industry Model',
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

dat[cols].to_pickle(r'..\data\dat_2008-2019_cleaned_20200901.pkl')

'''
dat.to_pickle(r'..\data\dat_2008-2019_cleaned_20200903.pkl')
# -*- coding: utf-8 -*-
"""
Created on Fri Jul 31 12:26:29 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner


df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_20200903.pkl')


#%%Check Browers who have unique "CIF" but multiple(>=3) "Names"
df_bvd['CIF_cleaned'] = gcar_cif_cleaner(df_bvd, col='CIF')
tmp = df_bvd.groupby('CIF_cleaned')['Company name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts(normalize=True)
'''
1    0.959053
2    0.031076
3    0.005393
4    0.002194
5    0.000914
6    0.000731
7    0.000366
8    0.000274
Name: Num_CusName, dtype: float64
'''




bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


df_bvd['Fiscal year'].value_counts().sort_index()
'''
Out[20]: 
1984       1
1985       1
1986       1
1987       1
1988       1
1989       2
1990       4
1991       5
1992       7
1993      12
1994      22
1995      29
1996      32
1997      35
1998      47
1999      82
2000     132
2001     209
2002     336
2003     433
2004     471
2005     617
2006    1078
2007    1692
2008    2962
2009    6279
2010    6813
2011    7214
2012    7572
2013    8556
2014    9367
2015    8940
2016    9075
2017    8597
2018    6637
2019     250
2020       3
Name: Fiscal year, dtype: int64

'''

dat['year-1'] = dat['year']-1

len(set(dat['Borrower_CIF_cleaned'].unique()))
# 21673

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(dat['Borrower_CIF_cleaned'].unique())&set(df_bvd['CIF'].unique()))
# 9362




#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% relink: Year (N-1)'s BvD links to Year N's GCARS data
df_merged = pd.merge(dat, df_bvd, left_on=['Borrower_CIF_cleaned','year-1'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_merged['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_merged['BvD_CIF']]
df_merged['inBvD'].value_counts()

'''
No     60712
Yes    42334
Name: inBvD, dtype: int64
'''




#%% Drop Browers who have unique "BvD_CIF" but multiple(>=3) "Names"
tmp = df_merged.groupby('BvD_CIF')['BvD_Company name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts().sort_index()
'''
0    2495
1    5646
2     113
3       7
Name: Num_CusName, dtype: int64

'''

list_toremove = tmp[tmp.Num_CusName>=3].index.tolist()
mask = df_merged.BvD_CIF.isin(list_toremove)
df_merged = df_merged[~mask].reset_index(drop=True)
# 102899



df_merged.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_BvD_relink.pkl')


df_merged_3yr = df_merged.query('2017<=year<=2019').reset_index(drop=True)
'''
Out[58]: 
No     18063
Yes    13819
Name: inBvD, dtype: int64
'''

df_merged_3yr.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2017-2019_cleaned_BvD_relink.pkl')


# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np
import pickle
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
lmdb = 0.3
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']

#dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2016-2018_cleaned_BvD_relink.pkl')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2017-2019_cleaned_BvD_relink.pkl')

cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']



#%% build CnI ratios
dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
# for invalid_neg
dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']


# MM new transformation
dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb



#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios
dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']



factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))



#%% build current tokyo model factors:
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']
# fill inf
for factor in curr:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))


# rename current factor

for name in curr:
    dat['Other_'+name] = np.nan
    dat['SP_'+name] = np.nan

    dat['Other_'+name] = dat['Other_'+name].mask(dat['Borrower Type']=="Corporation (Other Company)", dat[name])
    dat['SP_'+name] = dat['SP_'+name].mask(dat['Borrower Type']!="Corporation (Other Company)", dat[name])

dat.drop(columns=curr, inplace=True)


#%% label geo info
dat = pd.concat([dat, pd.get_dummies(dat['Geo'], prefix='Geo')], axis=1)

#%% label tokyo model info
dat = pd.concat([dat, pd.get_dummies(dat['Borrower Type'], prefix='Tokyo')], axis=1)




#%% add portfolio info
dat['portfolio'] = 'unknown'
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']>1e9, 'LC')
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']<=1e9, 'MM')

dat['portfolio'].value_counts()
'''
unknown    20275
MM         10190
LC          1417
Name: portfolio, dtype: int64
'''



#%% split
dat_LC = dat.query('portfolio=="LC"').reset_index(drop=True)
dat_MM = dat.query('portfolio=="MM"').reset_index(drop=True)



dat_LC.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat_MM.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression
from scipy.optimize import differential_evolution

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat_def = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink_pseudodef.pkl')
dat = pd.concat([dat, dat_def], axis=0)
dat.dropna(subset=['Primary Evaluation'], inplace=True)


dat['Applied Rating'].value_counts().sort_index()
'''
10      32
20     266
21       2
22       7
30     513
31       7
32      25
33      23
40     170
51     124
52      56
61      33
62      20
70      36
81       4
82      16
83       1
90       5
102      2
Name: Applied Rating, dtype: int64
'''

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(dat['Applied Rating']>=81, 1)

assert dat.def_flag.sum() == (dat['Applied Rating']>=81).sum()

print(dat.def_flag.sum())
# 28


#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    979
1    363
Name: Gua_override, dtype: int64
'''

# Use Secondary as Final for all Guarantor impacted obs
mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,102:82})
dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


# show the discrpency in different rating
SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7033062175919319
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7058687864416892





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd





df_EMEA = dat_LC_norm.query('Geo=="EMEA"').reset_index(drop=True)
df_Asia = dat_LC_norm.query('Geo=="Asia"').reset_index(drop=True)
df_Americas = dat_LC_norm.query('Geo=="Americas"').reset_index(drop=True)
# remove 2-0 and 3-0 record in Americas portfolio
mask = df_Americas['Applied Rating'].isin([20,30])
df_Americas = df_Americas[~mask].reset_index(drop=True)





dict_NonAM_BTMU_to_ylabel={
10:1, 20:2, 30:3, 40:4, 51:5, 52:6, 61:7, 62:8, 70:9, 81:10, 82:11, 83:12, 90:13}
dict_NonAM_ylabel_to_BTMU={
1:10, 2:20, 3:30, 4:40, 5:51, 6:52, 7:61, 8:62, 9:70, 10:81, 11:82, 12:83, 13:90}

df_Asia['y_label'] = df_Asia['Applied Rating'].replace(dict_NonAM_BTMU_to_ylabel)
df_EMEA['y_label'] = df_EMEA['Applied Rating'].replace(dict_NonAM_BTMU_to_ylabel)


dict_AM_BTMU_to_ylabel={
10:1, 21:2, 22:3, 31:4, 32:5, 33:6, 40:7, 51:8, 
52:9, 61:10, 62:11, 70:12, 81:13, 82:14}
dict_AM_ylabel_to_BTMU={
1:10, 2:21, 3:22, 4:31, 5:32, 6:33, 7:40, 8:51, 
9:52, 10:61, 11:62, 12:70, 13:81, 14:82}

df_Americas['y_label'] = df_Americas['Applied Rating'].replace(dict_AM_BTMU_to_ylabel)


#%% define func

def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))


def valuefunc_matchrate(calib):

    df['y_est'] = calib[0] + calib[1]*df['LC_quant_score']
    df['y_est'] = round(df['y_est'])
    return(-(df['y_label']==df['y_est']).sum()/N)


def apply_calib(df, calib, col='y_est'):

    df[col] = calib[0] + calib[1]*df['LC_quant_score']
    df[col] = round(df[col])
    return(df)



#%% Asia
df = df_Asia
N= len(df)
df['y_label'].value_counts().sort_index()


# naive linear regression
x_train = sm.add_constant(df['LC_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             3.249108
LC_quant_score    0.010536
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
#Out[108]: 0.2882882882882883

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
#Out[109]: 0.9009009009009009


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-10,4), (0,0.1)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
#Out[113]: -0.5945945945945946

res1.x
#Out[114]: array([2.59850132, 0.00619815])

df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
#Out[115]: 0.5945945945945946

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
#Out[116]: 0.8918918918918919

fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo2312.png")

# Convert to BTMU for future overall SomersD Calculation
df_Asia['BTMU_est_lr'] = df_Asia['y_est_lr'].replace(dict_NonAM_ylabel_to_BTMU)
df_Asia['BTMU_est_maxmatch'] = df_Asia['y_est_maxmatch'].replace(dict_NonAM_ylabel_to_BTMU)

# check
match_rate(df_Asia, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_Asia, col_a='Applied Rating', col_b='BTMU_est_maxmatch')












#%% EMEA
df = df_EMEA
N= len(df)
df['y_label'].value_counts().sort_index()


# naive linear regression
x_train = sm.add_constant(df['LC_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             4.930388
LC_quant_score    0.028169
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[121]: 0.2762096774193548

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[122]: 0.8991935483870968


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-5,10), (0,0.5)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[123]: -0.4637096774193548

res1.x
# Out[124]: array([3.10407701, 0.00522776])

df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
# Out[125]: 0.4637096774193548

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[126]: 0.8225806451612904

fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo2312.png")

# Convert to BTMU for future overall SomersD Calculation
df_EMEA['BTMU_est_lr'] = df_EMEA['y_est_lr'].replace(dict_NonAM_ylabel_to_BTMU)
df_EMEA['BTMU_est_maxmatch'] = df_EMEA['y_est_maxmatch'].replace(dict_NonAM_ylabel_to_BTMU)

# check
match_rate(df_EMEA, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_EMEA, col_a='Applied Rating', col_b='BTMU_est_maxmatch')















#%% Americas
df = df_Americas
N= len(df)
df['y_label'].value_counts().sort_index()


# naive linear regression
x_train = sm.add_constant(df['LC_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             7.470425
LC_quant_score    0.032013
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[132]: 0.2542372881355932

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[133]: 0.8135593220338984


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,15,15))#
ax.set_yticklabels(['1-0','2-1', '2-2','3-1','3-2','3-3','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-20,20), (0,1)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[134]: -0.3050847457627119

res1.x
# Out[135]: array([6.03133924, 0.02779509])

df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
#Out[136]: 0.3050847457627119

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[137]: 0.7966101694915254


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,15,15))#
ax.set_yticklabels(['1-0','2-1', '2-2','3-1','3-2','3-3','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo2312.png")

# Convert to BTMU for future overall SomersD Calculation
df_Americas['BTMU_est_lr'] = df_Americas['y_est_lr'].replace(dict_AM_ylabel_to_BTMU)
df_Americas['BTMU_est_maxmatch'] = df_Americas['y_est_maxmatch'].replace(dict_AM_ylabel_to_BTMU)

# check
match_rate(df_Americas, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_Americas, col_a='Applied Rating', col_b='BTMU_est_maxmatch')







df = pd.concat([df_Americas, df_Asia, df_EMEA], axis=0)

#%% Geo 
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2017')
df5 = df.query('year==2018')
df6 = df.query('year==2019')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']





list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2017','2018','2019','Major','Other']#+list_dataset_name

def calc_fun_fitted(list_dataset, list_dataset_name):

    list_obs_newmodel = []

    list_sd_lr = []
    list_mr_lr = []
    list_w2r_lr = []

    list_sd_maxmatch = []
    list_mr_maxmatch = []
    list_w2r_maxmatch = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))

        list_sd_lr.append(SomersD(dat['Applied Rating'], dat['BTMU_est_lr']))
        list_sd_maxmatch.append(SomersD(dat['Applied Rating'], dat['BTMU_est_maxmatch']))

        list_mr_lr.append(match_rate(dat, 'y_label', 'y_est_lr'))
        list_mr_maxmatch.append(match_rate(dat, 'y_label', 'y_est_maxmatch'))

        list_w2r_lr.append(within_k_rate(dat, 'y_label', 'y_est_lr', 2))
        list_w2r_maxmatch.append(within_k_rate(dat, 'y_label', 'y_est_maxmatch', 2))


        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['LCModel_size'] = list_obs_newmodel

    result['LR_SD'] = list_sd_lr
    result['LR_MatchRate'] = list_mr_lr
    result['LR_Within2Rate'] = list_w2r_lr

    result['MaxMatch_SD'] = list_sd_maxmatch
    result['MaxMatch_MatchRate'] = list_mr_maxmatch
    result['MaxMatch_Within2Rate'] = list_w2r_maxmatch

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)







result = calc_fun_fitted(list_dataset, list_dataset_name)
result.to_excel(r'SFAMFA\LC\af_Geo_calib.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='Applied Rating PDRR', data= dat_LC_norm, hue="Geo", alpha=0.9, palette=google_color[:3])

ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('Mapped to PDRR')#




ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
x_ = [-120, 120]
y_ = [model_LC.intercept1 + model_LC.slope1*x for x in x_]
ax2.plot(x_, y_, 'k-')
#ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
#ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")






low_pd = MS['new_low'].to_list()









def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'
    elif y_type=='LC':
        y_pred = 'LC_PDRR'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['NewModel_MatchRate'] = list_mr_newmodel
    result['NewModel_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)





#%% pseudo def
df['def_flag'].sum()
# 17



SomersD(df['def_flag'], df['BTMU_est_lr'])
# Out[158]: 0.7042206713193752

SomersD(df['def_flag'], df['BTMU_est_maxmatch'])
# Out[159]: 0.27558989697573943

SomersD(df['def_flag'], df['PrimaryEvaluationPDRR'])
# 0.7133899843247257

SomersD(df['def_flag'], df['Primary Evaluation'])
# 0.6977972114512004

# trivial 
SomersD(df['def_flag'], df['Applied Rating'])
# .9062783598712978


# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat_def = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink_pseudodef.pkl')
dat = pd.concat([dat, dat_def], axis=0)
dat.dropna(subset=['Primary Evaluation'], inplace=True)


dat['Applied Rating'].value_counts().sort_index()
'''
10      32
20     266
21       2
22       7
30     513
31       7
32      25
33      23
40     170
51     124
52      56
61      33
62      20
70      36
81       4
82      16
83       1
90       5
102      2
Name: Applied Rating, dtype: int64
'''

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(dat['Applied Rating']>=81, 1)

assert dat.def_flag.sum() == (dat['Applied Rating']>=81).sum()

print(dat.def_flag.sum())
# 28


#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    979
1    363
Name: Gua_override, dtype: int64
'''

# Use Secondary as Final for all Guarantor impacted obs
mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,102:82})
dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


# show the discrpency in different rating
SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7033062175919319
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7058687864416892





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['LC_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['LC_PD'] = list_PD
    dat['LC_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_LC_norm = get_proxy_rating(dat_LC_norm, model_LC)






def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))



df = dat_LC_norm.copy()

#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2017')
df5 = df.query('year==2018')
df6 = df.query('year==2019')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
#list_dataset_name = list(df['Industry Model'].value_counts().index)
#list_dataset=[]
#for ind in list_dataset_name:
#    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2017','2018','2019','Major','Other']#+list_dataset_name



def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'
    elif y_type=='LC':
        y_pred = 'LC_PDRR'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['LC Model_size'] = list_obs_newmodel
    result['LC Model_SD'] = list_sd_newmodel
    result['LC Model_MatchRate'] = list_mr_newmodel
    result['LC Model_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name, y_type='LC')


result.to_excel(r'SFAMFA\LC\result_bf_calib.xlsx')




#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by CnI LC Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='LC_quant_score', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\LC_score_vs_BTMURating.png")








#%% pseudo def
df['def_flag'].sum()
# 17
df['def_flag'].sum()/len(df)
# 0.023448275862068966

SomersD(df['def_flag'], df['LC_PDRR'])
# 0.7066248659351538

SomersD(df['def_flag'], df['PrimaryEvaluationPDRR'])
# 0.7133899843247257

SomersD(df['def_flag'], df['Primary Evaluation'])
# 0.6977972114512004

# trivial 
SomersD(df['def_flag'], df['Applied Rating'])
# .9062783598712978



df.query('def_flag==1')['PrimaryEvaluationPDRR'].value_counts().sort_index()
'''
6     3
8     3
9     2
10    2
11    1
12    2
13    3
14    1
Name: PrimaryEvaluationPDRR, dtype: int64
'''


df.query('def_flag==1')['LC_PDRR'].value_counts().sort_index()
'''
4     1
7     1
8     1
10    2
11    6
12    5
13    1
Name: LC_PDRR, dtype: int64
'''
# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression
from scipy.optimize import differential_evolution

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')
dat_def = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink_pseudodef.pkl')
dat = pd.concat([dat, dat_def], axis=0)
dat.dropna(subset=['Primary Evaluation'], inplace=True)

dat['Applied Rating'].value_counts().sort_index()
'''
10       16
20      940
22        4
30     2744
33        3
40     3124
51      891
52      418
61      261
62      199
70      738
81      309
82      325
83        8
90        7
101       3
102       1
Name: Applied Rating, dtype: int64
'''

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(dat['Applied Rating']>=81, 1)

assert dat.def_flag.sum() == (dat['Applied Rating']>=81).sum()

print(dat.def_flag.sum())
# 653




#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    5775
1    4216
Name: Gua_override, dtype: int64
'''

# Use Secondary as Final for all Guarantor impacted obs
mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,101: 82, 102:82})
dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


# show the discrpency in different rating
SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
#Out[22]: 0.8844416719524808

SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
#Out[23]: 0.8843993317777482






#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

df_EMEA = dat_MM_norm.query('Geo=="EMEA"').reset_index(drop=True)
df_Asia = dat_MM_norm.query('Geo=="Asia"').reset_index(drop=True)
df_Americas = dat_MM_norm.query('Geo=="Americas"').reset_index(drop=True)
# remove 2-0 and 3-0 record in Americas portfolio
mask = df_Americas['Applied Rating'].isin([20,30])
df_Americas = df_Americas[~mask].reset_index(drop=True)





dict_NonAM_BTMU_to_ylabel={
10:1, 20:2, 30:3, 40:4, 51:5, 52:6, 61:7, 62:8, 70:9, 81:10, 82:11, 83:12, 90:13, 101:14, 102:15}
dict_NonAM_ylabel_to_BTMU={
1:10, 2:20, 3:30, 4:40, 5:51, 6:52, 7:61, 8:62, 9:70, 10:81, 11:82, 12:83, 13:90, 14:101, 15:102}

df_Asia['y_label'] = df_Asia['Applied Rating'].replace(dict_NonAM_BTMU_to_ylabel)
df_EMEA['y_label'] = df_EMEA['Applied Rating'].replace(dict_NonAM_BTMU_to_ylabel)


dict_AM_BTMU_to_ylabel={
10:1, 21:2, 22:3, 31:4, 32:5, 33:6, 40:7, 51:8, 
52:9, 61:10, 62:11, 70:12, 81:13, 82:14, 83:15}
dict_AM_ylabel_to_BTMU={
1:10, 2:21, 3:22, 4:31, 5:32, 6:33, 7:40, 8:51, 
9:52, 10:61, 11:62, 12:70, 13:81, 14:82, 15:83}

df_Americas['y_label'] = df_Americas['Applied Rating'].replace(dict_AM_BTMU_to_ylabel)


#%% define func

def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))


def valuefunc_matchrate(calib):

    df['y_est'] = calib[0] + calib[1]*df['MM_quant_score']
    df['y_est'] = round(df['y_est']).clip(boundary[0],boundary[1])
    return(-(df['y_label']==df['y_est']).sum()/N)


def apply_calib(df, calib, col='y_est'):

    df[col] = calib[0] + calib[1]*df['MM_quant_score']
    df[col] = round(df[col]).clip(boundary[0],boundary[1])
    return(df)



#%% Asia
df = df_Asia
N= len(df)
df['y_label'].value_counts().sort_index()

boundary=(1,15)

# naive linear regression
x_train = sm.add_constant(df['MM_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             5.199996
MM_quant_score    0.025809
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[51]: 0.23596442468037798

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[52]: 0.7926625903279599


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 120]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-10,4), (0,0.1)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[53]: -0.3665925514174541

res1.x
# Out[54]: array([3.77446797e+00, 2.02423136e-03])


df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
# Out[55]: 0.3665925514174541

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[56]: 0.8107281823235131


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 120]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobaMMorporation\src\SFAMFA\MM\demo2312.png")

# Convert to BTMU for future overall SomersD CaMMulation
df_Asia['BTMU_est_lr'] = df_Asia['y_est_lr'].replace(dict_NonAM_ylabel_to_BTMU)
df_Asia['BTMU_est_maxmatch'] = df_Asia['y_est_maxmatch'].replace(dict_NonAM_ylabel_to_BTMU)

# check
match_rate(df_Asia, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_Asia, col_a='Applied Rating', col_b='BTMU_est_maxmatch')












#%% EMEA
df = df_EMEA
N= len(df)
df['y_label'].value_counts().sort_index()

boundary=(1,13)

# naive linear regression
x_train = sm.add_constant(df['MM_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             6.13475
MM_quant_score    0.02804
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[61]: 0.20439739413680783

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[62]: 0.8086319218241043



fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 120]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-1,10), (0,0.2)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[63]: -0.38925081433224756

res1.x
# Out[64]: array([4.43187050e+00, 2.15450184e-03])


df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
# Out[65]: 0.38925081433224756

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[66]: 0.6701954397394136



fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 120]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobaMMorporation\src\SFAMFA\MM\demo2312.png")

# Convert to BTMU for future overall SomersD CaMMulation
df_EMEA['BTMU_est_lr'] = df_EMEA['y_est_lr'].replace(dict_NonAM_ylabel_to_BTMU)
df_EMEA['BTMU_est_maxmatch'] = df_EMEA['y_est_maxmatch'].replace(dict_NonAM_ylabel_to_BTMU)

# check
match_rate(df_EMEA, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_EMEA, col_a='Applied Rating', col_b='BTMU_est_maxmatch')















#%% Americas
df = df_Americas
N= len(df)
df['y_label'].value_counts().sort_index()
'''
3      1
6      2
7     37
8     19
9      4
10     9
11     3
12     7
13     4
14     7
15     1
Name: y_label, dtype: int64
'''
boundary=(2,15)


# naive linear regression
x_train = sm.add_constant(df['MM_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             8.712767
MM_quant_score    0.015609
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[74]: 0.11702127659574468

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[75]: 0.7978723404255319


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,15,15))#
ax.set_yticklabels(['1-0','2-1', '2-2','3-1','3-2','3-3','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 90]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-20,20), (0,1)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[76]: -0.44680851063829785

res1.x
# Out[77]: array([7.11220989, 0.00797209])


df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
# Out[78]: 0.44680851063829785

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[79]: 0.6808510638297872


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,15,15))#
ax.set_yticklabels(['1-0','2-1', '2-2','3-1','3-2','3-3','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 90]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobaMMorporation\src\SFAMFA\MM\demo2312.png")

# Convert to BTMU for future overall SomersD CaMMulation
df_Americas['BTMU_est_lr'] = df_Americas['y_est_lr'].replace(dict_AM_ylabel_to_BTMU)
df_Americas['BTMU_est_maxmatch'] = df_Americas['y_est_maxmatch'].replace(dict_AM_ylabel_to_BTMU)

# check
match_rate(df_Americas, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_Americas, col_a='Applied Rating', col_b='BTMU_est_maxmatch')







#%%
df = pd.concat([df_Americas, df_Asia, df_EMEA], axis=0)

#%% Geo 
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2017')
df5 = df.query('year==2018')
df6 = df.query('year==2019')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']



list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2017','2018','2019','Major','Other']#+list_dataset_name

#%%
def calc_fun_fitted(list_dataset, list_dataset_name):

    list_obs_newmodel = []

    list_sd_lr = []
    list_mr_lr = []
    list_w2r_lr = []

    list_sd_maxmatch = []
    list_mr_maxmatch = []
    list_w2r_maxmatch = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))

        list_sd_lr.append(SomersD(dat['Applied Rating'], dat['BTMU_est_lr']))
        list_sd_maxmatch.append(SomersD(dat['Applied Rating'], dat['BTMU_est_maxmatch']))

        list_mr_lr.append(match_rate(dat, 'y_label', 'y_est_lr'))
        list_mr_maxmatch.append(match_rate(dat, 'y_label', 'y_est_maxmatch'))

        list_w2r_lr.append(within_k_rate(dat, 'y_label', 'y_est_lr', 2))
        list_w2r_maxmatch.append(within_k_rate(dat, 'y_label', 'y_est_maxmatch', 2))


        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['MMModel_size'] = list_obs_newmodel

    result['LR_SD'] = list_sd_lr
    result['LR_MatchRate'] = list_mr_lr
    result['LR_Within2Rate'] = list_w2r_lr

    result['MaxMatch_SD'] = list_sd_maxmatch
    result['MaxMatch_MatchRate'] = list_mr_maxmatch
    result['MaxMatch_Within2Rate'] = list_w2r_maxmatch

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


#%%




result = calc_fun_fitted(list_dataset, list_dataset_name)
result.to_excel(r'SFAMFA\MM\af_Geo_calib.xlsx')





CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='Applied Rating PDRR', data= dat_MM_norm, hue="Geo", alpha=0.9, palette=google_color[:3])

ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('Mapped to PDRR')#




ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
x_ = [-120, 120]
y_ = [model_MM.intercept1 + model_MM.slope1*x for x in x_]
ax2.plot(x_, y_, 'k-')
#ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
#ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\MM\demo.png")





#%% pseudo def
df['def_flag'].sum()
# 324



SomersD(df['def_flag'], df['BTMU_est_lr'])
# Out[83]: 0.4541925882947061

SomersD(df['def_flag'], df['BTMU_est_maxmatch'])
# Out[84]: 0.09731691003449054

SomersD(df['def_flag'], df['PrimaryEvaluationPDRR'])
# Out[85]: 0.7418064822873353

SomersD(df['def_flag'], df['Primary Evaluation'])
# Out[86]: 0.741759474153585

SomersD(df['def_flag'], df['Applied Rating'])
# Out[87]: 0.8308499607817855
# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')
dat_def = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink_pseudodef.pkl')
dat = pd.concat([dat, dat_def], axis=0)
dat.dropna(subset=['Primary Evaluation'], inplace=True)

dat['Applied Rating'].value_counts().sort_index()
'''
10       16
20      940
22        4
30     2744
33        3
40     3124
51      891
52      418
61      261
62      199
70      738
81      309
82      325
83        8
90        7
101       3
102       1
Name: Applied Rating, dtype: int64
'''

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(dat['Applied Rating']>=81, 1)

assert dat.def_flag.sum() == (dat['Applied Rating']>=81).sum()

print(dat.def_flag.sum())
# 653




#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    5775
1    4216
Name: Gua_override, dtype: int64
'''

# Use Secondary as Final for all Guarantor impacted obs
mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,101: 82, 102:82})
dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


# show the discrpency in different rating
SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
#Out[22]: 0.8844416719524808

SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
#Out[23]: 0.8843993317777482






#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['MM_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['MM_PD'] = list_PD
    dat['MM_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_MM_norm = get_proxy_rating(dat_MM_norm, model_MM)






def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))





df = dat_MM_norm.copy()

#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2017')
df5 = df.query('year==2018')
df6 = df.query('year==2019')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
#list_dataset_name = list(df['Industry Model'].value_counts().index)
#list_dataset=[]
#for ind in list_dataset_name:
#    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2017','2018','2019','Major','Other']#+list_dataset_name



def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'
    elif y_type=='MM':
        y_pred = 'MM_PDRR'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['MM Model_size'] = list_obs_newmodel
    result['MM Model_SD'] = list_sd_newmodel
    result['MM Model_MatchRate'] = list_mr_newmodel
    result['MM Model_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name, y_type='MM')


result.to_excel(r'SFAMFA\MM\result_bf_calib.xlsx')







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by CnI MM Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='MM_quant_score', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\MM\MM_score_vs_BTMURating.png")







#%% pseudo def
df['def_flag'].sum()
#324
df['def_flag'].sum()/len(df)
# 0.06585365853658537



SomersD(df['def_flag'], df['MM_PDRR'])
# Out[28]: 0.44769532110042476

SomersD(df['def_flag'], df['PrimaryEvaluationPDRR'])
# Out[29]: 0.7419093916302848

SomersD(df['def_flag'], df['Primary Evaluation'])
# Out[30]: 0.7418624039437437

SomersD(df['def_flag'], df['Applied Rating'])
# Out[31]: 0.8309168237397903



df.query('def_flag==1')['PrimaryEvaluationPDRR'].value_counts().sort_index()
'''
3       3
6       5
8      36
9      13
10     12
11     14
12     13
13     23
14    205
Name: PrimaryEvaluationPDRR, dtype: int64

'''


df.query('def_flag==1')['MM_PDRR'].value_counts().sort_index()
'''
2     13
3      6
4     14
5     16
6     24
7     19
8     12
9     15
10    27
11     6
12    15
13    27
14    41
15    89
Name: MM_PDRR, dtype: int64
'''
# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')


#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1250
1     355
Name: Gua_override, dtype: int64
'''

dat = dat.query('Gua_override==0').reset_index(drop=True)


#%%
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])

dat = dat.iloc[df_temp.query('diff<=4').index,]
dat.reset_index(drop=True, inplace=True)
# 1000


SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7331962963964695
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7380434194747963



from minepy import MINE
mine = MINE(alpha=0.6, c=15) 



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
list_mic=[]

for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        mine.compute_score(df_tmp_dropna[factor],df_tmp_dropna['Applied Rating PDRR']) 
        list_mic.append(mine.mic() )
    else:
        list_sd.append(np.nan)
        list_mic.append(np.nan)

result['SomersD'] = list_sd
result['MIC'] = list_mic
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)

#result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.23
list_short1 = result.query('SomersD>=0.27 and DataQuality>=0.7').index.tolist()
list_short2 = result.query('MIC>=0.2 and DataQuality>=0.7').index.tolist()

list_short = list(set(list_short1)|set(list_short2))

# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'BvD_P/L before tax',
'BvD_Current liabilities', 
'BvD_Shareholders funds',
'bs@TL_to_TA',
'MM_NOP_to_NS', 
'BvD_Debtors',
'LC_TD_to_Capt',
'new@CashFlowRatio',
'LC_Total Assets'
]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')



for i, factor in enumerate(list_final): 
    tmp = dat_norm[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')




for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,3,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
'BvD_P/L before tax',
'BvD_Current liabilities', 
'BvD_Shareholders funds',
0,0,
'BvD_Debtors',
0,0,0]



dat['BvD_P/L before tax'] = dat['BvD_P/L before tax'] + np.abs(dat['BvD_P/L before tax'].min()) +1 





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'bs@TL_to_TA',
'MM_NOP_to_NS', 
'LC_TD_to_Capt',
'new@CashFlowRatio',
]


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0, 0, 0, 0]


# add 'LC_TD_to_Capt', 
list_neg = [
0, 0, 0, 0, 0, 0,'BvD_Proxy_Capitalization_inUSD',0, 0]



#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\LC\LC_MFA_modelselection.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['LC_quantscore'] = dat_LC_norm['LC_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\LC\LC_MFA_modelselection_keepLC.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[2:], factor_num):
        model = ['LC_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['BvD_Current liabilities', 'bs@TL_to_TA', 'MM_NOP_to_NS', 'BvD_Debtors', 'LC_TD_to_Capt']
wts = [0.22407759053777218, 0.1681209924906741, 0.2746752780869888, 0.1042296050840112, 0.22889653380055378]


df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# .5097965428490594
bounds1 = [(0.05,0.99), (0.05,0.99), (0.05,0.99), (0.05,0.99),(0.05,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun
#  -0.5123785561052253

optSD_wts = res1.x / res1.x.sum()
# array([0.26176079, 0.13793903, 0.27465621, 0.08383611, 0.24180785])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5121917721675452





#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5570528790821218
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.538132764924174

score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5570528790821218 matched!



def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(10,50), (10,50), (10,50)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5579153813825857

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5579153813825857



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= aa, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()

df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# .5579153813825857  matched
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# .538844191980926



y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# .5677792213856622

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5522816482034407

df['iso'] = y_
df['iso_round'] = y_round




def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))



match_rate(df, 'Applied Rating PDRR', 'fittedvalues_round')
# 0.23340040241448692
within_k_rate(df, 'Applied Rating PDRR', 'fittedvalues_round',2)
# 0.8350100603621731


match_rate(df, 'Applied Rating PDRR', 'iso_round')
# 0.30885311871227367
within_k_rate(df, 'Applied Rating PDRR', 'iso_round',2)
# 0.8309859154929577





#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
#list_dataset_name = list(df['Industry Model'].value_counts().index)
#list_dataset=[]
#for ind in list_dataset_name:
#    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']#+list_dataset_name



def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['NewModel_MatchRate'] = list_mr_newmodel
    result['NewModel_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\LC\result_noGua.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1483
1     362
Name: Gua_override, dtype: int64
'''
dat = dat.query('Gua_override==0')
dat.reset_index(drop=True, inplace=True)


#%%
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])

dat = dat.iloc[df_temp.query('diff<=4').index,]
dat.reset_index(drop=True, inplace=True)

SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7331962963964695
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7380434194747963



from minepy import MINE
mine = MINE(alpha=0.6, c=15) 



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
list_mic=[]

for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        mine.compute_score(df_tmp_dropna[factor],df_tmp_dropna['Applied Rating PDRR']) 
        list_mic.append(mine.mic() )
    else:
        #list_sd.append(np.nan)
        list_mic.append(np.nan)

#result['SomersD'] = list_sd
result['MIC'] = list_mic
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('MIC', ascending=False, inplace=True)

#result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.23
list_short = result.query('MIC>=0.18 and DataQuality>=0.7').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'BvD_Current assets', 
'LC_Total Assets',
'BvD_Tangible fixed assets', 
'BvD_Debtors',
'BvD_Net current assets',
'MM_NOP_to_NS', 
'cf@TD_NOP',
'BvD_Intangible fixed assets']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


list_transform = [
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'BvD_Current assets', 
0,
'BvD_Tangible fixed assets', 
'BvD_Debtors',
'BvD_Net current assets',
0, 
0,
'BvD_Intangible fixed assets']


dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1 
dat['BvD_Net current assets'] = dat['BvD_Net current assets'] + np.abs(dat['BvD_Net current assets'].min()) +1 
dat['BvD_Intangible fixed assets'] = dat['BvD_Intangible fixed assets'] + np.abs(dat['BvD_Intangible fixed assets'].min()) +1 

for name in list_transform:
    if name:
        dat[name] = np.log(dat[name])
    else:
        continue



# replot after log-transformation

for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')





for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(2,5,p)
    sns.distplot(tmp[factor])
    plt.title(factor)






#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'MM_NOP_to_NS', 
'cf@TD_NOP',
]


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 'cf@TD_NOP']


#
list_neg = [
0,
0,
0,
0,
0,
0,
0,
0,
'BvD_Proxy_Net_Op_Profit_inUSD',
0]


#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
'BvD_Proxy_Net_Op_Profit_inUSD'
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation --alredy done in 
        # if list_transform[i]:
            #dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()



#%% 

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR

X, y = (dat_norm[list_final].fillna(dat_norm[list_final].median()), dat_norm['Applied Rating PDRR'])

estimator = LinearRegression()
selector = RFE(estimator, n_features_to_select=5, step=1)
selector = selector.fit(X, y)
selector.support_
# array([ True, False, False,  True, False,  True, False,  True,  True,   False])
selector.ranking_
# array([1, 6, 3, 1, 2, 1, 4, 1, 1, 5])


from sklearn.svm import SVR
estimator = SVR(kernel="linear")
selector = RFE(estimator, n_features_to_select=5, step=1)
selector = selector.fit(X, y)
selector.support_
# array([ True, False,  True,  True, False, False,  True, False,  True,       False])
# array([1, 5, 1, 1, 3, 2, 1, 4, 1, 6])






#%%




#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['BvD_P/L for period [=Net income] ',
 'LC_Total Assets',
  'BvD_Debtors',
   'MM_NOP_to_NS',
 'cf@TD_NOP',]

df = dat_norm.dropna(subset=model_setting, how='any')


bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun
#  -0.46343817386652547




# machine learning method
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

X, y = (dat_norm[list_final].fillna(dat_norm[list_final].median()), dat_norm['Applied Rating PDRR'])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0, test_size=0.2)

reg = GradientBoostingRegressor(random_state=0)
reg.fit(X_train, y_train)

SomersD(y_test, reg.predict(X_test))
# .5336114974501622



# 5 factors
model_setting = ['BvD_P/L for period [=Net income] ',
 'LC_Total Assets',
  'BvD_Debtors',
   'MM_NOP_to_NS',
 'cf@TD_NOP',]
X, y = (dat_norm[model_setting].fillna(dat_norm[model_setting].median()), dat_norm['Applied Rating PDRR'])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0, test_size=0.2)

reg = GradientBoostingRegressor(random_state=0)
reg.fit(X_train, y_train)

SomersD(y_test, reg.predict(X_test))
# 0.49986091794158555# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    6541
1    4289
Name: Gua_override, dtype: int64
'''
dat = dat.query('Gua_override==0')
dat.reset_index(drop=True, inplace=True)


#%%
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])

dat = dat.iloc[df_temp.query('diff<=4').index,]
dat.reset_index(drop=True, inplace=True)

SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.8837113198764265
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.883094348980927



from minepy import MINE
mine = MINE(alpha=0.6, c=15) 



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
list_mic=[]

for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        mine.compute_score(df_tmp_dropna[factor],df_tmp_dropna['Applied Rating PDRR']) 
        list_mic.append(mine.mic() )
    else:
        list_sd.append(np.nan)
        list_mic.append(np.nan)

result['SomersD'] = list_sd
result['MIC'] = list_mic
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)

#result.to_excel(r'SFAMFA\MM\SFA.xlsx')


#%% pick SD > 0.27
list_short1 = result.query('SomersD>=0.27 and DataQuality>=0.7').index.tolist()
list_short2 = result.query('MIC>=0.2 and DataQuality>=0.7').index.tolist()

list_short = list(set(list_short1)|set(list_short2))

# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'BvD_Shareholders funds', 
'BvD_P/L for period [=Net income] ',
'cf@TD_NOP',
'bs@TL_to_TA',
'prof@NOP_to_TA',
'new@GrossMarkup',
'BvD_Taxation',
'BvD_Gross profit',
'MM_NOP_to_NS', 
]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')



for i, factor in enumerate(list_final): 
    tmp = dat_norm[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')




for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,3,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
'BvD_Shareholders funds', 
'BvD_P/L for period [=Net income] ',
0,0,0,
'new@GrossMarkup',
'BvD_Taxation',
'BvD_Gross profit',
0, 
]


dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1 





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'cf@TD_NOP',
'bs@TL_to_TA',
'prof@NOP_to_TA',
'MM_NOP_to_NS', 
]


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# ['cf@TD_NOP', 0, 0, 0]


# add 'LC_TD_to_Capt', 
list_neg = [ 0,0,'BvD_Proxy_Net_Op_Profit_inUSD',0,0,0,0,0,0 ] 


#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'BvD_Proxy_Net_Op_Profit_inUSD',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\MM\MM_MFA_modelselection.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'BvD_Proxy_Net_Op_Profit_inUSD',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\MM\MM_MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[2:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['BvD_Shareholders funds', 'BvD_P/L for period [=Net income] ', 'bs@TL_to_TA', 'prof@NOP_to_TA', 'MM_NOP_to_NS']
wts = [0.2272382523030523, 0.1249774355078123, 0.20580792395476946, 0.38968552457314865, 0.05229086366121709]


df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5638041871130282
bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun
#  -0.5760340685974339



optSD_wts = res1.x / res1.x.sum()
#  array([0.26935967, 0.19810441, 0.14292322, 0.18743158, 0.20218111])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5760340685974339





#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5719373370094449
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5470797128468539

score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5613049604869567 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(15,45), (15,45), (15,45)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5812776372884713

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5812776372884713



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= aa, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()

df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5812776372884713
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5520284248081053






y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5848191622809893

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5697723262175707


df['iso'] = y_
df['iso_round'] = y_round




def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))



match_rate(df, 'Applied Rating PDRR', 'fittedvalues_round')
# 0.12227834871973524
within_k_rate(df, 'Applied Rating PDRR', 'fittedvalues_round',2)
# 0.6011147883643965

match_rate(df, 'Applied Rating PDRR', 'iso_round')
# 0.19561052081518898
within_k_rate(df, 'Applied Rating PDRR', 'iso_round',2)
# 0.769726528479359




#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
#list_dataset_name = list(df['Industry Model'].value_counts().index)
#list_dataset=[]
#for ind in list_dataset_name:
#    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']#+list_dataset_name



def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['NewModel_MatchRate'] = list_mr_newmodel
    result['NewModel_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\MM\result_noGua.xlsx')



CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:11:29 2019

@author: ub71894 (4e8e6d0b), CSG
"""

import glob
import os, sys, pandas as pd, numpy as np
dir_path = r'C:\Users\ub71894\Documents\Projects\GlobalCorporation'
os.chdir(os.path.join(dir_path,'src'))

#%% consolidate GCAR data
if 1:
    dt_list=[os.path.join(dir_path,r'Data\GCAR_BR_Approved',e) for e in os.listdir(os.path.join(dir_path,r'Data\GCAR_BR_Approved')) if e.find('.tsv')>0]
    dt0=pd.DataFrame()
    for e in dt_list:
        tmp=pd.read_csv(e, sep='\t', header=0,encoding = "ISO-8859-1")
        dt0=dt0.append(tmp,ignore_index=True)

dt0.to_pickle(os.path.join(dir_path,'Data\GCAR_2008-2019.pkl'))




#%%
model_names = [
 'Academic Corporation', 
 'Bank',
 'Corporation (Major Company)', 
 'Corporation (Other Company)',
 'Finance Company',
 'Fund', 
 'Health Care Company', 
 'Individual',
 'Life Insurance Company', 
 'Municipality',
 'Non-Life Insurance Company', 
 'Securities Company']


#%%
data = pd.read_pickle(r'..\Data\GCAR BR Approved All.pkl')
regex_str = '^(Current) (Financial Data #|Score #|Total Score)|^Scoring Name #|\
^Borrower (CIF|Name)$|^(Primary|Second|Third|Final).*Evaluation|\
^Application No.$|^Approval Date$|Borrower Type|Data From|Statement Date|Applying Office Name|\
^External Rating|Country$'
all_model_data = data.filter(regex=regex_str)
model_name_inuse = 'Corporation (Other Company)'
# 'Corporation (Major Company)'

#%%  non-filtered data
is_model = all_model_data['Borrower Type']==model_name_inuse
model_data_inuse = all_model_data[is_model]
print(model_data_inuse.shape)

model_data_inuse.to_pickle('model_date_inuse.pkl')
 
#%% filterd by CD
is_model_filtered = (all_model_data['Borrower Type']==model_name_inuse) & (all_model_data['Data From']=='CD')
model_data_filtered = all_model_data[is_model_filtered]
print(model_data_filtered.shape)

model_data_filtered['Approval year'] = pd.DatetimeIndex(model_data_filtered['Approval Date']).year
obs_count = model_data_filtered['Approval year'].value_counts().sort_index()

model_data_filtered.to_pickle('model_data_filtered.pkl')




#%% filter by replicated score
Corp_Others_ModelData = all_model_data[is_model_filtered]
isempty_cols = Corp_Others_ModelData.filter(regex='^Current Financial Data').isnull().all() 
# Corp_Others_ModelData[isempty_cols.index[~isempty_cols]].apply(lambda x:pd.to_numeric(x.str.replace('\,|\%',''), errors='coerce'))

factor_bins = \
{'Corporation (Other Company)':
    {

     '1. Capability for Debt Repayment - (1) Cash Flow Amount': {'bins': [-pd.np.Inf, 10*10**6, 30*10**6, 50*10**6, 70*10**6, 100*10**6, 300*10**6, 500*10**6, 700*10**6, 1000*10**6, pd.np.Inf],
                                                                 'labels': [2, 3, 4, 6, 8, 9, 11, 12, 13, 15]},
     '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA': {'bins': [-pd.np.Inf, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2, 8.2, 9.2, 10.2, pd.np.Inf],
                                                                                  'labels': [10, 8, 6, 4, 2, 0, -2, -4, -6, -8, -10]},#EBITA is negative then set it to -10
     '1. Capability for Debt Repayment - (3) EBITDA / Net Interest Payment': {'bins': [-pd.np.Inf, 0.85, 0.95, 2, 3, 4, 6, 8, pd.np.Inf],
                                                                              'labels': [0, 2, 4, 6, 8, 10, 11, 12]},
     '2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio': {'bins': [-pd.np.Inf, -80, -70, -60, -50, -40, -30, -20, -10, 0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, pd.np.Inf],
                                                                                  'labels': [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
     '2. Financial Condition - 2-1. Safety - (2) Gearing Ratio': {'bins': [-pd.np.Inf, 50, 75, 100, 150, 200, pd.np.Inf],
                                                                  'labels': [10, 8, 6, 4, 2, 0]},
     '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales': {'bins': [-pd.np.Inf, 1, 2, 3, 6, 8, 10, 12, 14, 16, 20, pd.np.Inf],
                                                                                    'labels': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
     '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets': {'bins': [-pd.np.Inf, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, pd.np.Inf],
                                                                                                             'labels': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]},
     '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth': {'bins': [-pd.np.Inf, -40, -30, -20, -10, -2.5, 0, 0.5, 5, 25, 35, pd.np.Inf],
                                                                                                                    'labels': [-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10]},
     '2. Financial Condition - 2-4. Size - (1) Net Worth': {'bins': [-pd.np.Inf, 50*10**6, 100*10**6, 150*10**6, 200*10**6, 350*10**6, 500*10**6, 750*10**6, 1000*10**6, 1500*10**6, 2000*10**6, pd.np.Inf],
                                                            'labels': [0, 1, 3, 4, 6, 7, 8, 10, 11, 13, 14]},
     '2. Financial Condition - 2-4. Size - (2) Sales': {'bins': [-pd.np.Inf, 100*10**6, 500*10**6, 1000*10**6, 3000*10**6, pd.np.Inf],
                                                        'labels': [0, 1, 2, 3, 4]}

    }
}



score_df = pd.DataFrame()
for idx in isempty_cols.index[~isempty_cols].str.replace('Current Financial Data #',''):
    factor_num='{}'.format(idx)
    scr_str=Corp_Others_ModelData['Scoring Name #'+factor_num].dropna().unique()[0]    
    print(scr_str)
    df=Corp_Others_ModelData['Current Financial Data #'+factor_num]    
    df_clean = pd.to_numeric(df.astype(str).apply(lambda x:x.replace('%','').replace(',','')), errors='coerce')
    
    is_right = True
    if scr_str in ['2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio',
                   '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales',
                   '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets',
                   '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth',
                   '2. Financial Condition - 2-4. Size - (1) Net Worth']:
        is_right = False
    
    binned_df=pd.cut(df_clean,
                     bins=factor_bins[model_names[3]][scr_str]['bins'], 
                     labels=factor_bins[model_names[3]][scr_str]['labels'], right=is_right)
    df_2 = pd.concat([Corp_Others_ModelData.filter(regex='^Borrower Name$|^Application No.$|^Approval Date'),
                      df,df_clean, binned_df, Corp_Others_ModelData['Current Score #'+factor_num]], axis=1)
#     display(df_2.head())
    bin_counts = binned_df.value_counts(dropna=False).sort_index()
#     display(bin_counts.to_frame())
    nan_diff = bin_counts[bin_counts.index[-1:]].values[0]-df.isna().value_counts()[True]
    if nan_diff>0:
        print('Please check Current Financial Data #{} as there are {} strings which are not NaNs:'.format(factor_num,nan_diff))
#         non_numbered_df = df[~df.isna()].astype(str).apply(lambda x:x.replace('%','').replace(',',''))   
#         print(non_numbered_df[pd.to_numeric(non_numbered_df, errors='coerce').isna()].value_counts(),'\n') 
  
    if scr_str == '1. Capability for Debt Repayment - (1) Cash Flow Amount':
        binned_df = binned_df.astype(float)
        binned_df[df_clean<0]=0
        binned_df = binned_df.astype('category')

    #Deal with negative EBITA for the factor - '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA'
    if scr_str == '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA':
        binned_df = binned_df.astype(float)
        binned_df[(df == 'Negative EBITDA')|(df == '#DIV/0!')]=-10
        binned_df = binned_df.astype('category')
#         print(df_2.iloc[[13781,12594]])        
#         print(df_2[(df == 'Negative EBITDA')|(df == '#DIV/0!')].shape)
#         print(df.isna().value_counts()) 

    if scr_str == '1. Capability for Debt Repayment - (3) EBITDA / Net Interest Payment':
        binned_df = binned_df.astype(float)
        binned_df[(df == 'EBITDA is 0 or less')|(df == 'Negative EBITDA')]=0
        binned_df[df=='#DIV/0!']=12
        binned_df = binned_df.astype('category')
        
    if scr_str in ['2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio',
                   '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets',
                   '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth']:
        binned_df = binned_df.astype(float)
        binned_df[df=='#DIV/0!']=0
        binned_df = binned_df.astype('category') #use this option right = False for binning

    if scr_str == '2. Financial Condition - 2-1. Safety - (2) Gearing Ratio':
        binned_df[(df=='#DIV/0!')|(df == 'Negative EQuity')]=0
        binned_df = binned_df.astype(float)
        binned_df[df_clean.le(0)]=0
        binned_df = binned_df.astype('category')
           
    if scr_str == '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales':
        binned_df[(df=='#DIV/0!')|(df == 'Negative Sales')]=0
    
    diff_score = binned_df.fillna(0).sub(Corp_Others_ModelData['Current Score #'+factor_num], fill_value=0.0)
#     print(df_2[(diff_score.abs()>0)].head(20))
    vCts = diff_score.value_counts(dropna=False).sort_index()
#     print(vCts)
    print('Number of mismatches:',vCts[(vCts.index!=0.0) & (~vCts.index.isna())].sum(),'\n')

    bin_counts = binned_df.value_counts(dropna=False).sort_index()
    nan_diff = bin_counts[bin_counts.index[-1:]].values[0]-df.isna().value_counts()[True]
#     print('Current Financial Data #{} after migtigating for the NaNs:{}\n'.format(factor_num,nan_diff))
    score_df=pd.concat([score_df, binned_df], axis=1)




score_mismatches = score_df.sum(axis=1).sub(Corp_Others_ModelData['Current Total Score'], fill_value=0.0).value_counts(dropna=False).sort_index()
print('Total matches out of {} are {} ({:.0%})'.format(score_df.shape[0],score_mismatches.loc[0.0],score_mismatches.loc[0.0]/score_df.shape[0]))



cols_need = [
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',
'Current Total Score',
'Primary Evaluation',
'Final Evaluation']

final_df = score_df.sum(axis=1).to_frame().join(Corp_Others_ModelData[cols_need])
final_df.rename({0:'Computed Total Score'}, axis='columns',inplace=True)
final_df['Score Difference'] = final_df['Computed Total Score'].fillna(0.0).eq(final_df['Current Total Score'].fillna(0.0))
final_df['Approval year'] = pd.DatetimeIndex(Corp_Others_ModelData['Approval Date']).year


final_df_same_score = final_df[final_df['Score Difference']]

final_df_same_score.to_pickle('final_df_same_score.pkl')

#%%

#Model Document
# bins=[-pd.np.Inf,15, 25, 30, 35, 40, 45, 55, 60, 65, 70, 75, 80, 85, pd.np.Inf]
# labels = [80, 70, 62, 61, 52, 51, 40, 33, 32, 31, 23, 22, 21, 10]
#Application
bins=[-pd.np.Inf, 15, 25, 30, 35, 40, 45, 55, 60, 65, 70, 75, 85, pd.np.Inf]
labels = [80, 70, 62, 61, 52, 51, 40, 33, 32, 31, 22, 21, 10]

final_df = final_df.join(pd.DataFrame({'Primary Evaluation Mapped':pd.cut(final_df['Computed Total Score'], bins=bins, labels=labels, right=True)}))

final_df['Primary Evaluation Difference'] = final_df['Primary Evaluation Mapped'].astype(float).fillna(0.0).eq(final_df['Primary Evaluation'].astype(float).fillna(0.0))
final_df_same_rating = final_df[(final_df['Score Difference']==True) & (final_df['Primary Evaluation Difference']==True)]


final_df_same_rating.to_pickle('final_df_same_rating.pkl')
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:11:29 2019

@author: ub71894 (4e8e6d0b), CSG
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


def  gcar_sfa(dat):
    nz_col = dat.columns[~dat.isnull().all()]
    dat = dat[nz_col]
    pl_x = [
    'Current Financial Data #1',
    'Current Financial Data #2',
    'Current Financial Data #3',
    'Current Financial Data #4',
    'Current Financial Data #5',
    'Current Financial Data #6',
    'Current Financial Data #7',
    'Current Financial Data #8',
    'Current Financial Data #9',
    'Current Financial Data #10',
    ]
    pl_y = [
    'Primary Evaluation',
    'Final Evaluation']   

    dat[pl_x] = dat[pl_x].astype(str).apply(lambda x:pd.to_numeric(x.replace('\\%|\\,','', regex=True), errors='coerce')) 
      

    #get the SomersD matrix 
    array_sd = np.zeros((len(pl_x), len(pl_y)))
    for i,x in enumerate(pl_x):
        for j,y in enumerate(pl_y):
            df = dat[[x,y]].dropna(how='any')
            print('x ='+x+'  y='+y+str(df.shape))
            array_sd[i,j] = np.abs(SomersD(df[y], df[x]))   

    df_sd = pd.DataFrame(array_sd)
    df_sd.index = pl_x
    df_sd.columns = pl_y  

    return(df_sd)

#%%
dat_filter1 = pd.read_pickle('model_data_filtered.pkl')
dat_filter2 = pd.read_pickle('final_df_same_score.pkl')
dat_filter3 = pd.read_pickle('final_df_same_rating.pkl')

a1 = gcar_sfa(dat_filter1)
a2 = gcar_sfa(dat_filter2)
a3 = gcar_sfa(dat_filter3)



#%%
raw = pd.read_pickle(r'..\Data\GCAR BR Approved All.pkl')
model_name_inuse = 'Corporation (Other Company)'
is_model = raw['Borrower Type']==model_name_inuse
dat = raw[is_model]

dat.dropna(subset=['Primary Evaluation'], inplace=True)
dat['Approval year'] = pd.DatetimeIndex(dat['Approval Date']).year
dat['Approvalyear'] = dat['Approval year']
for i in range(9):
    year = 2010+i
    temp = dat.query('Approvalyear=={}'.format(year))
    print('In {}, unique customer# : {}'.format(year, len(temp.drop_duplicates(subset=['Borrower Name']))))

dat_uni = dat.drop_duplicates(subset=['Borrower Name'])

obs_count = dat['Approval year'].value_counts().sort_index()
obs_count_uni = dat_uni['Approval year'].value_counts().sort_index()


#pl_todelete = raw.columns[raw.isna().all()].tolist()
#raw.drop(columns=pl_todelete, inplace=True)

pl_cols = [
'Applying Office Name',
'Borrower Country',
'Borrower Office Name',
'Credit Division',
'Group Core Office Name',
'Parent Office Name',
'Parent / Group Core Office Name',
'in the Americas',
]


for name in pl_cols:
    print(dat_uni[name].unique())


Unit
book1 = dat_uni['Borrower Office Name'].value_counts()

book2 = dat_uni['Parent / Group Core Office Name'].value_counts()

book3 = dat_uni['in the Americas'].value_counts()
with pd.ExcelWriter('book1.xlsx', engine='openpyxl', mode='a') as writer:
    book1.to_excel(writer, sheet_name='book1')
    book2.to_excel(writer, sheet_name='book2')
    book3.to_excel(writer, sheet_name='book3')# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:11:29 2019

@author: ub71894 (4e8e6d0b), CSG
"""

import os, sys, pandas as pd, numpy as np
import difflib
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


gdat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR BR Approved All.pkl')
gdat = gdat[gdat['Borrower Type']=='Corporation (Other Company)']
gdat_uni = gdat.drop_duplicates(subset=['Borrower Name'])


sp  = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\CNI\data\Compustat\compustat_data.pickle')
sp_uni = sp.drop_duplicates(subset=['CONM'], keep='last')
pl_name_base = sp_uni['CONM'].tolist()




#%% name match
data_gcar = pd.DataFrame()
data_gcar['name_gcar'] = gdat_uni['Borrower Name'].tolist()
data_gcar['diffmatchname_70'] = 'NAN'
data_gcar['diffmatchname_80'] = 'NAN'
data_gcar['diffmatchname_90'] = 'NAN'

for i in range(len(data_gcar)):
    temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.9)
    if temp:
        data_gcar.diffmatchname_90[i] = temp[0]
    else:
        temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.8)
        if temp:
            data_gcar.diffmatchname_80[i] = temp[0]
        else:
            temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.7)
            if temp:
                data_gcar.diffmatchname_70[i] = temp[0]    


data_gcar.to_excel(r'output\03\data_gcar.xlsx')





#%% after manual 
#############GCAR
data_gcar = pd.read_excel(r'output\data_gcar.xlsx')
data_gcar = data_gcar.query('correct==1')

pl_nameinRA = []
for i, row in data_gcar.iterrows():
    if row.loc['diffmatchname_90'] != "NAN":
        pl_nameinRA.append(row.loc['diffmatchname_90'])
        continue
    elif row.loc['diffmatchname_80'] != "NAN":
        pl_nameinRA.append(row.loc['diffmatchname_80'])
        continue
    else:
        pl_nameinRA.append(row.loc['diffmatchname_70'])
data_gcar['Customer_Name_lower'] = pl_nameinRA

data_gcar = pd.merge(data_gcar[['name_gcar','Customer_Name_lower']], dat[[
    'Customer_Name','Customer_Name_lower', 'CUSTOMERID', 
    'Borrow_US_entity', 'Country']], 
    on=['Customer_Name_lower'], how='left')

data_gcar.rename(columns={'name_gcar':'ForeignCompany'}, inplace=True)
data_gcar.drop(['Customer_Name_lower'], axis=1, inplace=True)
data_gcar['FC_source'] = 'GCAR'
# -*- coding: utf-8 -*-
"""
Created on Thu Feb 28 15:11:29 2019

@author: ub71894 (4e8e6d0b), CSG
"""

import os, sys, pandas as pd, numpy as np
import difflib
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


gdat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR BR Approved All.pkl')
gdat = gdat[gdat['Borrower Type']=='Corporation (Major Company)']
gdat_uni = gdat.drop_duplicates(subset=['Borrower Name'])


sp  = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\CNI\data\Compustat\compustat_data.pickle')
sp_uni = sp.drop_duplicates(subset=['CONM'], keep='last')
pl_name_base = sp_uni['CONM'].tolist()




#%% name match
data_gcar = pd.DataFrame()
data_gcar['name_gcar'] = gdat_uni['Borrower Name'].tolist()
data_gcar['diffmatchname_70'] = 'NAN'
data_gcar['diffmatchname_80'] = 'NAN'
data_gcar['diffmatchname_90'] = 'NAN'

for i in range(len(data_gcar)):
    temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.9)
    if temp:
        data_gcar.diffmatchname_90[i] = temp[0]
    else:
        temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.8)
        if temp:
            data_gcar.diffmatchname_80[i] = temp[0]
        else:
            temp = difflib.get_close_matches(data_gcar.name_gcar[i], pl_name_base, n=1, cutoff=0.7)
            if temp:
                data_gcar.diffmatchname_70[i] = temp[0]    


data_gcar.to_excel(r'output\03\data_gcar_major.xlsx')





#%% after manual 
#############GCAR
data_gcar = pd.read_excel(r'output\data_gcar.xlsx')
data_gcar = data_gcar.query('correct==1')

pl_nameinRA = []
for i, row in data_gcar.iterrows():
    if row.loc['diffmatchname_90'] != "NAN":
        pl_nameinRA.append(row.loc['diffmatchname_90'])
        continue
    elif row.loc['diffmatchname_80'] != "NAN":
        pl_nameinRA.append(row.loc['diffmatchname_80'])
        continue
    else:
        pl_nameinRA.append(row.loc['diffmatchname_70'])
data_gcar['Customer_Name_lower'] = pl_nameinRA

data_gcar = pd.merge(data_gcar[['name_gcar','Customer_Name_lower']], dat[[
    'Customer_Name','Customer_Name_lower', 'CUSTOMERID', 
    'Borrow_US_entity', 'Country']], 
    on=['Customer_Name_lower'], how='left')

data_gcar.rename(columns={'name_gcar':'ForeignCompany'}, inplace=True)
data_gcar.drop(['Customer_Name_lower'], axis=1, inplace=True)
data_gcar['FC_source'] = 'GCAR'
# -*- coding: utf-8 -*-
"""
Created on Fri Jan  3 12:01:22 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np
import difflib, time
from heapq import nlargest
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR BR Approved All.pkl')
gdat_unim = data[data['Borrower Type']=='Corporation (Major Company)'].drop_duplicates(subset=['Borrower Name'])
gdat_unio = data[data['Borrower Type']=='Corporation (Other Company)'].drop_duplicates(subset=['Borrower Name'])

list_cols = ['Borrower Name', 'Borrower Country','Borrower Industry','Borrower Type']
gdat_unim = gdat_unim[list_cols]
gdat_unio = gdat_unio[list_cols]

list_overlap = list(set(gdat_unim['Borrower Name'])&set(gdat_unio['Borrower Name']))
gdat_unim.loc[gdat_unim['Borrower Name'].isin(list_overlap), 'Borrower Type'] = 'Both'
gdat_unio.loc[gdat_unio['Borrower Name'].isin(list_overlap), 'Borrower Type'] = 'Both'

dat = pd.concat([gdat_unim, gdat_unio], axis=0)
dat_uni = dat.drop_duplicates(subset=['Borrower Name'])
dat_uni['Borrower Type'].value_counts()
'''
Out[15]: 
Corporation (Major Company)    3900
Corporation (Other Company)    3617
Both                            734
Name: Borrower Type, dtype: int64

'''


country_count = dat_uni['Borrower Country'].value_counts()
# 71 countries

(country_count.cumsum()/len(dat_uni)).head(20)
'''
# top 20 cum pct:
UNITED STATES     0.563810
#BRAZIL            0.656163
#CANADA            0.741122
#MEXICO            0.795782
UNITED KINGDOM    0.818204
#NETHERLANDS       0.838201
#CHILE             0.853351
#LUXEMBOURG        0.864138
#IRELAND           0.874197
#SWITZERLAND       0.884256
#ARGENTINA         0.893710
GERMANY           0.901709
#PERU              0.908375
FRANCE            0.913950
JAPAN             0.919404
#AUSTRALIA         0.924494
#COLOMBIA          0.929463
#SINGAPORE         0.934311
#BERMUDA           0.938553
#CAYMAN ISLANDS    0.942310


0.005575+0.007999
Batch 1:  33.70%
#BRAZIL          
#CANADA          
#MEXICO          
#NETHERLANDS     
#CHILE           
#LUXEMBOURG      
#IRELAND         
#SWITZERLAND     
#ARGENTINA               
#PERU                      
#AUSTRALIA       
#COLOMBIA        
#SINGAPORE       
#BERMUDA         
#CAYMAN ISLANDS  

Batch 2:  1.35%
GERMANY 
FRANCE 

Batch 3:  0.54%
JAPAN  

Batch 4:
Mid Atlantic 
Midwest

Batch 5:
Northeast
Southeast
Southwest


Batch 6:
west

Batch 7:
UNITED KINGDOM 
    Industry: Financials; Energy and Utilities; Materials; Healthcare

    
'''




ind_count = dat_uni['Borrower Industry'].value_counts()
#434 industries

(ind_count.cumsum()/len(dat_uni)).head(20)
'''
# top 20 cum pct:
Transportation Equipment             0.058054
Electric Utility Services            0.108835
Oil and Gas Extraction               0.146285
Machinery Wholesale                  0.180099
Other Finance Services               0.211126
Industrial & Commercial Machinery    0.234396
Food and Kindred Products            0.256333
Food and Kindred Products            0.256333
Food and Kindred Products            0.256333
Gas Utility Services                 0.277300
Chemicals and Related Products       0.296813
Prescription Pharmaceuticals         0.315477
Metal Mining                         0.330990
Other Products Wholesale             0.346261
Aircraft and Components              0.360441
Special Social Services              0.374379
Fabricated Metal Products            0.387226
Other Financial Institutions         0.399709
General Machinery Wholesale          0.410980
Iron Metal Industries                0.421403
Other Health Care Services           0.431463
Chemical Product Wholesale           0.441401
'''



sp_batch  = pd.read_excel(r'..\data\SNL_batch6.xlsx',sheet_name='batch6', skiprows=1)
sp_batch = sp_batch.drop([0])
sp_batch_uni = sp_batch.drop_duplicates(subset=[ 'Entity Name '], keep='last')
pl_name_base = sp_batch_uni['Entity Name '].tolist()

pl_name_base_str=[]
for name in pl_name_base:
    if isinstance(name,str):
        pass
    else:
        name = str(name)
    pl_name_base_str.append(name )



data_tokyo = pd.DataFrame()
data_tokyo['name_tokyo'] = dat_uni['Borrower Name'].tolist()
data_tokyo['diffmatchname_80'] = np.nan
data_tokyo['diffmatchname_90'] = np.nan

start_time = time.time()
for i in range(len(data_tokyo)):
    if i%82==0:
        print(f'{i/82:.0f}% complete{20*"."}')
    s = difflib.SequenceMatcher()
    s.set_seq2(data_tokyo.name_tokyo[i])
    max_score=0.8; 
    for x in pl_name_base_str:
        s.set_seq1(x)
        if s.real_quick_ratio() >= max_score and \
           s.quick_ratio() >= max_score and \
           s.ratio() >= max_score:
            max_score = s.ratio()
            bb = x
        else:
            continue
    if max_score>=0.9:
        data_tokyo.diffmatchname_90[i] = bb
    elif max_score>0.8:
        data_tokyo.diffmatchname_80[i] = bb
    else:
        continue
print("--- %s seconds ---" % (time.time() - start_time))


data_tokyo.to_excel(r'output\tokyonamematching\data_tokyo_batch6.xlsx')



#%%
'''
EBITDA  132709
Net sales
    sales revenue  132527
    gross profit 132529   all NA
    total  revenue  141780

total debt 132319

capitalization  133846
interest expense  227248

Net Operating Profit
    Net Operating Income 138302
    Net Operating Profit after tax 138304
Ending Cash & Equivalents 132167

Tangible Net Worth 
    Adjust net worth 255357
    Intangible Assets 266826
    Tangible Assets  132265

Compustat

Total Asset  268552
Total Liabilities  268420
Total debt 276026
EBITDA  271528
Net Sales  275450
Capitalization 
    Total Capitalization, at Book Value 278237
Interest Expense  271486
Net Operating Profit
    Net Operating Income 291625
    Net Operating Profit after tax 291628
    Net Operating Income(reported) 275461
Ending cash & Equivalents 
    cash & cash Equivalents(SP) 275616
    cash & cash Equivalents(SNL) 273676
Tangible Net Worth 
    Tangible Assets  273697

we can pull sp rating
'''



#%% after manual work

combined_data = pd.DataFrame()
for i in range(6):

    batch_No = i+1

    sp_batch  = pd.read_excel(r'..\data\SNL_batch{}.xlsx'.format(batch_No), sheet_name='batch{}'.format(batch_No),skiprows=1)
    sp_batch = sp_batch.drop([0])
    sp_batch_uni = sp_batch.drop_duplicates(subset=[ 'Entity Name '], keep='last')
    sp_batch_uni = sp_batch_uni[[
    'Entity Name ',
    'Entity ID ',
    'Company Type ',
    'Company Status ',
    'Geography ']]
    
    matched_data_batch = pd.read_excel(r'output\tokyonamematching\data_tokyo_batch{}_corrected.xlsx'.format(batch_No))
    matched = matched_data_batch.query('correct==1')
    matched.reset_index(drop=True, inplace=True)
    matched['diffmatchname_90'].replace({'NAN':np.nan}, inplace=True) # for batch 1
    matched['Entity Name '] = matched['diffmatchname_90'].fillna(matched['diffmatchname_80'])
    matched = matched[['name_tokyo', 'Entity Name ']]
    matched['source'] = f'batch{batch_No}'
    mergered_data = pd.merge(matched, sp_batch_uni, on=['Entity Name '], how='left')
    combined_data = pd.concat([combined_data, mergered_data], axis=0)
                        

combined_data.reset_index(drop=True, inplace=True)
combined_data.to_pickle(r'output\tokyonamematching\combined_data_raw.pkl')

combined_data.drop_duplicates(subset=['name_tokyo'], keep='last', inplace=True)
combined_data.reset_index(drop=True, inplace=True)
combined_data.to_pickle(r'output\tokyonamematching\combined_data_cleaned.pkl')

dat_uni = pd.read_csv('TokyoDataUniqueName.csv')
print( f'The raw availability pct is {100*len(combined_data)/len(dat_uni):.2f}%')

# The raw availability pct is 9.89%


#%%
data_forpull = pd.concat([combined_data]*4, axis=0)
data_forpull.sort_values(by=['name_tokyo'], inplace=True)
data_forpull['year'] = ['FY2018','FY2017','FY2016','FY2015']*len(combined_data)
data_forpull.reset_index(drop=True, inplace=True)
data_forpull = data_forpull[[
'Entity ID ',
'year',
'name_tokyo',
'Entity Name ',
'Entity ID ',
'Company Type ',
'Company Status ',
'Geography ',
]]

data_forpull.to_excel(r'output\tokyonamematching\data_forpull.xlsx')


#%% after pulling data


dat_2018['ValidFieldsNo'] = 10-dat_2018[[
 'Total Assets (Reported)',
 'Total Liabilities (Reported)',
 'Total Debt (Reported)',
 'EBITDA (Reported)',
 'Net Sales - (Reported) (Reported)',
 'Total Capitalization, at Book Value (Reported)',
 'Interest Expense (Reported)',
 'Net Operating Profit After Tax (Reported)',
 'Cash and Cash Equivalents (Reported)',
 'Tangible Assets (Reported)']].isnull().sum(axis=1)

temp = dat_2018.groupby(by=['Borrower Country'])['ValidFieldsNo'].mean()

temp[[
'UNITED STATES',
'BRAZIL',
'CANADA',
'MEXICO',
'UNITED KINGDOM',
'NETHERLANDS',
'CHILE',
'LUXEMBOURG',
'SWITZERLAND',
'IRELAND',
'ARGENTINA',
'GERMANY',
'PERU',
'FRANCE',
'JAPAN',
'AUSTRALIA',
'COLOMBIA',
'SINGAPORE',
'BERMUDA',
]]
UNITED STATES     1.965587
BRAZIL            3.090909
CANADA            2.641509
MEXICO            0.384615
UNITED KINGDOM    5.000000
NETHERLANDS       0.095238
CHILE             1.600000
LUXEMBOURG        0.000000
SWITZERLAND       0.555556
IRELAND           0.500000
ARGENTINA         5.000000
GERMANY           0.933333
PERU              1.666667
FRANCE            2.333333
JAPAN                  NaN
AUSTRALIA         2.714286
COLOMBIA          5.000000
SINGAPORE         0.000000
BERMUDA           3.000000# -*- coding: utf-8 -*-
"""
Created on Mon Jan 13 10:40:56 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
import missingno as msno
from pandas_profiling import ProfileReport

dat_tokyo = pd.read_csv('TokyoDataUniqueName.csv')
dat_tokyo = dat_tokyo.drop(['Unnamed: 0'],axis=1)

data = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\output\tokyonamematching\data_pulled.xlsx',
 sheet_name='CapitalIQ',
 skiprows=1)
data = data.drop([0])
data.rename(columns={'name_tokyo':'Borrower Name'}, inplace=True)
dat_foranalysis = pd.merge(data[['Borrower Name','year']].drop_duplicates(subset=['Borrower Name']),
							 dat_tokyo, on=['Borrower Name'], how='right')

dat_last4 = pd.merge(data, dat_tokyo, on=['Borrower Name'], how='left')
dat_2018 = dat_last4.query('year=="FY2018"')

cols_forqc = [
 'Borrower Country', 'Borrower Industry', 'Borrower Type',
 'Entity ID ',
 'Company Type ',
 'Company Status ',
 'Geography ',
 'Total Assets (Reported)',
 'Total Liabilities (Reported)',
 'Total Debt (Reported)',
 'EBITDA (Reported)',
 'Net Sales - (Reported) (Reported)',
 'Total Capitalization, at Book Value (Reported)',
 'Interest Expense (Reported)',
 'Net Operating Income (Reported)',
 'Net Operating Profit After Tax (Reported)',
 'Net Operating Income - (Reported) (Reported)',
 'Cash and Cash Equivalents (Reported)',
 'Cash and Cash Equivalents (Reported).1',
 'Tangible Assets (Reported)']

cols_formsno = [
 'Entity ID ',
 'Total Assets (Reported)',
 'Total Liabilities (Reported)',
 'Total Debt (Reported)',
 'EBITDA (Reported)',
 'Net Sales - (Reported) (Reported)',
 'Total Capitalization, at Book Value (Reported)',
 'Interest Expense (Reported)',
 'Net Operating Profit After Tax (Reported)',
 'Cash and Cash Equivalents (Reported)',
 'Tangible Assets (Reported)']


cols_factors = [
 'Total Assets (Reported)',
 'Total Liabilities (Reported)',
 'Total Debt (Reported)',
 'EBITDA (Reported)',
 'Net Sales - (Reported) (Reported)',
 'Total Capitalization, at Book Value (Reported)',
 'Interest Expense (Reported)',
 'Net Operating Income (Reported)',
 'Net Operating Profit After Tax (Reported)',
 'Net Operating Income - (Reported) (Reported)',
 'Cash and Cash Equivalents (Reported)',
 'Cash and Cash Equivalents (Reported).1',
 'Tangible Assets (Reported)'
       ]
#%%
dat_plot1 = dat_last4[cols_forqc]
profile = ProfileReport(dat_plot1, correlation_threshold=1)
profile.to_file()

msno.matrix(dat_plot1[cols_formsno])

len(dat_last4[cols_factors].dropna(how='all'))
1266


#%%

dat_plot2 = dat_2018[cols_forqc]

profile = ProfileReport(dat_plot2, correlation_threshold=1)
profile.to_file()

msno.matrix(dat_plot2[cols_formsno])


len(dat_2018[cols_factors].dropna(how='all'))
294
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_cif_cleaner, gcar_americas_office_code

data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019.pkl')
# 482712
data = data[data['Data From']=='CD']
# 222034
data1 = data[data['Borrower Type']=='Corporation (Major Company)']
# 36858
data2 = data[data['Borrower Type']=='Corporation (Other Company)']
# 125359
data = pd.concat([data1, data2], axis=0)

data.dropna(subset=['Borrower CIF'], inplace=True)
# 162217
data['Borrower CIF_cleaned'] =gcar_cif_cleaner(data)
# 162217



data['timestamp'] = data['Approval Date'].mask(
                        pd.isnull(data['Approval Date']),
                        data['Approval/Agreed Date'])
data['timestamp'] = pd.to_datetime(data['timestamp'])
data.dropna(subset=['timestamp'], inplace=True)
# 162217

data['year'] = [int(x.year) for x in data['timestamp'] ]
gdat_unim = data[data['Borrower Type']=='Corporation (Major Company)'].drop_duplicates(subset=['Borrower CIF_cleaned','year'])
gdat_unio = data[data['Borrower Type']=='Corporation (Other Company)'].drop_duplicates(subset=['Borrower CIF_cleaned','year'])
#gdat_unifff = data[data['Borrower Type']=='MUB Model'].drop_duplicates(subset=['Borrower CIF_cleaned'])

list_overlap = list(set(gdat_unim['Borrower CIF_cleaned'])&set(gdat_unio['Borrower CIF_cleaned']))
gdat_unim['Borrower Type 2'] = gdat_unim['Borrower Type'].copy()
gdat_unio['Borrower Type 2'] = gdat_unio['Borrower Type'].copy()

gdat_unim.loc[gdat_unim['Borrower CIF_cleaned'].isin(list_overlap), 'Borrower Type 2'] = 'Both'
gdat_unio.loc[gdat_unio['Borrower CIF_cleaned'].isin(list_overlap), 'Borrower Type 2'] = 'Both'

dat = pd.concat([gdat_unim, gdat_unio], axis=0)
# 105060
dat = dat.drop_duplicates(subset=['Borrower CIF_cleaned','year'])
# 104062
dat.shape
#Out[26]: (104062, 904)


dat['Borrower Type'].value_counts()
'''
Out[27]: 
Corporation (Other Company)    78039
Corporation (Major Company)    16405
Both                            9618
Name: Borrower Type, dtype: int64
'''
#dat.to_pickle(r'..\data\GCAR_2008-2019_filtered2.pkl')
#country_str = 'Applying Office Name'



country_str = 'Borrower Country'
country_count = dat[country_str].value_counts()
# 124

dat['year'].value_counts().sort_index()

'''
2008      195
2009     3585
2010     9393
2011    10151
2012    10946
2013    11249
2014    11698
2015     9981
2016     9889
2017     9722
2018     8957
2019     1309
Name: year, dtype: int64
'''
#%%
def _top5_country(data):
    return(data[country_str].value_counts().head(10))

df_1 = dat.groupby(by=['year']).apply(_top5_country).reset_index()
df_1.to_excel(r'top10county_by_year.xlsx')




dat_GCAR = dat.loc[(dat['Borrower Office Code']).isin([3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770])] 
dat_GCAR['year'].value_counts()
'''
2016    2588
2014    2482
2015    2385
2011    2323
2013    2251
2012    2133
2010    2048
2017    1607
2018    1124
2019     227
'''
df_2 = dat_GCAR.groupby(by=['year']).apply(_top5_country).reset_index()

# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_cif_cleaner, gcar_converter

data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019.pkl')
# 482712



#%% add part of BL besides CD # modified on 08/28/2020

data_BL = data.loc[(data['Data From'].isin(['BL']))]
data_BL = data_BL[data_BL['Ringi/Sairyo']=='Sairyo'] 
data_CD = data.loc[(data['Data From'].isin(['CD']))]

data = pd.concat([data_CD, data_BL])
# 260879

#  add on 2020/09/01 , but no impact

data = data.sort_values(by=['Data From'])
data = data.drop_duplicates(subset=['Application No.'],keep='last')





data1 = data[data['Borrower Type']=='Corporation (Major Company)']
# 43831
data2 = data[data['Borrower Type']=='Corporation (Other Company)']
# 154135
data = pd.concat([data1, data2], axis=0)

data.dropna(subset=['Borrower CIF'], inplace=True)
# 197966
data['Borrower_CIF_cleaned'] = gcar_cif_cleaner(data, col='Borrower CIF')



data['timestamp'] = data['Approval Date'].mask(
                        pd.isnull(data['Approval Date']),
                        data['Approval/Agreed Date'])
data['timestamp'] = pd.to_datetime(data['timestamp'])
data.dropna(subset=['timestamp'], inplace=True)
# 197966

data['year'] = [int(x.year) for x in data['timestamp'] ]
gdat_unim = data[data['Borrower Type']=='Corporation (Major Company)'].drop_duplicates(subset=['Borrower_CIF_cleaned','year'])
gdat_unio = data[data['Borrower Type']=='Corporation (Other Company)'].drop_duplicates(subset=['Borrower_CIF_cleaned','year'])
#gdat_unifff = data[data['Borrower Type']=='MUB Model'].drop_duplicates(subset=['Borrower_CIF_cleaned'])

list_overlap = list(set(gdat_unim['Borrower_CIF_cleaned'])&set(gdat_unio['Borrower_CIF_cleaned']))
gdat_unim['Borrower Type 2'] = gdat_unim['Borrower Type'].copy()
gdat_unio['Borrower Type 2'] = gdat_unio['Borrower Type'].copy()

gdat_unim.loc[gdat_unim['Borrower_CIF_cleaned'].isin(list_overlap), 'Borrower Type 2'] = 'Both'
gdat_unio.loc[gdat_unio['Borrower_CIF_cleaned'].isin(list_overlap), 'Borrower Type 2'] = 'Both'

dat = pd.concat([gdat_unim, gdat_unio], axis=0)
# 121645

# keep just one record per year per customer, keep one from "CD"
dat = dat.drop_duplicates(subset=['Borrower_CIF_cleaned','year'], keep='last')
# 120384

dat.reset_index(drop=True, inplace=True)



dat.to_pickle(r'..\data\GCAR_2008-2019_filtered3.pkl')

# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered.pkl')
dat = data.query('year>=2017')

#%% Drop defaulters
def_mask = (dat['Applied Rating']==101) | (dat['Applied Rating']==102)
dat = dat[~def_mask]

country_str = 'Borrower Country'
country_count = dat[country_str].value_counts()
# 104
dat['year'].value_counts().sort_index()

'''
2017    9714
2018    8948
2019    8283
Name: year, dtype: int64
'''

#%% split to Americas and Non-Americas
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]

dat_americas = dat.loc[(dat['Borrower Office Code']).isin(list_americas_office_code)] 
dat_nonamericas = dat.loc[~(dat['Borrower Office Code']).isin(list_americas_office_code)] 

# americas
dat_americas['year'].value_counts()
'''
2017    1606
2018    1124
2019     972
'''
dat_nonamericas['year'].value_counts()
'''
2017    8108
2018    7824
2019    7311
'''


'''
list_officename = []
list_countryname = []
for code in list_americas_office_code:
    mask = dat_americas['Borrower Office Code']==code
    officename = dat_americas[mask]['Borrower Office Name'].unique().tolist()
    countryname = dat_americas[mask]['Borrower Country'].unique().tolist()
    list_officename.append(officename)
    list_countryname.append(countryname)
df_am = pd.DataFrame()
df_am['code'] = list_americas_office_code
df_am['OfficeName'] = list_officename
df_am['CountryName'] = list_countryname

df_am.to_excel('americas_code.xlsx')
'''

#%% cleaning rating 
# Americas: 2-3-->2-2 and 8-3 --> 8-2
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].replace({23:22,83:82})
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].replace({23:22,83:82})
# Non- Americas: 8-3 --> 8-2 and remove outliers (21)
dat_nonamericas['Applied Rating'] = dat_nonamericas['Applied Rating'].replace({83:82})
dat_nonamericas = dat_nonamericas[dat_nonamericas['Applied Rating']!=21]


# Americas: replace 2-0 to 50% 2-1 and 50% 2-2
mask_20 = dat_americas['Applied Rating']==20
mask_21_filter = mask_20.cumsum() < (mask_20.sum()/2)
mask_22_filter = mask_20.cumsum() >= (mask_20.sum()/2)
mask_21 = mask_20 * mask_21_filter
mask_22 = mask_20 * mask_22_filter
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_21.astype('bool'), 21)
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_22.astype('bool'), 22)


# Americas: replace 3-0 to 33.3% to 3-1, 33.3% to 3-2, and 33.3% to 3-3
mask_30 = dat_americas['Applied Rating']==30
mask_31_filter = mask_30.cumsum() < (mask_30.sum()/3)
mask_32_filter = (mask_30.cumsum() >= (mask_30.sum()/3)) & ( mask_30.cumsum() < (mask_30.sum()*2/3))
mask_33_filter = mask_30.cumsum() >= (mask_30.sum()*2/3)

mask_31 = mask_30 * mask_31_filter
mask_32 = mask_30 * mask_32_filter
mask_33 = mask_30 * mask_33_filter

dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_31.astype('bool'), 31)
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_32.astype('bool'), 32)
dat_americas['Applied Rating'] = dat_americas['Applied Rating'].mask(mask_33.astype('bool'), 33)



#%% cast integer rating to string
dat_americas['BTMU Rating'] = dat_americas['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat_nonamericas['BTMU Rating'] = dat_nonamericas['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])

dat_americas['BTMU Rating'].value_counts().sort_index() 
dat_nonamericas['BTMU Rating'].value_counts().sort_index() 



'''
#%% find some missed office code for Americas:
list_cty = ['BRAZIL','Br. Virgin Is.','BERMUDA','CAYMAN ISLANDS','UNITED STATES','CANADA']
list_df_tmp = {}
for cty in list_cty:
    tmp = dat_nonamericas[dat_nonamericas['Borrower Country'] ==cty]
    list_code= tmp['Borrower Office Code'].unique().tolist()
    list_officename=[]
    for code in list_code:
        list_officename.append(tmp[tmp['Borrower Office Code']==code]['Borrower Office Name'].unique().tolist())
    
    df_tmp = pd.DataFrame()
    df_tmp['code'] = list_code
    df_tmp['officename'] = list_officename
    list_df_tmp.update({cty:df_tmp})
# manual check find code below:

additional_BorrowerOfficeCode = [3281,3282,3286]  # add this to previous code


addtional = dat_nonamericas.loc[(dat_nonamericas['Borrower Office Code']).isin(additional_BorrowerOfficeCode)] 
dat_americas = pd.concat([dat_americas, addtional], axis=0)

dat_nonamericas = dat_nonamericas.loc[~(dat_nonamericas['Borrower Office Code']).isin(additional_BorrowerOfficeCode)] 
'''

dat_americas.to_pickle(r'..\data\dat_americas.pkl')
dat_nonamericas.to_pickle(r'..\data\dat_nonamericas.pkl')


'''
dat_americas.to_pickle(r'..\data\dat_americas_2008-2019.pkl')
dat_nonamericas.to_pickle(r'..\data\dat_nonamericas_2008-2019.pkl')

'''

# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCARS 2008-2019_matches.pkl')


#%% Drop defaulters
def_mask = (dat['Applied Rating']==101) | (dat['Applied Rating']==102)
dat = dat[~def_mask]

country_str = 'Borrower Country'
country_count = dat[country_str].value_counts()
dat['year'].value_counts().sort_index()


#%% split to Americas and Non-Americas
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286
]

dat_americas = dat.loc[(dat['Borrower Office Code']).isin(list_americas_office_code)] 
dat_nonamericas = dat.loc[~(dat['Borrower Office Code']).isin(list_americas_office_code)] 



#%% Drop rating 2-0, 2-3, 3-0, 8-3, 9-0

mask = dat_americas['Applied Rating'].isin([20,23,30,83,90])
dat_americas = dat_americas[~mask]
mask = dat_nonamericas['Applied Rating'].isin([20,23,30,83,90])
dat_nonamericas = dat_nonamericas[~mask]


# cast integer rating to string
dat_americas['BTMU Rating'] = dat_americas['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat_nonamericas['BTMU Rating'] = dat_nonamericas['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])

dat_americas['BTMU Rating'].value_counts().sort_index() 
dat_nonamericas['BTMU Rating'].value_counts().sort_index() 
'''
1-0      333
2-1        2
4-0    19301
5-1     7942
5-2     3979
6-1     3513
6-2     5341
7-0     8749
8-1     4641
8-2     3935
Name: BTMU Rating, dtype: int64
'''

# map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat_americas['Applied Rating PDRR'] = dat_americas['Applied Rating'].transform(lambda x: pd_mapping[x])
dat_nonamericas['Applied Rating PDRR'] = dat_nonamericas['Applied Rating'].transform(lambda x: pd_mapping[x])


cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Match'
]


dat_americas[cols].to_pickle(r'..\data\dat_americas_2008-2019_dropratings.pkl')
dat_nonamericas[cols].to_pickle(r'..\data\dat_nonamericas_2008-2019_dropratings.pkl')



#%% EU 
# load data and then 
def_mask = (dat['Applied Rating']==101) | (dat['Applied Rating']==102)
dat = dat[~def_mask]


list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286
]

list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

dat_eu = dat.loc[(dat['Borrower Office Code']).isin(list_eu_office_code)] 
dat_row = dat.loc[~(dat['Borrower Office Code']).isin(list_code)] 





#%% Drop rating 2-0, 2-3, 3-0, 8-3, 9-0
mask = dat_eu['Applied Rating'].isin([20,23,30,83,90])
dat_eu = dat_eu[~mask]
mask = dat_row['Applied Rating'].isin([20,23,30,83,90])
dat_row = dat_row[~mask]


# cast integer rating to string
dat_eu['BTMU Rating'] = dat_eu['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat_row['BTMU Rating'] = dat_row['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])

dat_eu['BTMU Rating'].value_counts().sort_index() 
dat_row['BTMU Rating'].value_counts().sort_index() 


# map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat_eu['Applied Rating PDRR'] = dat_eu['Applied Rating'].transform(lambda x: pd_mapping[x])
dat_row['Applied Rating PDRR'] = dat_row['Applied Rating'].transform(lambda x: pd_mapping[x])


cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Match'
]


dat_eu[cols].to_pickle(r'..\data\dat_eu_2008-2019_dropratings.pkl')
dat_row[cols].to_pickle(r'..\data\dat_row_2008-2019_dropratings.pkl')

# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner


df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
df1 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_2008-2019_dropratings.pkl')
df2 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_2008-2019_dropratings.pkl')
df3 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_2008-2019_dropratings.pkl')
df4 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_row_2008-2019_dropratings.pkl')



bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df1, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df2, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df3, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df4, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


df_bvd['Fiscal year'].value_counts().sort_index()
'''
Out[20]: 
1984       1
1985       1
1986       1
1987       1
1988       1
1989       2
1990       4
1991       5
1992       7
1993      12
1994      22
1995      29
1996      32
1997      35
1998      47
1999      82
2000     132
2001     209
2002     336
2003     433
2004     471
2005     617
2006    1078
2007    1692
2008    2962
2009    6279
2010    6813
2011    7214
2012    7572
2013    8556
2014    9367
2015    8940
2016    9075
2017    8597
2018    6637
2019     250
2020       3
Name: BvD_Fiscal year, dtype: int64
'''

len(set(df1['Borrower CIF'].unique()))
# 5761

len(set(df2['Borrower CIF'].unique()))
# 14080

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(df1['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 1061

len(set(df2['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 8187


len(set(df3['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 2386

len(set(df4['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 5873


#%%

#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% merge
df_am = pd.merge(df1, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_am['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_am['BvD_CIF']]
df_am['inBvD'].value_counts()
'''
Out[11]: 
No     16584
Yes     2976
Name: inBvD, dtype: int64

'''

df_nam = pd.merge(df2, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_nam['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_nam['BvD_CIF']]
df_nam['inBvD'].value_counts()

'''
No     30826
Yes    29054
Name: inBvD, dtype: int64
'''

df_eu = pd.merge(df3, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_eu['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_eu['BvD_CIF']]
df_eu['inBvD'].value_counts()
'''
Yes    9016
No     6874
Name: inBvD, dtype: int64
'''

df_row = pd.merge(df4, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_row['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_row['BvD_CIF']]
df_row['inBvD'].value_counts()

'''
No     23952
Yes    20038
Name: inBvD, dtype: int64
'''


df_am.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_SFA_BvD_2008-2019.pkl')
df_nam.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_SFA_BvD_2008-2019.pkl')

df_eu.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_SFA_BvD_2008-2019.pkl')
df_row.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_row_SFA_BvD_2008-2019.pkl')

# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']


df_am = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_SFA_BvD_2008-2019.pkl')
df_nam = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_SFA_BvD_2008-2019.pkl')

df_eu = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_SFA_BvD_2008-2019.pkl')
df_row = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_row_SFA_BvD_2008-2019.pkl')


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

df_am = build_cni_factors(df_am)
df_nam = build_cni_factors(df_nam)
df_eu = build_cni_factors(df_eu)
df_row = build_cni_factors(df_row)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df_am[factor] = df_am[factor].clip(
            np.nanmin(df_am[factor][df_am[factor] != -np.inf]), 
            np.nanmax(df_am[factor][df_am[factor] != np.inf]))
        df_nam[factor] = df_nam[factor].clip(
            np.nanmin(df_nam[factor][df_nam[factor] != -np.inf]),
             np.nanmax(df_nam[factor][df_nam[factor] != np.inf]))
        df_eu[factor] = df_eu[factor].clip(
            np.nanmin(df_eu[factor][df_eu[factor] != -np.inf]),
             np.nanmax(df_eu[factor][df_eu[factor] != np.inf]))
        df_row[factor] = df_row[factor].clip(
            np.nanmin(df_row[factor][df_row[factor] != -np.inf]),
             np.nanmax(df_row[factor][df_row[factor] != np.inf]))
    
 
   


#%% factor level SFA
def sfa_result(data, filename):
    dat = data.query('2016<=year<=2018')
    dat_LC = dat.query('BvD_Proxy_Net_Sales_inUSD>1e9')
    dat_MM = dat.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
    list_dataset = [dat, dat_LC, dat_MM]
    list_tabname = ['Full', 'LC', 'MM']
    list_factor = model_LC.quant_factor + model_MM.quant_factor
    list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier)
    list_year = ['==2016','==2017','==2018','<=2018']
    list_colname = ['2016_SD','2017_SD','2018_SD','ALL_SD']
    list_colname2 = ['2016_obs','2017_obs','2018_obs','ALL_obs']
    
    writer = pd.ExcelWriter(filename)
    for i, df in enumerate(list_dataset):
        result = pd.DataFrame()
        for p,cond in enumerate(list_year):
            df_tmp = df.query(f'year{cond}')
            list_sd = []
            list_obs = []
            for q, factor in enumerate(list_factor):
                df_tmp_dropna = df_tmp.dropna(subset=[factor])
                list_obs.append(len(df_tmp_dropna))
                list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
            result[list_colname[p]] = list_sd
            result[list_colname2[p]] = list_obs
    
        result.index = list_factor
        result.to_excel(writer, list_tabname[i])
    
    writer.save()



sfa_result(df_am, r'SFA/SFA_results_AM_3year.xlsx')

sfa_result(df_nam, r'SFA/SFA_results_NAM_3year.xlsx')

sfa_result(df_eu, r'SFA/SFA_results_EU_3year.xlsx')

sfa_result(df_row, r'SFA/SFA_results_RoW_3year.xlsx')

sfa_result(pd.concat([df_am, df_eu]), r'SFA/SFA_results_AM+EU_3year.xlsx')

# -*- coding: utf-8 -*-
"""
Created on Mon May 18 12:42:58 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
df_am = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_SFA_BvD_2008-2019.pkl')
df_nam = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_SFA_BvD_2008-2019.pkl')

df_eu = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_SFA_BvD_2008-2019.pkl')
df_row = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_row_SFA_BvD_2008-2019.pkl')


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    


def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 



df_am = build_cni_factors(df_am)
df_nam = build_cni_factors(df_nam)
df_eu = build_cni_factors(df_eu)
df_row = build_cni_factors(df_row)

df_am = other_processing(df_am)
df_nam = other_processing(df_nam)
df_eu = other_processing(df_eu)
df_row = other_processing(df_row)






# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df_am[factor] = df_am[factor].clip(
            np.nanmin(df_am[factor][df_am[factor] != -np.inf]), 
            np.nanmax(df_am[factor][df_am[factor] != np.inf]))
        df_nam[factor] = df_nam[factor].clip(
            np.nanmin(df_nam[factor][df_nam[factor] != -np.inf]),
             np.nanmax(df_nam[factor][df_nam[factor] != np.inf]))
        df_eu[factor] = df_eu[factor].clip(
            np.nanmin(df_eu[factor][df_eu[factor] != -np.inf]),
             np.nanmax(df_eu[factor][df_eu[factor] != np.inf]))
        df_row[factor] = df_row[factor].clip(
            np.nanmin(df_row[factor][df_row[factor] != -np.inf]),
             np.nanmax(df_row[factor][df_row[factor] != np.inf]))
    
 
   



def get_tokyo_rating(dat, model):

    pdrr_tokyo_mapping = dict(zip(MS.PDRR, MS.BTMU))
    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
        rating_tokyo = pdrr_tokyo_mapping[rating_PDRR]
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)
        list_Ratings_Tokyo.append(rating_tokyo)
    
    dat['PD'] = list_PD
    dat['Ratings_PDRR'] = list_Ratings_PDRR
    dat['Ratings_Tokyo'] = list_Ratings_Tokyo

    return (dat)


#%% 
def sfa_quant_result(data, filename):
    dat = data.query('2016<=year<=2018')
    dat_LC = dat.query('BvD_Proxy_Net_Sales_inUSD>1e9')
    dat_MM = dat.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
    list_dataset = [dat, dat_LC, dat_MM]
    list_tabname = ['Full', 'LC', 'MM']
    
    list_factor = model_LC.quant_factor + model_MM.quant_factor
    list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier)
    list_year = ['==2016','==2017','==2018','<=2018']
    list_colname = ['2016_SD','2017_SD','2018_SD','ALL_SD']
    list_colname2 = ['2016_obs','2017_obs','2018_obs','ALL_obs']
    
    writer = pd.ExcelWriter(filename)
    for i, df in enumerate(list_dataset):
        result = pd.DataFrame()
        list_result=[]
        # by LC model
        dat_LC = df.dropna(subset=model_LC.quant_factor)
        dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC_norm['quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
        dat_LC_norm['quant_score'] = 50*(dat_LC_norm['quant_score']-model_LC.quantmean) / model_LC.quantstd
        dat_LC_norm = get_tokyo_rating(dat_LC_norm, model_LC)
        # by MM model
        dat_MM = df.dropna(subset=model_MM.quant_factor)
        dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM_norm['quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
        dat_MM_norm['quant_score'] = 50*(dat_MM_norm['quant_score']-model_MM.quantmean) / model_MM.quantstd
        dat_MM_norm = get_tokyo_rating(dat_MM_norm, model_MM)
    
    
        list_result.append(len(dat_LC_norm))
        list_result.append(SomersD(dat_LC_norm['Applied Rating'], dat_LC_norm['quant_score']))
        list_result.append(SomersD(dat_LC_norm['Applied Rating'], dat_LC_norm['Ratings_Tokyo']))
        list_result.append(len(dat_MM_norm))
        list_result.append(SomersD(dat_MM_norm['Applied Rating'], dat_MM_norm['quant_score']))
        list_result.append(SomersD(dat_MM_norm['Applied Rating'], dat_MM_norm['Ratings_Tokyo']))
        result[' '] = list_result
        result.index = ['valid_obs_by_LC_model', 'SD_LC_quant_score', 'SD_LC_ratings',
        'valid_obs_by_MM_model', 'SD_MM_quant_score', 'SD_MM_ratings',]
    
        result.to_excel(writer, list_tabname[i])
    
        TM_table_LC = pd.DataFrame(TMstats(dat_LC_norm, 'Ratings_PDRR', 'Applied Rating PDRR', PDRR=range(1,16)), index=['LC_model_result'])
        TM_table_LC.to_excel(writer, list_tabname[i], startrow=9)
        TM_table_MM = pd.DataFrame(TMstats(dat_MM_norm, 'Ratings_PDRR', 'Applied Rating PDRR', PDRR=range(1,16)), index=['MM_model_result'])
        TM_table_MM.to_excel(writer, list_tabname[i], startrow=13)
    
    
    
    writer.save()



sfa_quant_result(df_am, r'SFA/SFA_quant_results_AM_3year.xlsx')
sfa_quant_result(df_nam, r'SFA/SFA_quant_results_NAM_3year.xlsx')
sfa_quant_result(df_eu, r'SFA/SFA_quant_results_EU_3year.xlsx')
sfa_quant_result(df_row, r'SFA/SFA_quant_results_RoW_3year.xlsx')
sfa_quant_result(pd.concat([df_am, df_eu]), r'SFA/SFA_quant_results_AM+EU_3year.xlsx')






#%% add benchmark
def cal_benchmark(data):
    dat = data.query('2016<=year<=2018')
    dat = dat.query('BvD_Proxy_Net_Sales_inUSD==BvD_Proxy_Net_Sales_inUSD')
    dat_LC = dat.query('BvD_Proxy_Net_Sales_inUSD>1e9')
    dat_MM = dat.query('BvD_Proxy_Net_Sales_inUSD<=1e9')

    list_dataset = [dat, dat_LC, dat_MM]

    list_size=[]
    list_sd=[]
    for df in list_dataset:
        tmp = df.dropna(subset=['Current Total Score', 'Applied Rating'])
        list_size.append(len(tmp))
        # since the larger the score is, the better rating we get. We need to flip the sign
        list_sd.append(-SomersD(tmp['Applied Rating'], tmp['Current Total Score']))
    for i in range(3):
        print(list_size[i])
        print(list_sd[i])


cal_benchmark(df_am)
336
0.3249063257659246
82
0.02690100430416069
254
0.4403415829841069

cal_benchmark(df_eu)
881
0.2608789194043254
193
0.06724045185583648
688
0.38227702009923387

cal_benchmark(df_row)
5618
0.55597180490371
218
0.3545129054667878
5400
0.5630587249570274


cal_benchmark(pd.concat([df_am, df_eu]))
1217
0.2931508784146656
275
0.00841040183030967
942
0.41950608872899686

cal_benchmark(df_nam)
6499
0.5110856208954311
411
0.12183034853183376
6088
0.5439936648041924




#%% add benchmark
def cal_benchmark2(data):
    dat = data.query('2016<=year<=2018')
    dat = dat.query('BvD_Proxy_Net_Sales_inUSD==BvD_Proxy_Net_Sales_inUSD')
    dat_LC = dat.query('BvD_Proxy_Net_Sales_inUSD>1e9')
    dat_MM = dat.query('BvD_Proxy_Net_Sales_inUSD<=1e9')

    list_dataset = [dat, dat_LC, dat_MM]

    list_size=[]
    list_sd=[]
    for df in list_dataset:
        tmp = df.dropna(subset=['Primary Evaluation', 'Applied Rating'])
        list_size.append(len(tmp))
        # since the larger the score is, the better rating we get. We need to flip the sign
        list_sd.append(SomersD(tmp['Applied Rating'], tmp['Primary Evaluation']))
    for i in range(3):
        print(list_size[i])
        print(list_sd[i])


cal_benchmark2(df_am)
512
0.3520805220252509
217
0.25250037694124744
295
0.4382639503985828

cal_benchmark2(df_eu)
1171
0.4998271606224997
303
0.4357264818277909
868
0.4771044303797468

cal_benchmark2(df_row)
5623
0.5260360260749327
220
0.3704269095120282
5403
0.5321673248399976


cal_benchmark2(pd.concat([df_am, df_eu]))
1683
0.4945046634736577
520
0.320753389726184
1163
0.493681523071406

cal_benchmark2(df_nam)
6794
0.5239868935911581
523
0.41288723894517393
6271
0.5292719557412381# -*- coding: utf-8 -*-
"""
Created on Thu May 21 11:39:13 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
#dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_americas_SFA_BvD_2008-2019.pkl')
#dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_nonamericas_SFA_BvD_2008-2019.pkl')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_eu_SFA_BvD_2008-2019.pkl')

cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']



#%% build CnI ratios
dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
# for invalid_neg
dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']


# MM new transformation
dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb



#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios

dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']





factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))


dat.to_pickle(r'EXP\dat_eu_withRatios.pkl')
#dat.to_pickle(r'EXP\dat_am_withRatios.pkl')
#dat.to_pickle(r'EXP\dat_nam_withRatios.pkl')
# -*- coding: utf-8 -*-
"""
Created on Thu May 21 11:39:13 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')

df_am = pd.read_pickle(r'EXP\dat_am_withRatios.pkl')
df_eu = pd.read_pickle(r'EXP\dat_eu_withRatios.pkl')
# MM porfolio
dat = pd.concat([df_am, df_eu]).query('BvD_Proxy_Net_Sales_inUSD<=1e9').query('2016<=year<=2018')


#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    list_quality.append(len(df_tmp_dropna)/ len(dat))
    list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor])))

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'EXP/SFA_AM+EU_MM_3year.xlsx')



#%% pick SD > 0.17
list_short = result.query('SomersD>=0.15 and DataQuality>=0.7').index.tolist()





# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(np.abs(dat[list_short].corr()), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)



# after manual selection
list_final=[
'MM_NOP_to_NS',
'MM_TangNW_to_TA', 
'prof@NOP_to_TA',
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'BvD_Shareholders funds',
'prof@NOP_to_TangNW',
'bs@TL_to_Capt',
'BvD_Net current assets']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')
    print(f'5% of {factor} is {tmp[factor].quantile(0.05)}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,3,p)
    sns.distplot(tmp[factor])
    plt.title(factor)


dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1
dat['BvD_Shareholders funds'] = dat['BvD_Shareholders funds'] + np.abs(dat['BvD_Shareholders funds'].min()) +1 
dat['BvD_Net current assets'] = dat['BvD_Net current assets'] + np.abs(dat['BvD_Net current assets'].min()) +1 


list_transform = [0, 0, 0, 1, 1, 1, 0, 0, 1]



#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'MM_NOP_to_NS',
'MM_TangNW_to_TA', 
'prof@NOP_to_TA',
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'BvD_Shareholders funds',
'prof@NOP_to_TangNW',
'bs@TL_to_Capt',
'BvD_Net current assets']

for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)


list_neg = [0, 0, 0, 0, 0, 0, 'BvD_Proxy_Tangible_Net_Worth_inUSD', 
'BvD_Proxy_Capitalization_inUSD', 0]



#%% MFA
dat_norm = dat[list_final+['Applied Rating PDRR', 'year','BvD_Proxy_Tangible_Net_Worth_inUSD', 
'BvD_Proxy_Capitalization_inUSD']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection

writer = pd.ExcelWriter(f'EXP\\MM_AM+EU_MFA_modelselection_tran_ineg.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+['Applied Rating PDRR', 'year','BvD_Proxy_Tangible_Net_Worth_inUSD', 
'BvD_Proxy_Capitalization_inUSD']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(f'EXP\\MM_AM+EU_MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[2:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()






#%%
dicretization output

# -*- coding: utf-8 -*-
"""
Created on Thu May 21 11:39:13 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat_full = pd.read_pickle(r'EXP\dat_am_withRatios.pkl')

dat = dat_full.query('BvD_Proxy_Net_Sales_inUSD<=1e9')


#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    list_quality.append(len(df_tmp_dropna)/ len(dat))
    list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor])))

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'EXP/SFA_am_MM.xlsx')


#%% pick SD > 0.20
list_short = result.query('SomersD>=0.2 and DataQuality>=0.7').index.tolist()





# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)



# after manual selection
list_final=[
'MM_NOP_to_NS',
'new@COGSRatio',
'BvD_P/L for period [=Net income] ',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'BvD_Gross profit',
'new@GrossMarkup',
'bs@CL_to_Capt',
'BvD_Shareholders funds',
'liq@CA_to_TA']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(2,5,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [0, 0, 0, 0, 0, 'BvD_Gross profit', 'new@GrossMarkup',0,
 'BvD_Shareholders funds', 0]



#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'new@COGSRatio',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'bs@CL_to_Capt',
'liq@CA_to_TA'
]
for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)


list_neg = [0, 0, 0, 0, 0, 0, 0,
'BvD_Proxy_Capitalization_inUSD', 0, 0]



#%% MFA
dat_norm = dat[list_final+['Applied Rating PDRR','BvD_Proxy_Capitalization_inUSD']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection

writer = pd.ExcelWriter(f'EXP\\MM_am_MFA_modelselection_tran_ineg.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model



dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+['Applied Rating PDRR','BvD_Proxy_Capitalization_inUSD', 'year']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(f'EXP\\MM_am_MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[1:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()



#%% test model performance in specific time window
pl_model = [
['MM_quantscore', 'new@COGSRatio', 'BvD_P/L for period [=Net income] '],
['MM_NOP_to_NS', 'new@COGSRatio', 'BvD_P/L for period [=Net income] '],
['MM_quantscore', 'new@COGSRatio', 'BvD_P/L for period [=Net income] ', 'BvD_Gross profit'],
['MM_NOP_to_NS', 'new@COGSRatio', 'BvD_P/L for period [=Net income] ', 'BvD_Shareholders funds']
]


pl_wts = [
[0.2760324498451464, 0.32910693847868816, 0.39486061167616543],
[0.5490265572037817, 0.22192574077591296, 0.2290477020203053],
[0.24017217165783636, 0.1873908152898595, 0.29393683458372544, 0.2785001784685788],
[0.5149208809202723, 0.11615951364216957, 0.15103506868066563, 0.21788453675689246]
]

pl_desc = [
'3-factor_forced MM quant score',
'3-factor_no restriction',
'4-factor_forced MM quant score',
'4-factor_no restriction',
]

def sd_fun(data):
    return (SomersD(data['Applied Rating PDRR'], data['score']))

writer = pd.ExcelWriter(f'EXP\\SD_3year_MM_am.xlsx')
dat_20161718 = dat_norm.query('2016<=year<=2018')
for i, model_setting in enumerate(pl_model):
    pl_result=[]
    pl_result.append(model_setting)
    pl_result.append(pl_wts[i])

    df = dat_20161718.dropna(subset=model_setting+['Applied Rating PDRR', 'year'], how='any')
    df_all = dat_norm.dropna(subset=model_setting+['Applied Rating PDRR', 'year'], how='any')

    df['score'] = (pl_wts[i]*df[model_setting]).sum(axis=1)
    df_all['score'] = (pl_wts[i]*df_all[model_setting]).sum(axis=1)
    pl_result.append(SomersD(df_all['Applied Rating PDRR'], df_all['score']))
    pl_result.append(len(df_all))

    pl_result.append(SomersD(df['Applied Rating PDRR'], df['score']))
    pl_result.append(len(df))
    pl_result =pl_result + df.groupby(by='year').apply(sd_fun).sort_index().tolist()
    pl_result =pl_result + df['year'].value_counts().sort_index().tolist()

    df_result = pd.DataFrame()
    df_result['Stats'] = pl_result
    df_result.index = ['Model', 'Weights', 'SomersD_allyears', 'Size_allyears', 'SomersD_3years', 'Size_3years',
    'SomersD_2016', 'SomersD_2017','SomersD_2018', 'Size_Year 2016', 'Size_Year 2017', 'Size_Year 2018']


    df_result.loc[['Model', 'Weights', 'SomersD_allyears', 'Size_allyears', 'SomersD_3years', 'Size_3years',
    'SomersD_2016', 'Size_Year 2016', 'SomersD_2017','Size_Year 2017', 'SomersD_2018', 'Size_Year 2018']].to_excel(writer, pl_desc[i])
writer.save()


#%%
dicretization output

# -*- coding: utf-8 -*-
"""
Created on Thu May 21 11:39:13 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat_full = pd.read_pickle(r'EXP\dat_nam_withRatios.pkl')

dat = dat_full.query('BvD_Proxy_Net_Sales_inUSD<=1e9')


#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    list_quality.append(len(df_tmp_dropna)/ len(dat))
    list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor])))

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'EXP/SFA_nonam_MM.xlsx')


#%% pick SD > 0.17
list_short = result.query('SomersD>=0.17 and DataQuality>=0.7').index.tolist()





# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(np.abs(dat[list_short].corr()), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)



# after manual selection
list_final=[
'MM_TangNW_to_TA', 
'MM_NOP_to_NS',
'MM_TD_to_Capt',
'liq@CA_to_TL',
'BvD_P/L for period [=Net income] ',
'act@NS_to_CL',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_TD',
'size@TangNW_to_TA_exc_CA'] 


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,3,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [0, 0, 0, 0, 0, 0, 0, 0, 0]



#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'MM_TangNW_to_TA', 
'MM_NOP_to_NS',
'MM_TD_to_Capt',
'liq@CA_to_TL',
'act@NS_to_CL',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_TD',
'size@TangNW_to_TA_exc_CA'] 

for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)


list_neg = [0, 0, 0, 0, 0, 0, 0, 0, 0]



#%% MFA
dat_norm = dat[list_final+['Applied Rating PDRR']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection

writer = pd.ExcelWriter(f'EXP\\MM_nonam_MFA_modelselection_tran_ineg.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+['Applied Rating PDRR', 'year']].copy()
list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(f'EXP\\MM_nonam_MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[3:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()




#%% test model performance in specific time window
pl_model = [
['MM_quantscore', 'BvD_P/L for period [=Net income] ', 'act@NS_to_CL'],
['MM_TangNW_to_TA', 'MM_NOP_to_NS', 'MM_TD_to_Capt'],
['MM_quantscore', 'BvD_P/L for period [=Net income] ', 'act@NS_to_CL', 'liq@CA_exc_CL_to_CA'],
['MM_TangNW_to_TA', 'MM_NOP_to_NS', 'MM_TD_to_Capt', 'act@NS_to_CL']
]


pl_wts = [
[0.6688393678686652, 0.25122288271927945, 0.07993774941205531],
[0.3140555408651869, 0.49440358720272076, 0.19154087193209246],
[0.6673877348286364, 0.25101356864725605, 0.07884720162529875, 0.002751494898808766],
[0.26750934594307624, 0.4739852217430573, 0.1719935833624545, 0.08651184895141205]
]

pl_desc = [
'3-factor_forced MM quant score',
'3-factor_no restriction',
'4-factor_forced MM quant score',
'4-factor_no restriction',
]

def sd_fun(data):
    return (SomersD(data['Applied Rating PDRR'], data['score']))


writer = pd.ExcelWriter(f'EXP\\SD_3year_MM_nam.xlsx')
dat_20161718 = dat_norm.query('2016<=year<=2018')
for i, model_setting in enumerate(pl_model):
    pl_result=[]
    pl_result.append(model_setting)
    pl_result.append(pl_wts[i])

    df = dat_20161718.dropna(subset=model_setting+['Applied Rating PDRR', 'year'], how='any')
    df_all = dat_norm.dropna(subset=model_setting+['Applied Rating PDRR', 'year'], how='any')

    df['score'] = (pl_wts[i]*df[model_setting]).sum(axis=1)
    df_all['score'] = (pl_wts[i]*df_all[model_setting]).sum(axis=1)
    pl_result.append(SomersD(df_all['Applied Rating PDRR'], df_all['score']))
    pl_result.append(len(df_all))

    pl_result.append(SomersD(df['Applied Rating PDRR'], df['score']))
    pl_result.append(len(df))
    pl_result =pl_result + df.groupby(by='year').apply(sd_fun).sort_index().tolist()
    pl_result =pl_result + df['year'].value_counts().sort_index().tolist()

    df_result = pd.DataFrame()
    df_result['Stats'] = pl_result
    df_result.index = ['Model', 'Weights', 'SomersD_allyears', 'Size_allyears', 'SomersD_3years', 'Size_3years',
    'SomersD_2016', 'SomersD_2017','SomersD_2018', 'Size_Year 2016', 'Size_Year 2017', 'Size_Year 2018']


    df_result.loc[['Model', 'Weights', 'SomersD_allyears', 'Size_allyears', 'SomersD_3years', 'Size_3years',
    'SomersD_2016', 'Size_Year 2016', 'SomersD_2017','Size_Year 2017', 'SomersD_2018', 'Size_Year 2018']].to_excel(writer, pl_desc[i])
writer.save()




#%%
dicretization output

# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter

os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCARS 2008-2019_matches2.pkl')


#%% Drop defaulters
mask = dat['Applied Rating'].isin([83, 90, 101, 102])
dat = dat[~mask]

#%% Drop rating 2-3
mask = dat['Applied Rating']==23
dat = dat[~mask]


#%% tag Geo info
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286
]

list_EMEA_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

dat['Geo'] = 'Asia'
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(list_americas_office_code), 'Americas')
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(list_EMEA_office_code), 'EMEA')



#%% Cast integer rating to string
dat['BTMU Rating'] = dat['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat['BTMU Rating'].value_counts().sort_index() 
'''
1-0      533
2-0     7231
2-1      134
2-2      369
3-0    18852
3-1      837
3-2     1540
3-3     1474
4-0    23073
5-1    10455
5-2     5634
6-1     4883
6-2     6239
7-0    11167
8-1     5979
8-2     4955
Name: BTMU Rating, dtype: int64

'''
#dat.query('Geo=="Americas"')['BTMU Rating'].value_counts().sort_index() 
#dat.query('Geo=="EMEA"')['BTMU Rating'].value_counts().sort_index() 
#dat.query('Geo=="Asia"')['BTMU Rating'].value_counts().sort_index() 



#%% Map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['Applied Rating PDRR'] = dat['Applied Rating'].transform(lambda x: pd_mapping[x])



#%% convert to numeric value
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

gcar_converter(dat, curr, inplace=True)


#%% Choose the columns we need 
cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'Secondary Evaluation',
'Third Evaluation',
'BTMU Rating',
'Final Result of Evaluation based on Financial Substance Score',
'Applied Rating',
'Applied Rating PDRR',
'Match',
'Borrower Industry',
'Industry Model',
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

dat[cols].to_pickle(r'..\data\dat_2008-2019_cleaned.pkl')



# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner


df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned.pkl')



bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(dat, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


df_bvd['Fiscal year'].value_counts().sort_index()
'''
Out[20]: 
1984       1
1985       1
1986       1
1987       1
1988       1
1989       2
1990       4
1991       5
1992       7
1993      12
1994      22
1995      29
1996      32
1997      35
1998      47
1999      82
2000     132
2001     209
2002     336
2003     433
2004     471
2005     617
2006    1078
2007    1692
2008    2962
2009    6279
2010    6813
2011    7214
2012    7572
2013    8556
2014    9367
2015    8940
2016    9075
2017    8597
2018    6637
2019     250
2020       3
Name: BvD_Fiscal year, dtype: int64
'''

len(set(dat['Borrower CIF'].unique()))
# 23102

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(dat['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 10727




#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% merge
df_merged = pd.merge(dat, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_merged['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_merged['BvD_CIF']]
df_merged['inBvD'].value_counts()

'''
Out[11]: 
No     61244
Yes    45052
Name: inBvD, dtype: int64

'''
df_merged.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_BvD.pkl')


df_merged_3yr = df_merged.query('2016<=year<=2018')
df_merged_3yr.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2016-2018_cleaned_BvD.pkl')

# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np
import pickle
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
lmdb = 0.3
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']

dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2016-2018_cleaned_BvD.pkl')


cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']



#%% build CnI ratios
dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
# for invalid_neg
dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']


# MM new transformation
dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb



#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios
dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']



factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))



#%% build current tokyo model factors:
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']
# fill inf
for factor in curr:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))




dat['Other#1'] = dat['BvD_Cash flow']
dat.groupby('Borrower Type')['Other#1'].mean()
'''Borrower Type
Corporation (Major Company)    1.609592e+09
Corporation (Other Company)    5.704757e+07 ****
Name: Other#1, dtype: float64
'''
dat['Other#1'] = dat['Other#1'].mask(dat['Borrower Type']=="Corporation (Other Company)",  dat['Current Financial Data #1'])
dat.groupby('Borrower Type')['Other#1'].mean()
'''
Borrower Type
Corporation (Major Company)    1.609592e+09
Corporation (Other Company)    3.716528e+08 ****
Name: Other#1, dtype: float64
'''



dat['Other#3'] = dat['LC_UBEBITDA_to_IE']
dat.groupby('Borrower Type')['Other#3'].mean()
'''
Borrower Type
Corporation (Major Company)     425.914331
Corporation (Other Company)    4939.195581  ****
Name: Other#3, dtype: float64
'''
dat['Other#3'] = dat['Other#3'].mask(dat['Borrower Type']=="Corporation (Other Company)",  dat['Current Financial Data #3'])
dat.groupby('Borrower Type')['Other#3'].mean()
'''
Borrower Type
Corporation (Major Company)    425.914331
Corporation (Other Company)    469.930354   ****
Name: Other#3, dtype: float64
'''

dat['SP#3'] = dat['LC_UBEBITDA_to_IE']
dat.groupby('Borrower Type')['SP#3'].mean()
'''
Borrower Type
Corporation (Major Company)     425.914331  ****
Corporation (Other Company)    4939.195581  
Name: SP#3, dtype: float64
'''
dat['SP#3'] = dat['SP#3'].mask(dat['Borrower Type']=="Corporation (Major Company)",  dat['Current Financial Data #3'])
dat.groupby('Borrower Type')['SP#3'].mean()
'''
Borrower Type
Corporation (Major Company)    86309.951587 ****
Corporation (Other Company)     4939.195581
Name: SP#3, dtype: float64
'''



dat['Other#6'] = dat['BvD_Operating revenue (Turnover)'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['Other#6'] = dat['Other#6'].clip(
            np.nanmin(dat['Other#6'][dat['Other#6'] != -np.inf]), 
            np.nanmax(dat['Other#6'][dat['Other#6'] != np.inf]))

dat.groupby('Borrower Type')['Other#6'].mean()
'''
Borrower Type
Corporation (Major Company)    3.327465
Corporation (Other Company)    2.392267 ****
Name: Other#6, dtype: float64
'''
dat['Other#6'] = dat['Other#6'].mask(dat['Borrower Type']=="Corporation (Other Company)",  dat['Current Financial Data #6'])
dat.groupby('Borrower Type')['Other#6'].mean()
'''
Borrower Type
Corporation (Major Company)    3.327465
Corporation (Other Company)   -3.562748 ****  
Name: Other#6, dtype: float64
'''


dat['Other#10'] = dat['BvD_Proxy_Net_Sales_inUSD']
dat.groupby('Borrower Type')['Other#10'].mean()
'''Borrower Type
Corporation (Major Company)    1.077222e+10
Corporation (Other Company)    5.117881e+08 ****
Name: Other#10, dtype: float64
'''
dat['Other#10'] = dat['Other#10'].mask(dat['Borrower Type']=="Corporation (Other Company)",  dat['Current Financial Data #10'])
dat.groupby('Borrower Type')['Other#10'].mean()
'''
Borrower Type
Corporation (Major Company)    1.077222e+10
Corporation (Other Company)    2.885040e+09 ****
Name: Other#10, dtype: float64
'''

list_toremove = [
 'Other#1',
 'Other#6',
 'Other#10',
 'Other#3',
 'SP#3']

dat.drop(columns=list_toremove, inplace=True)


#%% rename current factor
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

for name in curr:
    dat['Other_'+name] = np.nan
    dat['SP_'+name] = np.nan

    dat['Other_'+name] = dat['Other_'+name].mask(dat['Borrower Type']=="Corporation (Other Company)", dat[name])
    dat['SP_'+name] = dat['SP_'+name].mask(dat['Borrower Type']!="Corporation (Other Company)", dat[name])


dat.drop(columns=curr, inplace=True)



#%% label geo info
dat = pd.concat([dat, pd.get_dummies(dat['Geo'], prefix='Geo')], axis=1)

#%% label tokyo model info
dat = pd.concat([dat, pd.get_dummies(dat['Borrower Type'], prefix='Tokyo')], axis=1)



#%% add portfolio info
dat['portfolio'] = 'unknown'
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']>1e9, 'LC')
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']<=1e9, 'MM')

dat['portfolio'].value_counts()
'''
unknown    17438
MM          9932
LC          1781

'''



#%% split
dat_LC = dat.query('portfolio=="LC"')
dat_MM = dat.query('portfolio=="MM"')



dat_LC.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready.pkl')
dat_MM.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready.pkl')# -*- coding: utf-8 -*-
"""
Created on Wed Jul  1 17:33:42 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1428
1     353
Name: Gua_override, dtype: int64
'''


dat = dat.query('Gua_override==0')

#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
    else:
        list_sd.append(np.nan)

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.23
list_short = result.query('SomersD>=0.23 and DataQuality>=0.6').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'LC_UBEBITDA_to_IE', 
'LC_Total Assets',
'LC_TD_to_Capt', 
'BvD_P/L for period [=Net income] ',
'BvD_Taxation',
'new@CashFlowRatio',
'new@GrossMarkup',
'MM_NOP_to_NS',
'bs@TL_to_TA',]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,4,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,4,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
0, 
0,
0, 
'BvD_P/L for period [=Net income] ',
'BvD_Taxation',
0,
'new@GrossMarkup',
0,
0,]



dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1
dat['new@GrossMarkup'] = dat['new@GrossMarkup'] + np.abs(dat['new@GrossMarkup'].min()) +1






#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'LC_UBEBITDA_to_IE', 
'LC_Total Assets',
'LC_TD_to_Capt', 
'new@CashFlowRatio',
'MM_NOP_to_NS',
'bs@TL_to_TA',]


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0, 0, 0, 0]

# add 'LC_UBEBITDA_to_IE'
list_neg = ['BvD_Proxy_Interest_expense_inUSD', 0, 0, 0, 0, 0, 0, 0, 0]





#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_noGua.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['LC_quantscore'] = dat_LC_norm['LC_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_noGua_keepLC.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[3:], factor_num):
        model = ['LC_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()




#%% test model performance in specific time window
pl_model = [
['LC_UBEBITDA_to_IE', 'BvD_P/L for period [=Net income] ', 'new@GrossProfitRatio', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'LC_Total Assets', 'MM_NOP_to_NS', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'new@GrossMarkup', 'MM_NOP_to_NS', 'bs@TL_to_TA', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'MM_NOP_to_NS', 'BvD_Current liabilities', 'bs@TL_to_TA', 'BvD_Number of employees'],

['LC_quantscore', 'MM_NOP_to_NS', 'ds@TL_to_IE', 'BvD_Number of employees'],
['LC_quantscore', 'BvD_P/L for period [=Net income] ', 'bs@TL_to_TA', 'new@GrossProfitRatio', 'BvD_Number of employees'],
['LC_quantscore', 'MM_NOP_to_NS', 'ds@TL_to_IE', 'bs@TL_to_TA', 'BvD_Number of employees'],
]


pl_wts = [
[0.3470943676883373, 0.3094237495903403, 0.2241067886496746, 0.11937509407164787],
[0.3028042522301452, 0.28754401464417806, 0.21049849683271868, 0.19915323629295809],
[0.2017635865470767, 0.11851394096645368, 0.25492115111394326, 0.1873349258684585, 0.2374663955040679],
[0.21452340344731483, 0.2579658156565938, 0.21283526809987038, 0.17506720142423865, 0.13960831137198226],

[0.3000239419148258, 0.27318194963235026, 0.1612281695398581, 0.2655659389129658],
[0.3082001561292678, 0.22668572659732553, 0.19389857177200756, 0.10350008941273288, 0.1677154560886661],
[0.16978881028899634, 0.255742355367358, 0.17453387707690923, 0.1480144791140035, 0.25192047815273294]
]

pl_desc = [
'4-factor_1',
'4-factor_2',
'5-factor_1',
'5-factor_2',
'LC+3-factor',
'LC+4-factor_1',
'LC+4-factor_2',
]


def calc_fun(list_dataset, list_dataset_name):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_new = dat.dropna(subset=model_setting, how='any')

        dat_new['score'] = (pl_wts[i]*dat_new[model_setting]).sum(axis=1)
        list_obs_newmodel.append(len(dat_new))
        if len(dat_new)>5:
            list_sd_newmodel.append(SomersD(dat_new['Applied Rating PDRR'], dat_new['score']))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)



#%% Size
df1 = dat_norm.query('Geo_Americas ==1')
df2 = dat_norm.query('Geo_EMEA ==1')
df3 = dat_norm.query('Geo_Asia ==1')


#%% Year
df4 = dat_norm.query('year==2016')
df5 = dat_norm.query('year==2017')
df6 = dat_norm.query('year==2018')

#%% model
df7 = dat_norm[dat_norm['Borrower Type']=='Corporation (Major Company)']
df8 = dat_norm[dat_norm['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(dat_norm['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(dat_norm[dat_norm['Industry Model']==ind])


list_dataset = [dat_norm, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name


writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelperf_original.xlsx')

for i, model_setting in enumerate(pl_model):

    result = calc_fun(list_dataset, list_dataset_name)
    result.to_excel(writer, pl_desc[i])

writer.save()







#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['LC_UBEBITDA_to_IE', 'LC_Total Assets', 'LC_TD_to_Capt', 'BvD_P/L for period [=Net income] ', 'MM_NOP_to_NS']
wts = [0.23174731768774995, 0.13753553928236661, 0.1613268684050183, 0.2142622172846372, 0.25512805734022803]

df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5325123445773098

bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun

optSD_wts = res1.x / res1.x.sum()
#  array([0.26935967, 0.19810441, 0.14292322, 0.18743158, 0.20218111])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5336082487138782






#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5644000237967756
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5333452317211018


score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5644000237967756 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(15,45), (15,45), (15,45)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5653268455808449

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5653268455808449



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= df, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()

df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5653268455808449
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5365233537171503






CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'round', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))




y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5736087496986264

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5524421440894759

df['iso'] = y_
df['iso_round'] = y_round






#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name





def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:

   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\LC\result_noGua.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1428
1     353
Name: Gua_override, dtype: int64
'''
dat = dat.query('Gua_override==0')
dat.reset_index(drop=True, inplace=True)


#%%
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])

dat = dat.iloc[df_temp.query('diff<=4').index,]
dat.reset_index(drop=True, inplace=True)

SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7243048491819798
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7301408184890371

from minepy import MINE
mine = MINE(alpha=0.6, c=15) 



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
list_mic=[]

for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        mine.compute_score(df_tmp_dropna[factor],df_tmp_dropna['Applied Rating PDRR']) 
        list_mic.append(mine.mic() )
    else:
        list_sd.append(np.nan)
        list_mic.append(np.nan)

result['SomersD'] = list_sd
result['MIC'] = list_mic
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)

result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.23
list_short = result.query('SomersD>=0.23 and DataQuality>=0.7').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'LC_Total Assets',
'LC_TD_to_Capt', 
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'new@CashFlowRatio',
'bs@TL_to_TA',
'MM_NOP_to_NS',
'bs@TD_to_TA_exc_TL',
'BvD_Net current assets', # from MIC
'BvD_Working capital'  # from MIC
]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')



for i, factor in enumerate(list_final): 
    tmp = dat_norm[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')




for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(2,5,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
0, 
0,
'BvD_P/L for period [=Net income] ',
'BvD_Taxation',
0,
0,
0,
0,
'BvD_Net current assets', # from MIC
'BvD_Working capital']



dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1
dat['BvD_Net current assets'] = dat['BvD_Net current assets'] + np.abs(dat['BvD_Net current assets'].min()) +1
dat['BvD_Working capital'] = dat['BvD_Working capital'] + np.abs(dat['BvD_Working capital'].min()) +1





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'LC_TD_to_Capt', 
'new@CashFlowRatio',
'bs@TL_to_TA',
'MM_NOP_to_NS',
'bs@TD_to_TA_exc_TL']


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0, 0, 0, 0]

# add 'LC_TD_to_Capt', 
list_neg = [0,'BvD_Proxy_Capitalization_inUSD', 0, 0, 0, 0, 0, 0,0,0]





#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_noGua_nooutliers.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['LC_quantscore'] = dat_LC_norm['LC_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_noGua_nooutliers_keepLC.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[2:], factor_num):
        model = ['LC_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = [
'LC_Total Assets',
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'new@CashFlowRatio',
'MM_NOP_to_NS',
]

wts = [0.23174731768774995, 0.13753553928236661, 0.1613268684050183, 0.2142622172846372, 0.25512805734022803]

df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5325123445773098

bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun

optSD_wts = res1.x / res1.x.sum()
#  array([0.26935967, 0.19810441, 0.14292322, 0.18743158, 0.20218111])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5336082487138782






#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5644000237967756
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5333452317211018


score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5644000237967756 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(15,45), (15,45), (15,45)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5653268455808449

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5653268455808449



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= df, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()

df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5653268455808449
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5365233537171503






CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'round', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))




y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5736087496986264

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5524421440894759

df['iso'] = y_
df['iso_round'] = y_round






#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name





def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:

   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\LC\result_noGua.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Wed Jul  1 17:33:42 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1428
1     353
Name: Gua_override, dtype: int64
'''




#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
    else:
        list_sd.append(np.nan)

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.20
list_short = result.query('SomersD>=0.2 and DataQuality>=0.6').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'LC_UBEBITDA_to_IE', 
'LC_UBEBITDA_to_NS',
'LC_Total Assets',
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'new@GrossMarkup',
'MM_NOP_to_NS',
'BvD_Current liabilities',
'ds@TL_to_IE',
'bs@TL_to_TA',
'new@GrossProfitRatio',
'BvD_Number of employees']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,4,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,4,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [0, 0, 0,'BvD_P/L for period [=Net income] ', 'BvD_Taxation',  'new@GrossMarkup', 0,
 'BvD_Current liabilities', 0,0,0,0]


dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'LC_UBEBITDA_to_IE', 
'LC_UBEBITDA_to_NS',
'MM_NOP_to_NS',
'ds@TL_to_IE',
'bs@TL_to_TA',
'new@GrossProfitRatio',
]
for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0, 'ds@TL_to_IE', 0, 0]

# add 'LC_UBEBITDA_to_IE'
list_neg = ['BvD_Proxy_Interest_expense_inUSD', 0, 0, 0, 0, 0, 0,
 0, 'BvD_Proxy_Interest_expense_inUSD', 0, 0, 0]





#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'Cty_Rating_Label'
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['LC_quantscore'] = dat_LC_norm['LC_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelselection_keepLC.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[3:], factor_num):
        model = ['LC_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()




#%% test model performance in specific time window
pl_model = [
['LC_UBEBITDA_to_IE', 'BvD_P/L for period [=Net income] ', 'new@GrossProfitRatio', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'LC_Total Assets', 'MM_NOP_to_NS', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'new@GrossMarkup', 'MM_NOP_to_NS', 'bs@TL_to_TA', 'BvD_Number of employees'],
['LC_UBEBITDA_to_IE', 'MM_NOP_to_NS', 'BvD_Current liabilities', 'bs@TL_to_TA', 'BvD_Number of employees'],

['LC_quantscore', 'MM_NOP_to_NS', 'ds@TL_to_IE', 'BvD_Number of employees'],
['LC_quantscore', 'BvD_P/L for period [=Net income] ', 'bs@TL_to_TA', 'new@GrossProfitRatio', 'BvD_Number of employees'],
['LC_quantscore', 'MM_NOP_to_NS', 'ds@TL_to_IE', 'bs@TL_to_TA', 'BvD_Number of employees'],
]


pl_wts = [
[0.3470943676883373, 0.3094237495903403, 0.2241067886496746, 0.11937509407164787],
[0.3028042522301452, 0.28754401464417806, 0.21049849683271868, 0.19915323629295809],
[0.2017635865470767, 0.11851394096645368, 0.25492115111394326, 0.1873349258684585, 0.2374663955040679],
[0.21452340344731483, 0.2579658156565938, 0.21283526809987038, 0.17506720142423865, 0.13960831137198226],

[0.3000239419148258, 0.27318194963235026, 0.1612281695398581, 0.2655659389129658],
[0.3082001561292678, 0.22668572659732553, 0.19389857177200756, 0.10350008941273288, 0.1677154560886661],
[0.16978881028899634, 0.255742355367358, 0.17453387707690923, 0.1480144791140035, 0.25192047815273294]
]

pl_desc = [
'4-factor_1',
'4-factor_2',
'5-factor_1',
'5-factor_2',
'LC+3-factor',
'LC+4-factor_1',
'LC+4-factor_2',
]


def calc_fun(list_dataset, list_dataset_name):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_new = dat.dropna(subset=model_setting, how='any')

        dat_new['score'] = (pl_wts[i]*dat_new[model_setting]).sum(axis=1)
        list_obs_newmodel.append(len(dat_new))
        if len(dat_new)>5:
            list_sd_newmodel.append(SomersD(dat_new['Applied Rating PDRR'], dat_new['score']))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)



#%% Size
df1 = dat_norm.query('Geo_Americas ==1')
df2 = dat_norm.query('Geo_EMEA ==1')
df3 = dat_norm.query('Geo_Asia ==1')


#%% Year
df4 = dat_norm.query('year==2016')
df5 = dat_norm.query('year==2017')
df6 = dat_norm.query('year==2018')

#%% model
df7 = dat_norm[dat_norm['Borrower Type']=='Corporation (Major Company)']
df8 = dat_norm[dat_norm['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(dat_norm['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(dat_norm[dat_norm['Industry Model']==ind])


list_dataset = [dat_norm, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name


writer = pd.ExcelWriter(r'SFAMFA\LC\MFA_modelperf_original.xlsx')

for i, model_setting in enumerate(pl_model):

    result = calc_fun(list_dataset, list_dataset_name)
    result.to_excel(writer, pl_desc[i])

writer.save()







#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['LC_UBEBITDA_to_IE', 'new@GrossMarkup', 'MM_NOP_to_NS', 'bs@TL_to_TA', 'BvD_Number of employees']
wts = [0.2017635865470767, 0.11851394096645368, 0.25492115111394326, 0.1873349258684585, 0.2374663955040679]
df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5500012719733395

bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun

optSD_wts = res1.x / res1.x.sum()
#  array([0.21192067, 0.16478885, 0.22968535, 0.1883884 , 0.20521674])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5516548373146098






#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5545549365285304
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5325370780228447


score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5545549365285304 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(15,45), (15,45), (15,45)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5580401434785927

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5580401434785927



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= df, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()
df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5545549365285304
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5325370780228447







CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'round', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))




y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5704418835381211

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5513241242463558

df['iso'] = y_
df['iso_round'] = y_round






#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name





def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:

   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\LC\result.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Wed Jul  1 17:33:42 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
Out[201]: 
0    6027
1    3905
Name: Gua_override, dtype: int64
'''



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
    else:
        list_sd.append(np.nan)

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'SFAMFA\MM\SFA.xlsx')


#%% pick SD > 0.20
list_short = result.query('SomersD>=0.2 and DataQuality>=0.6').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'MM_NOP_to_NS', 
'MM_TangNW_to_TA', 
'Other_Current Financial Data #1',
'Other_Current Financial Data #6',
'Other_Current Financial Data #7',
'Other_Current Financial Data #8',
'Other_Current Financial Data #9',
'Other_Current Financial Data #10',
'BvD_P/L for period [=Net income] ',
'BvD_Taxation',
'prof@NOP_to_TA',
'BvD_Gross profit',]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,4,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,4,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [0, 0, 'Other_Current Financial Data #1',0,0,0,
'Other_Current Financial Data #9','Other_Current Financial Data #10','BvD_P/L for period [=Net income] ',
'BvD_Taxation',0,'BvD_Gross profit']


dat['Other_Current Financial Data #1'] = dat['Other_Current Financial Data #1'] + np.abs(dat['Other_Current Financial Data #1'].min()) +1
dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'MM_NOP_to_NS', 
'MM_TangNW_to_TA',
'prof@NOP_to_TA'
]
for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0]


list_neg = [0,]*12





#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'Gua_override',
'Cty_Rating_Label',
'Cty_Rating_Label2',
'Cty_Rating_Label3',
'Cty_Rating_Label4'
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\MM\MFA_modelselection.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'Gua_override',
'Cty_Rating_Label',
'Cty_Rating_Label2',
'Cty_Rating_Label3',
'Cty_Rating_Label4'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\MM\MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[3:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()




#%% test model performance in specific time window
pl_model = [
['MM_TangNW_to_TA', 'Other_Current Financial Data #7', 'Other_Current Financial Data #9', 'prof@NOP_to_TA'],
['MM_TangNW_to_TA', 'Other_Current Financial Data #7', 'Other_Current Financial Data #8', 'Other_Current Financial Data #9', 'prof@NOP_to_TA'],
['MM_quantscore', 'Other_Current Financial Data #7', 'Other_Current Financial Data #8', 'Other_Current Financial Data #9'],
['MM_quantscore', 'Other_Current Financial Data #6', 'Other_Current Financial Data #7', 'Other_Current Financial Data #9', 'Other_Current Financial Data #10']
]


pl_wts = [
[0.08863298057963558, 0.45066161689225986, 0.429129491329704, 0.031575911198400475],
[0.09033936630467564, 0.3772702389878451, 0.08327461539905458, 0.41327286706749405, 0.035842912240930704],
[0.13411697294300234, 0.37033066086384886, 0.06991838722806433, 0.4256339789650845],
[0.16274477658990186, 0.1327840853072716, 0.30059737188942964, 0.21778194951020405, 0.18609181670319294]
]

pl_desc = [
'4-factor',
'5-factor',
'MM+3-factor',
'MM+4-factor',
]


def calc_fun(list_dataset, list_dataset_name):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_new = dat.dropna(subset=model_setting, how='any')

        dat_new['score'] = (pl_wts[i]*dat_new[model_setting]).sum(axis=1)
        list_obs_newmodel.append(len(dat_new))
        if len(dat_new)>5:
            list_sd_newmodel.append(SomersD(dat_new['Applied Rating PDRR'], dat_new['score']))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)



#%% Size
df1 = dat_norm.query('Geo_Americas ==1')
df2 = dat_norm.query('Geo_EMEA ==1')
df3 = dat_norm.query('Geo_Asia ==1')


#%% Year
df4 = dat_norm.query('year==2016')
df5 = dat_norm.query('year==2017')
df6 = dat_norm.query('year==2018')

#%% model
df7 = dat_norm[dat_norm['Borrower Type']=='Corporation (Major Company)']
df8 = dat_norm[dat_norm['Borrower Type']=='Corporation (Other Company)']


#%% Industry
list_dataset_name = list(dat_norm['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(dat_norm[dat_norm['Industry Model']==ind])


list_dataset = [dat_norm, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name


writer = pd.ExcelWriter(r'SFAMFA\MM\MFA_modelperf_original.xlsx')

for i, model_setting in enumerate(pl_model):

    result = calc_fun(list_dataset, list_dataset_name)
    result.to_excel(writer, pl_desc[i])

writer.save()







#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['MM_quantscore', 'Other_Current Financial Data #6', 'Other_Current Financial Data #7', 'Other_Current Financial Data #9', 'Other_Current Financial Data #10']

wts = [0.16274477658990186, 0.1327840853072716, 0.30059737188942964, 0.21778194951020405, 0.18609181670319294]
df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5813386858932407

bounds1 = [(0.1,0.5), (0.1,0.5), (0.1,0.5), (0.1,0.5),(0.1,0.5)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun

optSD_wts = res1.x / res1.x.sum()
#  array([0.19127431, 0.11132909, 0.25876922, 0.23658854, 0.20203884])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
#  0.5825335980383117






#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.481
Model:                             OLS   Adj. R-squared:                  0.481
Method:                  Least Squares   F-statistic:                     1340.
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):               0.00
Time:                         15:53:37   Log-Likelihood:                -9871.1
No. Observations:                 4343   AIC:                         1.975e+04
Df Residuals:                     4339   BIC:                         1.978e+04
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            7.0752      0.079     89.383      0.000       6.920       7.230
score            0.0645      0.001     60.417      0.000       0.062       0.067
Geo_Americas     3.4234      0.224     15.268      0.000       2.984       3.863
Geo_Asia         1.7993      0.084     21.345      0.000       1.634       1.965
Geo_EMEA         1.8525      0.111     16.645      0.000       1.634       2.071
==============================================================================
Omnibus:                       59.991   Durbin-Watson:                   1.798
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               98.353
Skew:                           0.108   Prob(JB):                     4.39e-22
Kurtosis:                       3.705   Cond. No.                     8.58e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 7.45e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""

'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# .5870772559834265
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5713107603347456


score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5870772559834265 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(10,100), (10,100), (10,100)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5879435866320977

score_Americas, score_EMEA, score_Asia = res1.x
# array([66.51779736, 24.97172697, 23.25744774])
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5879435866320977


sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= df, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()
df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj'].mean()

df_plot.plot()
df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.587901288857051
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# .5723921906443253









y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5918184754876011

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5791968452049734

df['iso'] = y_
df['iso_round'] = y_round






#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry

list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']+list_dataset_name





def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:

   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)

    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel

    result.index = list_dataset_name

    return(result)



result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\MM\result.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))





#%% DEMO
df_demo = pd.concat([df.query('Geo=="Asia"').sample(63),
    df.query('Geo=="Americas"'),
    df.query('Geo=="EMEA"').sample(63)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\MM\demo_MM.png")



# -*- coding: utf-8 -*-
"""
Created on Tue Sep  1 14:12:27 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
#dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered3.pkl')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCARS 2008-2019_matches3.pkl')


#%% Drop defaulters
mask = dat['Applied Rating'].isin([83, 90, 101, 102])
dat = dat[~mask].reset_index(drop=True)

# Drop rating 2-3
mask = dat['Applied Rating']==23
dat = dat[~mask].reset_index(drop=True)



#%% Drop Borrowers who have unique "CIF" but multiple(>=3) "Names"
tmp = dat.groupby('Borrower_CIF_cleaned')['Borrower Name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts(normalize=True)
'''
1    0.657148
2    0.255707
3    0.063053
4    0.017100
5    0.005476
6    0.001011
7    0.000421
9    0.000042
8    0.000042
Name: Num_CusName, dtype: float64

'''

list_toremove = tmp[tmp.Num_CusName>=3].index.tolist()
mask = dat.Borrower_CIF_cleaned.isin(list_toremove)
dat = dat[~mask].reset_index(drop=True)
# 102200



#%% tag Geo info
dat['Geo'] = 'Asia'
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(gcar_Americas_office_code), 'Americas')
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(gcar_EMEA_office_code), 'EMEA')

dat['Geo'].value_counts()
'''
Asia        57698
Americas    23118
EMEA        21384
Name: Geo, dtype: int64

'''



#%% Cast integer rating to string
dat['BTMU Rating'] = dat['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat['BTMU Rating'].value_counts().sort_index() 
'''
1-0      564
2-0     7238
2-1      135
2-2      338
3-0    19273
3-1      776
3-2     1440
3-3     1442
4-0    24528
5-1    10509
5-2     5434
6-1     4782
6-2     5764
7-0     9947
8-1     5499
8-2     4531
Name: BTMU Rating, dtype: int64
'''

#dat.query('Geo=="Americas"')['BTMU Rating'].value_counts().sort_index() 
#dat.query('Geo=="EMEA"')['BTMU Rating'].value_counts().sort_index() 
#dat.query('Geo=="Asia"')['BTMU Rating'].value_counts().sort_index() 



#%% Map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['Applied Rating PDRR'] = dat['Applied Rating'].transform(lambda x: pd_mapping[x])



#%% convert to numeric value
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

gcar_converter(dat, curr, inplace=True)


#%% Choose the columns we need 
'''
cols=[
'Data From',
'Application No.',
'Borrower CIF',
'Borrower_CIF_cleaned',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'Secondary Evaluation',
'Third Evaluation',
'BTMU Rating',
'Final Result of Evaluation based on Financial Substance Score',
'Applied Rating',
'Applied Rating PDRR',
'Borrower Industry',
'Industry Model',
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

dat[cols].to_pickle(r'..\data\dat_2008-2019_cleaned_20200901.pkl')

'''
dat.to_pickle(r'..\data\dat_2008-2019_cleaned_20200903.pkl')
# -*- coding: utf-8 -*-
"""
Created on Fri Jul 31 12:26:29 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner


df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_20200903.pkl')


#%%Check Browers who have unique "CIF" but multiple(>=3) "Names"
df_bvd['CIF_cleaned'] = gcar_cif_cleaner(df_bvd, col='CIF')
tmp = df_bvd.groupby('CIF_cleaned')['Company name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts(normalize=True)
'''
1    0.959053
2    0.031076
3    0.005393
4    0.002194
5    0.000914
6    0.000731
7    0.000366
8    0.000274
Name: Num_CusName, dtype: float64
'''




bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


df_bvd['Fiscal year'].value_counts().sort_index()
'''
Out[20]: 
1984       1
1985       1
1986       1
1987       1
1988       1
1989       2
1990       4
1991       5
1992       7
1993      12
1994      22
1995      29
1996      32
1997      35
1998      47
1999      82
2000     132
2001     209
2002     336
2003     433
2004     471
2005     617
2006    1078
2007    1692
2008    2962
2009    6279
2010    6813
2011    7214
2012    7572
2013    8556
2014    9367
2015    8940
2016    9075
2017    8597
2018    6637
2019     250
2020       3
Name: Fiscal year, dtype: int64

'''

dat['year-1'] = dat['year']-1

len(set(dat['Borrower_CIF_cleaned'].unique()))
# 21673

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(dat['Borrower_CIF_cleaned'].unique())&set(df_bvd['CIF'].unique()))
# 9362




#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% relink: Year (N-1)'s BvD links to Year N's GCARS data
df_merged = pd.merge(dat, df_bvd, left_on=['Borrower_CIF_cleaned','year-1'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_merged['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_merged['BvD_CIF']]
df_merged['inBvD'].value_counts()

'''
No     60712
Yes    42334
Name: inBvD, dtype: int64
'''




#%% Drop Browers who have unique "BvD_CIF" but multiple(>=3) "Names"
tmp = df_merged.groupby('BvD_CIF')['BvD_Company name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts().sort_index()
'''
0    2495
1    5646
2     113
3       7
Name: Num_CusName, dtype: int64

'''

list_toremove = tmp[tmp.Num_CusName>=3].index.tolist()
mask = df_merged.BvD_CIF.isin(list_toremove)
df_merged = df_merged[~mask].reset_index(drop=True)
# 102899



df_merged.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_BvD_relink.pkl')


df_merged_3yr = df_merged.query('2017<=year<=2019').reset_index(drop=True)
'''
Out[58]: 
No     18063
Yes    13819
Name: inBvD, dtype: int64
'''

df_merged_3yr.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2017-2019_cleaned_BvD_relink.pkl')


# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np
import pickle
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
lmdb = 0.3
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']

#dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2016-2018_cleaned_BvD_relink.pkl')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2017-2019_cleaned_BvD_relink.pkl')

cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']



#%% build CnI ratios
dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
# for invalid_neg
dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']


# MM new transformation
dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb



#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios
dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']



factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))



#%% build current tokyo model factors:
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']
# fill inf
for factor in curr:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))


# rename current factor

for name in curr:
    dat['Other_'+name] = np.nan
    dat['SP_'+name] = np.nan

    dat['Other_'+name] = dat['Other_'+name].mask(dat['Borrower Type']=="Corporation (Other Company)", dat[name])
    dat['SP_'+name] = dat['SP_'+name].mask(dat['Borrower Type']!="Corporation (Other Company)", dat[name])

dat.drop(columns=curr, inplace=True)


#%% label geo info
dat = pd.concat([dat, pd.get_dummies(dat['Geo'], prefix='Geo')], axis=1)

#%% label tokyo model info
dat = pd.concat([dat, pd.get_dummies(dat['Borrower Type'], prefix='Tokyo')], axis=1)




#%% add portfolio info
dat['portfolio'] = 'unknown'
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']>1e9, 'LC')
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']<=1e9, 'MM')

dat['portfolio'].value_counts()
'''
unknown    20275
MM         10190
LC          1417
Name: portfolio, dtype: int64
'''



#%% split
dat_LC = dat.query('portfolio=="LC"').reset_index(drop=True)
dat_MM = dat.query('portfolio=="MM"').reset_index(drop=True)



dat_LC.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat_MM.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression
from scipy.optimize import differential_evolution

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat_def = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink_pseudodef.pkl')
dat = pd.concat([dat, dat_def], axis=0)
dat.dropna(subset=['Primary Evaluation'], inplace=True)


dat['Applied Rating'].value_counts().sort_index()
'''
10      32
20     266
21       2
22       7
30     513
31       7
32      25
33      23
40     170
51     124
52      56
61      33
62      20
70      36
81       4
82      16
83       1
90       5
102      2
Name: Applied Rating, dtype: int64
'''

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(dat['Applied Rating']>=81, 1)

assert dat.def_flag.sum() == (dat['Applied Rating']>=81).sum()

print(dat.def_flag.sum())
# 28


#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    979
1    363
Name: Gua_override, dtype: int64
'''

# Use Secondary as Final for all Guarantor impacted obs
mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,102:82})
dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


# show the discrpency in different rating
SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7033062175919319
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7058687864416892





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd





df_EMEA = dat_LC_norm.query('Geo=="EMEA"').reset_index(drop=True)
df_Asia = dat_LC_norm.query('Geo=="Asia"').reset_index(drop=True)
df_Americas = dat_LC_norm.query('Geo=="Americas"').reset_index(drop=True)
# remove 2-0 and 3-0 record in Americas portfolio
mask = df_Americas['Applied Rating'].isin([20,30])
df_Americas = df_Americas[~mask].reset_index(drop=True)





dict_NonAM_BTMU_to_ylabel={
10:1, 20:2, 30:3, 40:4, 51:5, 52:6, 61:7, 62:8, 70:9, 81:10, 82:11, 83:12, 90:13}
dict_NonAM_ylabel_to_BTMU={
1:10, 2:20, 3:30, 4:40, 5:51, 6:52, 7:61, 8:62, 9:70, 10:81, 11:82, 12:83, 13:90}

df_Asia['y_label'] = df_Asia['Applied Rating'].replace(dict_NonAM_BTMU_to_ylabel)
df_EMEA['y_label'] = df_EMEA['Applied Rating'].replace(dict_NonAM_BTMU_to_ylabel)


dict_AM_BTMU_to_ylabel={
10:1, 21:2, 22:3, 31:4, 32:5, 33:6, 40:7, 51:8, 
52:9, 61:10, 62:11, 70:12, 81:13, 82:14}
dict_AM_ylabel_to_BTMU={
1:10, 2:21, 3:22, 4:31, 5:32, 6:33, 7:40, 8:51, 
9:52, 10:61, 11:62, 12:70, 13:81, 14:82}

df_Americas['y_label'] = df_Americas['Applied Rating'].replace(dict_AM_BTMU_to_ylabel)


#%% define func

def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))


def valuefunc_matchrate(calib):

    df['y_est'] = calib[0] + calib[1]*df['LC_quant_score']
    df['y_est'] = round(df['y_est'])
    return(-(df['y_label']==df['y_est']).sum()/N)


def apply_calib(df, calib, col='y_est'):

    df[col] = calib[0] + calib[1]*df['LC_quant_score']
    df[col] = round(df[col])
    return(df)



#%% Asia
df = df_Asia
N= len(df)
df['y_label'].value_counts().sort_index()


# naive linear regression
x_train = sm.add_constant(df['LC_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             3.249108
LC_quant_score    0.010536
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
#Out[108]: 0.2882882882882883

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
#Out[109]: 0.9009009009009009


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-10,4), (0,0.1)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
#Out[113]: -0.5945945945945946

res1.x
#Out[114]: array([2.59850132, 0.00619815])

df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
#Out[115]: 0.5945945945945946

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
#Out[116]: 0.8918918918918919

fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo2312.png")

# Convert to BTMU for future overall SomersD Calculation
df_Asia['BTMU_est_lr'] = df_Asia['y_est_lr'].replace(dict_NonAM_ylabel_to_BTMU)
df_Asia['BTMU_est_maxmatch'] = df_Asia['y_est_maxmatch'].replace(dict_NonAM_ylabel_to_BTMU)

# check
match_rate(df_Asia, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_Asia, col_a='Applied Rating', col_b='BTMU_est_maxmatch')












#%% EMEA
df = df_EMEA
N= len(df)
df['y_label'].value_counts().sort_index()


# naive linear regression
x_train = sm.add_constant(df['LC_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             4.930388
LC_quant_score    0.028169
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[121]: 0.2762096774193548

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[122]: 0.8991935483870968


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-5,10), (0,0.5)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[123]: -0.4637096774193548

res1.x
# Out[124]: array([3.10407701, 0.00522776])

df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
# Out[125]: 0.4637096774193548

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[126]: 0.8225806451612904

fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo2312.png")

# Convert to BTMU for future overall SomersD Calculation
df_EMEA['BTMU_est_lr'] = df_EMEA['y_est_lr'].replace(dict_NonAM_ylabel_to_BTMU)
df_EMEA['BTMU_est_maxmatch'] = df_EMEA['y_est_maxmatch'].replace(dict_NonAM_ylabel_to_BTMU)

# check
match_rate(df_EMEA, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_EMEA, col_a='Applied Rating', col_b='BTMU_est_maxmatch')















#%% Americas
df = df_Americas
N= len(df)
df['y_label'].value_counts().sort_index()


# naive linear regression
x_train = sm.add_constant(df['LC_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             7.470425
LC_quant_score    0.032013
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[132]: 0.2542372881355932

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[133]: 0.8135593220338984


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,15,15))#
ax.set_yticklabels(['1-0','2-1', '2-2','3-1','3-2','3-3','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-20,20), (0,1)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[134]: -0.3050847457627119

res1.x
# Out[135]: array([6.03133924, 0.02779509])

df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
#Out[136]: 0.3050847457627119

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[137]: 0.7966101694915254


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,15,15))#
ax.set_yticklabels(['1-0','2-1', '2-2','3-1','3-2','3-3','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-100, 100]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo2312.png")

# Convert to BTMU for future overall SomersD Calculation
df_Americas['BTMU_est_lr'] = df_Americas['y_est_lr'].replace(dict_AM_ylabel_to_BTMU)
df_Americas['BTMU_est_maxmatch'] = df_Americas['y_est_maxmatch'].replace(dict_AM_ylabel_to_BTMU)

# check
match_rate(df_Americas, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_Americas, col_a='Applied Rating', col_b='BTMU_est_maxmatch')







df = pd.concat([df_Americas, df_Asia, df_EMEA], axis=0)

#%% Geo 
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2017')
df5 = df.query('year==2018')
df6 = df.query('year==2019')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']





list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2017','2018','2019','Major','Other']#+list_dataset_name

def calc_fun_fitted(list_dataset, list_dataset_name):

    list_obs_newmodel = []

    list_sd_lr = []
    list_mr_lr = []
    list_w2r_lr = []

    list_sd_maxmatch = []
    list_mr_maxmatch = []
    list_w2r_maxmatch = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))

        list_sd_lr.append(SomersD(dat['Applied Rating'], dat['BTMU_est_lr']))
        list_sd_maxmatch.append(SomersD(dat['Applied Rating'], dat['BTMU_est_maxmatch']))

        list_mr_lr.append(match_rate(dat, 'y_label', 'y_est_lr'))
        list_mr_maxmatch.append(match_rate(dat, 'y_label', 'y_est_maxmatch'))

        list_w2r_lr.append(within_k_rate(dat, 'y_label', 'y_est_lr', 2))
        list_w2r_maxmatch.append(within_k_rate(dat, 'y_label', 'y_est_maxmatch', 2))


        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['LCModel_size'] = list_obs_newmodel

    result['LR_SD'] = list_sd_lr
    result['LR_MatchRate'] = list_mr_lr
    result['LR_Within2Rate'] = list_w2r_lr

    result['MaxMatch_SD'] = list_sd_maxmatch
    result['MaxMatch_MatchRate'] = list_mr_maxmatch
    result['MaxMatch_Within2Rate'] = list_w2r_maxmatch

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)







result = calc_fun_fitted(list_dataset, list_dataset_name)
result.to_excel(r'SFAMFA\LC\af_Geo_calib.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='Applied Rating PDRR', data= dat_LC_norm, hue="Geo", alpha=0.9, palette=google_color[:3])

ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by LC_quant_score')#
ax.set_ylabel('Mapped to PDRR')#




ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
x_ = [-120, 120]
y_ = [model_LC.intercept1 + model_LC.slope1*x for x in x_]
ax2.plot(x_, y_, 'k-')
#ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
#ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")






low_pd = MS['new_low'].to_list()









def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'
    elif y_type=='LC':
        y_pred = 'LC_PDRR'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['NewModel_MatchRate'] = list_mr_newmodel
    result['NewModel_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)





#%% pseudo def
df['def_flag'].sum()
# 17



SomersD(df['def_flag'], df['BTMU_est_lr'])
# Out[158]: 0.7042206713193752

SomersD(df['def_flag'], df['BTMU_est_maxmatch'])
# Out[159]: 0.27558989697573943

SomersD(df['def_flag'], df['PrimaryEvaluationPDRR'])
# 0.7133899843247257

SomersD(df['def_flag'], df['Primary Evaluation'])
# 0.6977972114512004

# trivial 
SomersD(df['def_flag'], df['Applied Rating'])
# .9062783598712978


# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat_def = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink_pseudodef.pkl')
dat = pd.concat([dat, dat_def], axis=0)
dat.dropna(subset=['Primary Evaluation'], inplace=True)


dat['Applied Rating'].value_counts().sort_index()
'''
10      32
20     266
21       2
22       7
30     513
31       7
32      25
33      23
40     170
51     124
52      56
61      33
62      20
70      36
81       4
82      16
83       1
90       5
102      2
Name: Applied Rating, dtype: int64
'''

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(dat['Applied Rating']>=81, 1)

assert dat.def_flag.sum() == (dat['Applied Rating']>=81).sum()

print(dat.def_flag.sum())
# 28


#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    979
1    363
Name: Gua_override, dtype: int64
'''

# Use Secondary as Final for all Guarantor impacted obs
mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,102:82})
dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


# show the discrpency in different rating
SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7033062175919319
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7058687864416892





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['LC_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['LC_PD'] = list_PD
    dat['LC_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_LC_norm = get_proxy_rating(dat_LC_norm, model_LC)






def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))



df = dat_LC_norm.copy()

#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2017')
df5 = df.query('year==2018')
df6 = df.query('year==2019')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
#list_dataset_name = list(df['Industry Model'].value_counts().index)
#list_dataset=[]
#for ind in list_dataset_name:
#    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2017','2018','2019','Major','Other']#+list_dataset_name



def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'
    elif y_type=='LC':
        y_pred = 'LC_PDRR'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['LC Model_size'] = list_obs_newmodel
    result['LC Model_SD'] = list_sd_newmodel
    result['LC Model_MatchRate'] = list_mr_newmodel
    result['LC Model_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name, y_type='LC')


result.to_excel(r'SFAMFA\LC\result_bf_calib.xlsx')




#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by CnI LC Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='LC_quant_score', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\LC_score_vs_BTMURating.png")








#%% pseudo def
df['def_flag'].sum()
# 17
df['def_flag'].sum()/len(df)
# 0.023448275862068966

SomersD(df['def_flag'], df['LC_PDRR'])
# 0.7066248659351538

SomersD(df['def_flag'], df['PrimaryEvaluationPDRR'])
# 0.7133899843247257

SomersD(df['def_flag'], df['Primary Evaluation'])
# 0.6977972114512004

# trivial 
SomersD(df['def_flag'], df['Applied Rating'])
# .9062783598712978



df.query('def_flag==1')['PrimaryEvaluationPDRR'].value_counts().sort_index()
'''
6     3
8     3
9     2
10    2
11    1
12    2
13    3
14    1
Name: PrimaryEvaluationPDRR, dtype: int64
'''


df.query('def_flag==1')['LC_PDRR'].value_counts().sort_index()
'''
4     1
7     1
8     1
10    2
11    6
12    5
13    1
Name: LC_PDRR, dtype: int64
'''
# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression
from scipy.optimize import differential_evolution

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')
dat_def = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink_pseudodef.pkl')
dat = pd.concat([dat, dat_def], axis=0)
dat.dropna(subset=['Primary Evaluation'], inplace=True)

dat['Applied Rating'].value_counts().sort_index()
'''
10       16
20      940
22        4
30     2744
33        3
40     3124
51      891
52      418
61      261
62      199
70      738
81      309
82      325
83        8
90        7
101       3
102       1
Name: Applied Rating, dtype: int64
'''

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(dat['Applied Rating']>=81, 1)

assert dat.def_flag.sum() == (dat['Applied Rating']>=81).sum()

print(dat.def_flag.sum())
# 653




#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    5775
1    4216
Name: Gua_override, dtype: int64
'''

# Use Secondary as Final for all Guarantor impacted obs
mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,101: 82, 102:82})
dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


# show the discrpency in different rating
SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
#Out[22]: 0.8844416719524808

SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
#Out[23]: 0.8843993317777482






#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

df_EMEA = dat_MM_norm.query('Geo=="EMEA"').reset_index(drop=True)
df_Asia = dat_MM_norm.query('Geo=="Asia"').reset_index(drop=True)
df_Americas = dat_MM_norm.query('Geo=="Americas"').reset_index(drop=True)
# remove 2-0 and 3-0 record in Americas portfolio
mask = df_Americas['Applied Rating'].isin([20,30])
df_Americas = df_Americas[~mask].reset_index(drop=True)





dict_NonAM_BTMU_to_ylabel={
10:1, 20:2, 30:3, 40:4, 51:5, 52:6, 61:7, 62:8, 70:9, 81:10, 82:11, 83:12, 90:13, 101:14, 102:15}
dict_NonAM_ylabel_to_BTMU={
1:10, 2:20, 3:30, 4:40, 5:51, 6:52, 7:61, 8:62, 9:70, 10:81, 11:82, 12:83, 13:90, 14:101, 15:102}

df_Asia['y_label'] = df_Asia['Applied Rating'].replace(dict_NonAM_BTMU_to_ylabel)
df_EMEA['y_label'] = df_EMEA['Applied Rating'].replace(dict_NonAM_BTMU_to_ylabel)


dict_AM_BTMU_to_ylabel={
10:1, 21:2, 22:3, 31:4, 32:5, 33:6, 40:7, 51:8, 
52:9, 61:10, 62:11, 70:12, 81:13, 82:14, 83:15}
dict_AM_ylabel_to_BTMU={
1:10, 2:21, 3:22, 4:31, 5:32, 6:33, 7:40, 8:51, 
9:52, 10:61, 11:62, 12:70, 13:81, 14:82, 15:83}

df_Americas['y_label'] = df_Americas['Applied Rating'].replace(dict_AM_BTMU_to_ylabel)


#%% define func

def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))


def valuefunc_matchrate(calib):

    df['y_est'] = calib[0] + calib[1]*df['MM_quant_score']
    df['y_est'] = round(df['y_est']).clip(boundary[0],boundary[1])
    return(-(df['y_label']==df['y_est']).sum()/N)


def apply_calib(df, calib, col='y_est'):

    df[col] = calib[0] + calib[1]*df['MM_quant_score']
    df[col] = round(df[col]).clip(boundary[0],boundary[1])
    return(df)



#%% Asia
df = df_Asia
N= len(df)
df['y_label'].value_counts().sort_index()

boundary=(1,15)

# naive linear regression
x_train = sm.add_constant(df['MM_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             5.199996
MM_quant_score    0.025809
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[51]: 0.23596442468037798

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[52]: 0.7926625903279599


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 120]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-10,4), (0,0.1)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[53]: -0.3665925514174541

res1.x
# Out[54]: array([3.77446797e+00, 2.02423136e-03])


df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
# Out[55]: 0.3665925514174541

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[56]: 0.8107281823235131


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 120]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobaMMorporation\src\SFAMFA\MM\demo2312.png")

# Convert to BTMU for future overall SomersD CaMMulation
df_Asia['BTMU_est_lr'] = df_Asia['y_est_lr'].replace(dict_NonAM_ylabel_to_BTMU)
df_Asia['BTMU_est_maxmatch'] = df_Asia['y_est_maxmatch'].replace(dict_NonAM_ylabel_to_BTMU)

# check
match_rate(df_Asia, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_Asia, col_a='Applied Rating', col_b='BTMU_est_maxmatch')












#%% EMEA
df = df_EMEA
N= len(df)
df['y_label'].value_counts().sort_index()

boundary=(1,13)

# naive linear regression
x_train = sm.add_constant(df['MM_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             6.13475
MM_quant_score    0.02804
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[61]: 0.20439739413680783

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[62]: 0.8086319218241043



fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 120]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-1,10), (0,0.2)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[63]: -0.38925081433224756

res1.x
# Out[64]: array([4.43187050e+00, 2.15450184e-03])


df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
# Out[65]: 0.38925081433224756

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[66]: 0.6701954397394136



fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,12,12))#
ax.set_yticklabels(['1-0','2-0','3-0','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 120]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobaMMorporation\src\SFAMFA\MM\demo2312.png")

# Convert to BTMU for future overall SomersD CaMMulation
df_EMEA['BTMU_est_lr'] = df_EMEA['y_est_lr'].replace(dict_NonAM_ylabel_to_BTMU)
df_EMEA['BTMU_est_maxmatch'] = df_EMEA['y_est_maxmatch'].replace(dict_NonAM_ylabel_to_BTMU)

# check
match_rate(df_EMEA, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_EMEA, col_a='Applied Rating', col_b='BTMU_est_maxmatch')















#%% Americas
df = df_Americas
N= len(df)
df['y_label'].value_counts().sort_index()
'''
3      1
6      2
7     37
8     19
9      4
10     9
11     3
12     7
13     4
14     7
15     1
Name: y_label, dtype: int64
'''
boundary=(2,15)


# naive linear regression
x_train = sm.add_constant(df['MM_quant_score'], prepend = True)
linear = sm.OLS(df['y_label'], x_train, missing='drop')
result = linear.fit(disp=0)
print(result.params)
'''
const             8.712767
MM_quant_score    0.015609
dtype: float64
'''

df = apply_calib(df, result.params, 'y_est_lr')
match_rate(df, col_a='y_label', col_b='y_est_lr')
# Out[74]: 0.11702127659574468

within_k_rate(df, col_a='y_label', col_b='y_est_lr', k=2)
# Out[75]: 0.7978723404255319


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,15,15))#
ax.set_yticklabels(['1-0','2-1', '2-2','3-1','3-2','3-3','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 90]
y_ = [result.params[0] + result.params[1]*x for x in x_]
ax.plot(x_, y_, 'k-')


# opt match rate
bounds = [(-20,20), (0,1)]
res1 = differential_evolution(valuefunc_matchrate, bounds)
res1.fun
# Out[76]: -0.44680851063829785

res1.x
# Out[77]: array([7.11220989, 0.00797209])


df = apply_calib(df, res1.x, 'y_est_maxmatch')
match_rate(df, col_a='y_label', col_b='y_est_maxmatch')
# Out[78]: 0.44680851063829785

within_k_rate(df, col_a='y_label', col_b='y_est_maxmatch', k=2)
# Out[79]: 0.6808510638297872


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='y_label', data= df, alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(1,15,15))#
ax.set_yticklabels(['1-0','2-1', '2-2','3-1','3-2','3-3','4-0', '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2',''])
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('BTMU Rating')#
x_ = [-35, 90]
y_ = [res1.x[0] + res1.x[1]*x for x in x_]
ax.plot(x_, y_, 'k-')

#plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobaMMorporation\src\SFAMFA\MM\demo2312.png")

# Convert to BTMU for future overall SomersD CaMMulation
df_Americas['BTMU_est_lr'] = df_Americas['y_est_lr'].replace(dict_AM_ylabel_to_BTMU)
df_Americas['BTMU_est_maxmatch'] = df_Americas['y_est_maxmatch'].replace(dict_AM_ylabel_to_BTMU)

# check
match_rate(df_Americas, col_a='Applied Rating', col_b='BTMU_est_lr')

match_rate(df_Americas, col_a='Applied Rating', col_b='BTMU_est_maxmatch')







#%%
df = pd.concat([df_Americas, df_Asia, df_EMEA], axis=0)

#%% Geo 
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2017')
df5 = df.query('year==2018')
df6 = df.query('year==2019')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']



list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2017','2018','2019','Major','Other']#+list_dataset_name

#%%
def calc_fun_fitted(list_dataset, list_dataset_name):

    list_obs_newmodel = []

    list_sd_lr = []
    list_mr_lr = []
    list_w2r_lr = []

    list_sd_maxmatch = []
    list_mr_maxmatch = []
    list_w2r_maxmatch = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))

        list_sd_lr.append(SomersD(dat['Applied Rating'], dat['BTMU_est_lr']))
        list_sd_maxmatch.append(SomersD(dat['Applied Rating'], dat['BTMU_est_maxmatch']))

        list_mr_lr.append(match_rate(dat, 'y_label', 'y_est_lr'))
        list_mr_maxmatch.append(match_rate(dat, 'y_label', 'y_est_maxmatch'))

        list_w2r_lr.append(within_k_rate(dat, 'y_label', 'y_est_lr', 2))
        list_w2r_maxmatch.append(within_k_rate(dat, 'y_label', 'y_est_maxmatch', 2))


        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['MMModel_size'] = list_obs_newmodel

    result['LR_SD'] = list_sd_lr
    result['LR_MatchRate'] = list_mr_lr
    result['LR_Within2Rate'] = list_w2r_lr

    result['MaxMatch_SD'] = list_sd_maxmatch
    result['MaxMatch_MatchRate'] = list_mr_maxmatch
    result['MaxMatch_Within2Rate'] = list_w2r_maxmatch

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


#%%




result = calc_fun_fitted(list_dataset, list_dataset_name)
result.to_excel(r'SFAMFA\MM\af_Geo_calib.xlsx')





CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='LC_quant_score', y='Applied Rating PDRR', data= dat_MM_norm, hue="Geo", alpha=0.9, palette=google_color[:3])

ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by MM_quant_score')#
ax.set_ylabel('Mapped to PDRR')#




ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
x_ = [-120, 120]
y_ = [model_MM.intercept1 + model_MM.slope1*x for x in x_]
ax2.plot(x_, y_, 'k-')
#ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
#ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\MM\demo.png")





#%% pseudo def
df['def_flag'].sum()
# 324



SomersD(df['def_flag'], df['BTMU_est_lr'])
# Out[83]: 0.4541925882947061

SomersD(df['def_flag'], df['BTMU_est_maxmatch'])
# Out[84]: 0.09731691003449054

SomersD(df['def_flag'], df['PrimaryEvaluationPDRR'])
# Out[85]: 0.7418064822873353

SomersD(df['def_flag'], df['Primary Evaluation'])
# Out[86]: 0.741759474153585

SomersD(df['def_flag'], df['Applied Rating'])
# Out[87]: 0.8308499607817855
# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')
dat_def = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink_pseudodef.pkl')
dat = pd.concat([dat, dat_def], axis=0)
dat.dropna(subset=['Primary Evaluation'], inplace=True)

dat['Applied Rating'].value_counts().sort_index()
'''
10       16
20      940
22        4
30     2744
33        3
40     3124
51      891
52      418
61      261
62      199
70      738
81      309
82      325
83        8
90        7
101       3
102       1
Name: Applied Rating, dtype: int64
'''

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(dat['Applied Rating']>=81, 1)

assert dat.def_flag.sum() == (dat['Applied Rating']>=81).sum()

print(dat.def_flag.sum())
# 653




#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    5775
1    4216
Name: Gua_override, dtype: int64
'''

# Use Secondary as Final for all Guarantor impacted obs
mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,101: 82, 102:82})
dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


# show the discrpency in different rating
SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
#Out[22]: 0.8844416719524808

SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
#Out[23]: 0.8843993317777482






#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['MM_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['MM_PD'] = list_PD
    dat['MM_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_MM_norm = get_proxy_rating(dat_MM_norm, model_MM)






def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))





df = dat_MM_norm.copy()

#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2017')
df5 = df.query('year==2018')
df6 = df.query('year==2019')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
#list_dataset_name = list(df['Industry Model'].value_counts().index)
#list_dataset=[]
#for ind in list_dataset_name:
#    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2017','2018','2019','Major','Other']#+list_dataset_name



def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'
    elif y_type=='MM':
        y_pred = 'MM_PDRR'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['MM Model_size'] = list_obs_newmodel
    result['MM Model_SD'] = list_sd_newmodel
    result['MM Model_MatchRate'] = list_mr_newmodel
    result['MM Model_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name, y_type='MM')


result.to_excel(r'SFAMFA\MM\result_bf_calib.xlsx')







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='MM_quant_score', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by CnI MM Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='MM_quant_score', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\MM\MM_score_vs_BTMURating.png")







#%% pseudo def
df['def_flag'].sum()
#324
df['def_flag'].sum()/len(df)
# 0.06585365853658537



SomersD(df['def_flag'], df['MM_PDRR'])
# Out[28]: 0.44769532110042476

SomersD(df['def_flag'], df['PrimaryEvaluationPDRR'])
# Out[29]: 0.7419093916302848

SomersD(df['def_flag'], df['Primary Evaluation'])
# Out[30]: 0.7418624039437437

SomersD(df['def_flag'], df['Applied Rating'])
# Out[31]: 0.8309168237397903



df.query('def_flag==1')['PrimaryEvaluationPDRR'].value_counts().sort_index()
'''
3       3
6       5
8      36
9      13
10     12
11     14
12     13
13     23
14    205
Name: PrimaryEvaluationPDRR, dtype: int64

'''


df.query('def_flag==1')['MM_PDRR'].value_counts().sort_index()
'''
2     13
3      6
4     14
5     16
6     24
7     19
8     12
9     15
10    27
11     6
12    15
13    27
14    41
15    89
Name: MM_PDRR, dtype: int64
'''
# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')


#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1250
1     355
Name: Gua_override, dtype: int64
'''

dat = dat.query('Gua_override==0').reset_index(drop=True)


#%%
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])

dat = dat.iloc[df_temp.query('diff<=4').index,]
dat.reset_index(drop=True, inplace=True)
# 1000


SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7331962963964695
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7380434194747963



from minepy import MINE
mine = MINE(alpha=0.6, c=15) 



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
list_mic=[]

for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        mine.compute_score(df_tmp_dropna[factor],df_tmp_dropna['Applied Rating PDRR']) 
        list_mic.append(mine.mic() )
    else:
        list_sd.append(np.nan)
        list_mic.append(np.nan)

result['SomersD'] = list_sd
result['MIC'] = list_mic
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)

#result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.23
list_short1 = result.query('SomersD>=0.27 and DataQuality>=0.7').index.tolist()
list_short2 = result.query('MIC>=0.2 and DataQuality>=0.7').index.tolist()

list_short = list(set(list_short1)|set(list_short2))

# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'BvD_P/L before tax',
'BvD_Current liabilities', 
'BvD_Shareholders funds',
'bs@TL_to_TA',
'MM_NOP_to_NS', 
'BvD_Debtors',
'LC_TD_to_Capt',
'new@CashFlowRatio',
'LC_Total Assets'
]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')



for i, factor in enumerate(list_final): 
    tmp = dat_norm[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')




for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,3,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
'BvD_P/L before tax',
'BvD_Current liabilities', 
'BvD_Shareholders funds',
0,0,
'BvD_Debtors',
0,0,0]



dat['BvD_P/L before tax'] = dat['BvD_P/L before tax'] + np.abs(dat['BvD_P/L before tax'].min()) +1 





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'bs@TL_to_TA',
'MM_NOP_to_NS', 
'LC_TD_to_Capt',
'new@CashFlowRatio',
]


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 0, 0, 0, 0, 0]


# add 'LC_TD_to_Capt', 
list_neg = [
0, 0, 0, 0, 0, 0,'BvD_Proxy_Capitalization_inUSD',0, 0]



#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\LC\LC_MFA_modelselection.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['LC_quantscore'] = dat_LC_norm['LC_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\LC\LC_MFA_modelselection_keepLC.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[2:], factor_num):
        model = ['LC_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['BvD_Current liabilities', 'bs@TL_to_TA', 'MM_NOP_to_NS', 'BvD_Debtors', 'LC_TD_to_Capt']
wts = [0.22407759053777218, 0.1681209924906741, 0.2746752780869888, 0.1042296050840112, 0.22889653380055378]


df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# .5097965428490594
bounds1 = [(0.05,0.99), (0.05,0.99), (0.05,0.99), (0.05,0.99),(0.05,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun
#  -0.5123785561052253

optSD_wts = res1.x / res1.x.sum()
# array([0.26176079, 0.13793903, 0.27465621, 0.08383611, 0.24180785])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5121917721675452





#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5570528790821218
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.538132764924174

score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5570528790821218 matched!



def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(10,50), (10,50), (10,50)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5579153813825857

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5579153813825857



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= aa, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()

df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# .5579153813825857  matched
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# .538844191980926



y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# .5677792213856622

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5522816482034407

df['iso'] = y_
df['iso_round'] = y_round




def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))



match_rate(df, 'Applied Rating PDRR', 'fittedvalues_round')
# 0.23340040241448692
within_k_rate(df, 'Applied Rating PDRR', 'fittedvalues_round',2)
# 0.8350100603621731


match_rate(df, 'Applied Rating PDRR', 'iso_round')
# 0.30885311871227367
within_k_rate(df, 'Applied Rating PDRR', 'iso_round',2)
# 0.8309859154929577





#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
#list_dataset_name = list(df['Industry Model'].value_counts().index)
#list_dataset=[]
#for ind in list_dataset_name:
#    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']#+list_dataset_name



def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['NewModel_MatchRate'] = list_mr_newmodel
    result['NewModel_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\LC\result_noGua.xlsx')


CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\LC\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")



# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1483
1     362
Name: Gua_override, dtype: int64
'''
dat = dat.query('Gua_override==0')
dat.reset_index(drop=True, inplace=True)


#%%
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])

dat = dat.iloc[df_temp.query('diff<=4').index,]
dat.reset_index(drop=True, inplace=True)

SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.7331962963964695
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.7380434194747963



from minepy import MINE
mine = MINE(alpha=0.6, c=15) 



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
list_mic=[]

for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        mine.compute_score(df_tmp_dropna[factor],df_tmp_dropna['Applied Rating PDRR']) 
        list_mic.append(mine.mic() )
    else:
        #list_sd.append(np.nan)
        list_mic.append(np.nan)

#result['SomersD'] = list_sd
result['MIC'] = list_mic
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('MIC', ascending=False, inplace=True)

#result.to_excel(r'SFAMFA\LC\SFA.xlsx')


#%% pick SD > 0.23
list_short = result.query('MIC>=0.18 and DataQuality>=0.7').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'BvD_Current assets', 
'LC_Total Assets',
'BvD_Tangible fixed assets', 
'BvD_Debtors',
'BvD_Net current assets',
'MM_NOP_to_NS', 
'cf@TD_NOP',
'BvD_Intangible fixed assets']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


list_transform = [
'BvD_P/L for period [=Net income] ', 
'BvD_Taxation',
'BvD_Current assets', 
0,
'BvD_Tangible fixed assets', 
'BvD_Debtors',
'BvD_Net current assets',
0, 
0,
'BvD_Intangible fixed assets']


dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1 
dat['BvD_Net current assets'] = dat['BvD_Net current assets'] + np.abs(dat['BvD_Net current assets'].min()) +1 
dat['BvD_Intangible fixed assets'] = dat['BvD_Intangible fixed assets'] + np.abs(dat['BvD_Intangible fixed assets'].min()) +1 

for name in list_transform:
    if name:
        dat[name] = np.log(dat[name])
    else:
        continue



# replot after log-transformation

for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(2,5,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')





for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(2,5,p)
    sns.distplot(tmp[factor])
    plt.title(factor)






#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'MM_NOP_to_NS', 
'cf@TD_NOP',
]


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# [0, 'cf@TD_NOP']


#
list_neg = [
0,
0,
0,
0,
0,
0,
0,
0,
'BvD_Proxy_Net_Op_Profit_inUSD',
0]


#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
'BvD_Proxy_Net_Op_Profit_inUSD'
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation --alredy done in 
        # if list_transform[i]:
            #dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()



#%% 

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR

X, y = (dat_norm[list_final].fillna(dat_norm[list_final].median()), dat_norm['Applied Rating PDRR'])

estimator = LinearRegression()
selector = RFE(estimator, n_features_to_select=5, step=1)
selector = selector.fit(X, y)
selector.support_
# array([ True, False, False,  True, False,  True, False,  True,  True,   False])
selector.ranking_
# array([1, 6, 3, 1, 2, 1, 4, 1, 1, 5])


from sklearn.svm import SVR
estimator = SVR(kernel="linear")
selector = RFE(estimator, n_features_to_select=5, step=1)
selector = selector.fit(X, y)
selector.support_
# array([ True, False,  True,  True, False, False,  True, False,  True,       False])
# array([1, 5, 1, 1, 3, 2, 1, 4, 1, 6])






#%%




#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['BvD_P/L for period [=Net income] ',
 'LC_Total Assets',
  'BvD_Debtors',
   'MM_NOP_to_NS',
 'cf@TD_NOP',]

df = dat_norm.dropna(subset=model_setting, how='any')


bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun
#  -0.46343817386652547




# machine learning method
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

X, y = (dat_norm[list_final].fillna(dat_norm[list_final].median()), dat_norm['Applied Rating PDRR'])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0, test_size=0.2)

reg = GradientBoostingRegressor(random_state=0)
reg.fit(X_train, y_train)

SomersD(y_test, reg.predict(X_test))
# .5336114974501622



# 5 factors
model_setting = ['BvD_P/L for period [=Net income] ',
 'LC_Total Assets',
  'BvD_Debtors',
   'MM_NOP_to_NS',
 'cf@TD_NOP',]
X, y = (dat_norm[model_setting].fillna(dat_norm[model_setting].median()), dat_norm['Applied Rating PDRR'])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0, test_size=0.2)

reg = GradientBoostingRegressor(random_state=0)
reg.fit(X_train, y_train)

SomersD(y_test, reg.predict(X_test))
# 0.49986091794158555# -*- coding: utf-8 -*-
"""
Created on Wed Jul 29 11:49:20 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')

dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    6541
1    4289
Name: Gua_override, dtype: int64
'''
dat = dat.query('Gua_override==0')
dat.reset_index(drop=True, inplace=True)


#%%
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])

dat = dat.iloc[df_temp.query('diff<=4').index,]
dat.reset_index(drop=True, inplace=True)

SomersD(dat['Applied Rating'], dat['Primary Evaluation'])
# 0.8837113198764265
SomersD(dat['Applied Rating PDRR'], dat['PrimaryEvaluationPDRR'])
# 0.883094348980927



from minepy import MINE
mine = MINE(alpha=0.6, c=15) 



#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
list_mic=[]

for factor in list_factor:
    df_tmp_dropna = dat.dropna(subset=[factor])
    n_df_tmp = len(df_tmp_dropna)
    list_quality.append( n_df_tmp / len(dat))
    if n_df_tmp>5:        
        #list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating PDRR'], df_tmp_dropna[factor])))
        mine.compute_score(df_tmp_dropna[factor],df_tmp_dropna['Applied Rating PDRR']) 
        list_mic.append(mine.mic() )
    else:
        list_sd.append(np.nan)
        list_mic.append(np.nan)

result['SomersD'] = list_sd
result['MIC'] = list_mic
result['DataQuality'] = list_quality 
result.index = list_factor
result.sort_values('SomersD', ascending=False, inplace=True)

#result.to_excel(r'SFAMFA\MM\SFA.xlsx')


#%% pick SD > 0.27
list_short1 = result.query('SomersD>=0.27 and DataQuality>=0.7').index.tolist()
list_short2 = result.query('MIC>=0.2 and DataQuality>=0.7').index.tolist()

list_short = list(set(list_short1)|set(list_short2))

# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)


# after manual selection
list_final=[
'BvD_Shareholders funds', 
'BvD_P/L for period [=Net income] ',
'cf@TD_NOP',
'bs@TL_to_TA',
'prof@NOP_to_TA',
'new@GrossMarkup',
'BvD_Taxation',
'BvD_Gross profit',
'MM_NOP_to_NS', 
]


f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_final].corr(), linewidths=.3, cmap='Blues', ax=ax)



for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')



for i, factor in enumerate(list_final): 
    tmp = dat_norm[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(3,3,p)
    plt.plot(aa[factor],aa.index)
    #aa.reset_index(inplace=True)
    sns.scatterplot(x=factor, y="Applied Rating PDRR", data=tmp)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')




for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(3,3,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
'BvD_Shareholders funds', 
'BvD_P/L for period [=Net income] ',
0,0,0,
'new@GrossMarkup',
'BvD_Taxation',
'BvD_Gross profit',
0, 
]


dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1 
dat['BvD_Taxation'] = dat['BvD_Taxation'] + np.abs(dat['BvD_Taxation'].min()) +1 





#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'cf@TD_NOP',
'bs@TL_to_TA',
'prof@NOP_to_TA',
'MM_NOP_to_NS', 
]


for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

pl_invalid_neg
# ['cf@TD_NOP', 0, 0, 0]


# add 'LC_TD_to_Capt', 
list_neg = [ 0,0,'BvD_Proxy_Net_Op_Profit_inUSD',0,0,0,0,0,0 ] 


#%% MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'BvD_Proxy_Net_Op_Profit_inUSD',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
]].copy()


list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()





#%%  model selection
writer = pd.ExcelWriter(r'SFAMFA\MM\MM_MFA_modelselection.xlsx')
for factor_num in range(3,6):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd



# MFA
dat_norm = dat[list_final+[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'Geo',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'PrimaryEvaluationPDRR',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Secondary Evaluation',
'Third Evaluation',
'Final Result of Evaluation based on Financial Substance Score',
'Match',
'Borrower Industry',
'Industry Model',
'BvD_Company name',
'BvD_ID',
'BvD_office code',
'BvD_CIF',
'BvD_CIF(8 digit coverted)',
'BvD_ID(12digit)',
'BvD_model',
'BvD_borrower industry',
'BvD_industry ID',
'BvD_oil & gas flag',
'BvD_BvD ID number ',
'BvD_Consolidation code',
'BvD_Fiscal year',
'BvD_Accounting template',
'BvD_Filing type',
'BvD_Proxy_Interest_expense_inUSD',
'Geo_Americas',
'Geo_Asia',
'Geo_EMEA',
'BvD_Proxy_Net_Op_Profit_inUSD',
'Gua_override',
'Tokyo_Corporation (Major Company)',
'Tokyo_Corporation (Other Company)',
'BvD_Proxy_Capitalization_inUSD',
#'Cty_Rating_Label'
]].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()

dat_norm['MM_quantscore'] = dat_MM_norm['MM_quant_score']



writer = pd.ExcelWriter(r'SFAMFA\MM\MM_MFA_modelselection_keepMM.xlsx')
for factor_num in range(2,5):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final[2:], factor_num):
        model = ['MM_quantscore'] + list(setting) 
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==(1+factor_num)):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()





#%% Optimize SD
from scipy.optimize import differential_evolution

def _sd_wts(wts_all):

    df['score_tmp'] = (wts_all*df[model_setting]).sum(axis=1)
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


model_setting = ['BvD_Shareholders funds', 'BvD_P/L for period [=Net income] ', 'bs@TL_to_TA', 'prof@NOP_to_TA', 'MM_NOP_to_NS']
wts = [0.2272382523030523, 0.1249774355078123, 0.20580792395476946, 0.38968552457314865, 0.05229086366121709]


df = dat_norm.dropna(subset=model_setting, how='any')
df['score'] = (wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5638041871130282
bounds1 = [(0.01,0.99), (0.01,0.99), (0.01,0.99), (0.01,0.99),(0.01,0.99)]
res1 = differential_evolution(_sd_wts, bounds1)
res1.fun
#  -0.5760340685974339



optSD_wts = res1.x / res1.x.sum()
#  array([0.26935967, 0.19810441, 0.14292322, 0.18743158, 0.20218111])
df['score'] = (optSD_wts*df[model_setting]).sum(axis=1)
df.reset_index(drop=True, inplace=True)
SomersD(df['Applied Rating PDRR'], df['score'])
# 0.5760340685974339





#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Geo_Americas', 'Geo_Asia', 'Geo_EMEA']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()
'''
<class 'statsmodels.iolib.summary.Summary'>
"""
                             OLS Regression Results                            
===============================================================================
Dep. Variable:     Applied Rating PDRR   R-squared:                       0.417
Model:                             OLS   Adj. R-squared:                  0.413
Method:                  Least Squares   F-statistic:                     102.9
Date:                 Thu, 02 Jul 2020   Prob (F-statistic):           2.97e-50
Time:                         11:51:23   Log-Likelihood:                -935.65
No. Observations:                  435   AIC:                             1879.
Df Residuals:                      431   BIC:                             1896.
Df Model:                            3                                         
Covariance Type:             nonrobust                                         
================================================================================
                   coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------
const            5.3690      0.086     62.481      0.000       5.200       5.538
score            0.0653      0.004     16.862      0.000       0.058       0.073
Geo_Americas     2.1903      0.132     16.618      0.000       1.931       2.449
Geo_Asia         1.5247      0.186      8.197      0.000       1.159       1.890
Geo_EMEA         1.6541      0.145     11.396      0.000       1.369       1.939
==============================================================================
Omnibus:                       63.838   Durbin-Watson:                   1.755
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.592
Skew:                           0.918   Prob(JB):                     1.43e-22
Kurtosis:                       4.475   Cond. No.                     1.83e+17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 9.79e-30. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
'''
df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5719373370094449
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5470797128468539

score_Americas = result.params['Geo_Americas'] / result.params['score']
score_EMEA = result.params['Geo_EMEA'] / result.params['score']
score_Asia = result.params['Geo_Asia'] / result.params['score']


df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5613049604869567 matched!


def _sd_geo(wts_all):

    df['score_tmp'] = df['score'] + wts_all[0]*df['Geo_Americas'] + wts_all[1]*df['Geo_EMEA'] + wts_all[2]*df['Geo_Asia'] 
    return(-np.abs(SomersD(df['Applied Rating PDRR'], df['score_tmp'])))


bounds1 = [(15,45), (15,45), (15,45)]
res1 = differential_evolution(_sd_geo, bounds1)
res1.fun
# -0.5812776372884713

score_Americas, score_EMEA, score_Asia = res1.x
df['score_geo_adj'] = df['score'] + score_Americas*df['Geo_Americas'] + score_Asia*df['Geo_Asia'] +score_EMEA*df['Geo_EMEA']
SomersD(df['Applied Rating PDRR'], df['score_geo_adj'])
# 0.5812776372884713



sns.relplot(x='score_geo_adj', y='Applied Rating PDRR', data= aa, hue="Geo", alpha=0.7, palette="husl", height=9)

df_plot = df.groupby('Applied Rating PDRR')['score_geo_adj','Geo_Americas','Geo_EMEA','Geo_Asia'].mean()

df_plot.plot()

df_plot['Geo_Americas'].plot()
df_plot['Geo_EMEA'].plot()
df_plot['Geo_Asia'].plot()




x_train = sm.add_constant(df[['score_geo_adj']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5812776372884713
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5520284248081053






y_ = IsotonicRegression(y_min=2, y_max=15).fit_transform(df['fittedvalues'], df['Applied Rating PDRR'])
SomersD(df['Applied Rating PDRR'], y_)
# 0.5848191622809893

y_round = [round(x) for x in y_]
SomersD(df['Applied Rating PDRR'], y_round)
# 0.5697723262175707


df['iso'] = y_
df['iso_round'] = y_round




def match_rate(dat, col_a, col_b):
    return((dat[col_a] == dat[col_b]).sum() / len(dat))

def within_k_rate(dat, col_a, col_b, k=2):
    return((np.abs(dat[col_a]-dat[col_b])<=k).sum() / len(dat))



match_rate(df, 'Applied Rating PDRR', 'fittedvalues_round')
# 0.12227834871973524
within_k_rate(df, 'Applied Rating PDRR', 'fittedvalues_round',2)
# 0.6011147883643965

match_rate(df, 'Applied Rating PDRR', 'iso_round')
# 0.19561052081518898
within_k_rate(df, 'Applied Rating PDRR', 'iso_round',2)
# 0.769726528479359




#%% Size
df1 = df.query('Geo_Americas ==1')
df2 = df.query('Geo_EMEA ==1')
df3 = df.query('Geo_Asia ==1')


#%% Year
df4 = df.query('year==2016')
df5 = df.query('year==2017')
df6 = df.query('year==2018')

#%% model
df7 = df[df['Borrower Type']=='Corporation (Major Company)']
df8 = df[df['Borrower Type']=='Corporation (Other Company)']


#%% Industry
#list_dataset_name = list(df['Industry Model'].value_counts().index)
#list_dataset=[]
#for ind in list_dataset_name:
#    list_dataset.append(df[df['Industry Model']==ind])


list_dataset = [df, df1, df2, df3, df4, df5, df6, df7, df8]#+list_dataset
list_dataset_name =['Full_data','Americas','EMEA','Asia','2016','2017','2018','Major','Other']#+list_dataset_name



def calc_fun_fitted(list_dataset, list_dataset_name, y_type='raw'):

    list_obs_newmodel = []
    list_sd_newmodel = []
    list_mr_newmodel = []
    list_w2r_newmodel = []

    list_obs_tokyomodel = []
    list_sd_tokyomodel = []
    list_mr_tokyomodel = []
    list_w2r_tokyomodel = []

    result = pd.DataFrame()
    if y_type=='raw':
        y_pred = 'fittedvalues'
    elif y_type=='round':
        y_pred = 'fittedvalues_round'
    elif y_type=='iso':
        y_pred = 'iso_round'

    for dat in list_dataset:
   
        list_obs_newmodel.append(len(dat))
        if len(dat)>5:
            list_sd_newmodel.append(SomersD(dat['Applied Rating PDRR'], dat[y_pred]))
        else:
            list_sd_newmodel.append(np.nan)
        list_mr_newmodel.append(match_rate(dat, 'Applied Rating PDRR', y_pred))
        list_w2r_newmodel.append(within_k_rate(dat, 'Applied Rating PDRR', y_pred, 2))

        dat_tokyo =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_tokyomodel.append(len(dat_tokyo))
        if len(dat_tokyo)>5:
            list_sd_tokyomodel.append(SomersD(dat_tokyo['Applied Rating'], dat_tokyo['Primary Evaluation']))
        else:
            list_sd_tokyomodel.append(np.nan)
        list_mr_tokyomodel.append(match_rate(dat_tokyo, 'Applied Rating PDRR','PrimaryEvaluationPDRR'))
        list_w2r_tokyomodel.append(within_k_rate(dat_tokyo, 'Applied Rating PDRR', 'PrimaryEvaluationPDRR', 2))


    result['NewModel_size'] = list_obs_newmodel
    result['NewModel_SD'] = list_sd_newmodel
    result['NewModel_MatchRate'] = list_mr_newmodel
    result['NewModel_Within2Rate'] = list_w2r_newmodel

    result['TokyopModel_size'] = list_obs_tokyomodel
    result['TokyopModel_SD'] = list_sd_tokyomodel
    result['TokyoModel_MatchRate'] = list_mr_tokyomodel
    result['TokyoModel_Within2Rate'] = list_w2r_tokyomodel
    result.index = list_dataset_name

    return(result)


result = calc_fun_fitted(list_dataset, list_dataset_name,y_type='iso')
result.to_excel(r'SFAMFA\MM\result_noGua.xlsx')



CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'fitted', 'fittedvalues_round', 'Applied Rating PDRR',  PDRR=range(1,16))
CreateBenchmarkMatrix(df, r'SFAMFA\MM\temp_TM.xlsx', 'iso', 'iso_round', 'Applied Rating PDRR',  PDRR=range(1,16))







#%% demo
df_demo = pd.concat([df.query('Geo=="Asia"'),
    df.query('Geo=="Americas"').sample(79),
    df.query('Geo=="EMEA"').sample(79)
    ])


fig, ax = plt.subplots(figsize=(14,10))
ax = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax.set_yticks(np.linspace(2,15,14))#
ax.set_xlabel('Total Score by a Demo Model')#
ax.set_ylabel('Mapped to PDRR')#

ax2 = ax.twinx()
ax2.set_ylim(ax.get_ylim())
ax2.set_yticks(np.linspace(2,15,14))
ax2.set_yticklabels(['1-0','2-0/2-1','2-2','3-1','3-0/3-2','3-3','4-0',
 '5-1','5-2','6-1', '6-2','7-0', '8-1', '8-2'])
ax2 = sns.scatterplot(x='score_geo_adj', y='Applied Rating PDRR', data= df_demo, hue="Geo", alpha=0.9, palette=google_color[:3])
ax2.set_ylabel('BTMU Rating')#
plt.savefig(r"C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\SFAMFA\LC\demo.png")




#%% consider year
cty = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\Country Rates Merged 01-29-2020.xlsx')
cty['Cty_Name'] = [x.upper() for x in cty['GCARS Country Name']]
cty["Country Ratings"] = cty["Country Ratings"].astype('category')
cty["Cty_Rating_Label"] = cty["Country Ratings"].cat.codes
cty.drop_duplicates(subset=['Cty_Name','Period'], inplace=True)

dat = pd.merge(dat, cty, left_on=['Borrower Country','year'], right_on=['Cty_Name','Period'], how='left')
dat['Cty_Rating_Label'].fillna(2, inplace=True)


dat['Cty_Rating_Label2'] = dat['Cty_Rating_Label']
dat['Cty_Rating_Label2'].replace({
0: 9.376918,
1: 8.162304,
2: 8.601478,
3: 9.028239,
4:10.050725,
5:10.818182,
6:13.375000,
    },
    inplace=True)


dat['Cty_Rating_Label3'] = [0 if x<4 else 1 for x in dat['Cty_Rating_Label']]
dat['Cty_Rating_Label4'] = [0 if x<5 else 1 for x in dat['Cty_Rating_Label']]

dat = pd.concat([dat, pd.get_dummies(dat['Cty_Rating_Label'], prefix='Cty_Rating')], axis=1)




'''
#%% not consider year
cty = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\Country Rates Merged 01-29-2020.xlsx')
cty_name = cty.query('Period==2019')['GCARS Country Name'].unique().tolist()
cty_name = [x.upper() for x in cty_name]

dat_name  = dat['Borrower Country'].unique().tolist()
for name in dat_name:
    if name in cty_name:
        continue
    else:
        print(f'{name} is not in cty')





df = cty.query('Period==2019')
df["Country Ratings"] = df["Country Ratings"].astype('category')
df["Cty_Rating_Label"] = df["Country Ratings"].cat.codes
df['Cty_Name'] = [x.upper() for x in df['GCARS Country Name']]
dict_code = dict(zip(df['Cty_Name'], df['Cty_Rating_Label']))
dict_code.update({
    'MARSHALL ISLANDS':2,
    'LIBERIA':2,
    'JERSEY,C.I.':2,
    'Br. Virgin Is.':2, 
    'REPUBLIC OF MONTENEG':2, 
    'Lao P.D.R':2, 
    })
dat['Cty_Rating_Label'] = dat['Borrower Country'].copy()
dat['Cty_Rating_Label'] = dat['Cty_Rating_Label'].replace(dict_code)
dat['Cty_Rating_Label'].fillna(2, inplace=True)


dat['Cty_Rating_Label'].value_counts()
'''

#%%


SomersD(dat['Applied Rating PDRR'], dat['Cty_Rating_Label'])


SomersD(dat['Applied Rating PDRR'], dat['Cty_Rating_0.0'])





dat.groupby('Cty_Rating_Label')['Applied Rating PDRR'].mean()

SomersD(dat['Applied Rating PDRR'], dat['Cty_Rating_Label2'])



dat[['Applied Rating PDRR','Cty_Rating_Label']].corr()




#%%



#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Cty_Rating_Label4']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5545549365285304
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5325370780228447




from PDScorecardTool.Process import google_color
df2 = df[df.Cty_Rating_Label.isin([0,1,2,3])]

sns.relplot(x='score', y='Applied Rating PDRR', data=df2, hue="Cty_Rating_Label", alpha=0.7, 
    palette=google_color, height=9)# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCARS 2008-2019_matches2.pkl')


#%% Drop defaulters
def_mask = (dat['Applied Rating']==101) | (dat['Applied Rating']==102)
dat = dat[~def_mask]

country_str = 'Borrower Country'
country_count = dat[country_str].value_counts()
dat['year'].value_counts().sort_index()



#%% Drop rating 2-0, 2-3, 3-0, 8-3, 9-0
mask = dat['Applied Rating'].isin([20,23,30,83,90])
dat_dropped = dat[~mask]


# cast integer rating to string
dat_dropped['BTMU Rating'] = dat_dropped['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat_dropped['BTMU Rating'].value_counts().sort_index() 
'''
1-0      533
2-1      134
2-2      369
3-1      837
3-2     1540
3-3     1474
4-0    23073
5-1    10455
5-2     5634
6-1     4883
6-2     6239
7-0    11167
8-1     5979
8-2     4955
Name: BTMU Rating, dtype: int64
'''

# map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat_dropped['Applied Rating PDRR'] = dat_dropped['Applied Rating'].transform(lambda x: pd_mapping[x])
cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Match',
'Borrower Industry',
'Industry Model',
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']



dat_dropped[cols].to_pickle(r'..\data\CPPD_dat_dropped_2008-2019.pkl')


#%%  keep all rating

# cast integer rating to string
dat['BTMU Rating'] = dat['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat['BTMU Rating'].value_counts().sort_index() 
'''
1-0      533
2-0     7231
2-1      134
2-2      369
2-3      302
3-0    18852
3-1      837
3-2     1540
3-3     1474
4-0    23073
5-1    10455
5-2     5634
6-1     4883
6-2     6239
7-0    11167
8-1     5979
8-2     4955
8-3       67
9-0      252
Name: BTMU Rating, dtype: int64
'''


cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Match',
'Borrower Industry',
'Industry Model',
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']



dat[cols].to_pickle(r'..\data\CPPD_dat_2008-2019.pkl')
# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner

df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019.pkl')
df2 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019.pkl')


bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df2, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)
df_bvd.drop_duplicates(subset=['CIF','Fiscal year'], inplace=True)

len(set(df['Borrower CIF'].unique()))
# 23190

len(set(df2['Borrower CIF'].unique()))
# 19742

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(df['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 10745

len(set(df2['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 9187


#%%

#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% merge
df = pd.merge(df, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df['BvD_CIF']]
df['inBvD'].value_counts()
'''
Out[11]: 
No     61708
Yes    42268
Name: inBvD, dtype: int64

'''

df2 = pd.merge(df2, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df2['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df2['BvD_CIF']]
df2['inBvD'].value_counts()
'''
No     47410
Yes    29862
Name: inBvD, dtype: int64
'''



df.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')
df2.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')

# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Major Company)"]

# 21972
df['year'].value_counts().sort_index()
'''
2008      98
2009     785
2010    2056
2011    2306
2012    2206
2013    2412
2014    2680
2015    2522
2016    2437
2017    1785
2018    1419
2019    1266
Name: year, dtype: int64
'''
df['inBvD'].value_counts()
'''
No     14307
Yes     7665
Name: inBvD, dtype: int64
'''


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

Major_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, Major_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_results_Major.xlsx')


list_factor = model_LC.quant_factor + model_MM.quant_factor + Major_factor_cols
list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,-1,-1,-1,-1,1,-1,-1,-1,1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        if len(df_tmp_dropna)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        if len(dat_LC)<5:
            list_sd_LC.append(np.nan)
        else:
            list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        if len(dat_MM)<5:
            list_sd_MM.append(np.nan)
        else:
            list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        if len(dat_model)<5:
            list_sd_model.append(np.nan)
        else:
            list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['MajorCorpModel_size'] = list_obs_model
    result['MajorCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Industry Model'
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Major Company)"]

# 16483
df['year'].value_counts().sort_index()
'''
2008      62
2009     498
2010    1651
2011    1837
2012    1767
2013    1966
2014    2175
2015    2009
2016    1913
2017    1117
2018     788
2019     700
Name: year, dtype: int64
'''
df['inBvD'].value_counts()
'''
No     11527
Yes     4956
Name: inBvD, dtype: int64
'''


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

Major_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, Major_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_dat_dropped_results_Major.xlsx')

list_factor = model_LC.quant_factor + model_MM.quant_factor + Major_factor_cols
list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,-1,-1,-1,-1,1,-1,-1,-1,1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        if len(df_tmp_dropna)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        if len(dat_LC)<5:
            list_sd_LC.append(np.nan)
        else:
            list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        if len(dat_MM)<5:
            list_sd_MM.append(np.nan)
        else:
            list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        if len(dat_model)<5:
            list_sd_model.append(np.nan)
        else:
            list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['MajorCorpModel_size'] = list_obs_model
    result['MajorCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Industry Model'
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Major Company)"]



def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

Major_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, Major_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_results_Major_dropped_3year.xlsx')

list_factor = model_LC.quant_factor + model_MM.quant_factor + Major_factor_cols
#df=df.dropna(subset=model_LC.quant_factor+model_MM.quant_factor+['Applied Rating','Primary Evaluation'], how='any')

df=df.query('2016<=year<=2018')


list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,-1,-1,-1,-1,1,-1,-1,-1,1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        if len(df_tmp_dropna)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        if len(dat_LC)<5:
            list_sd_LC.append(np.nan)
        else:
            list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        if len(dat_MM)<5:
            list_sd_MM.append(np.nan)
        else:
            list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        if len(dat_model)<5:
            list_sd_model.append(np.nan)
        else:
            list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['MajorCorpModel_size'] = list_obs_model
    result['MajorCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Industry Model'
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Other Company)"]
# 82004
df['year'].value_counts().sort_index()
'''
2008      97
2009    2800
2010    7323
2011    7838
2012    8728
2013    8830
2014    9011
2015    7455
2016    7447
2017    7929
2018    7529
2019    7017
Name: year, dtype: int64
'''
df['inBvD'].value_counts()
'''
No     47401
Yes    34603
Name: inBvD, dtype: int64
'''


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

other_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, other_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_results_Other.xlsx')


list_factor = model_LC.quant_factor + model_MM.quant_factor + other_factor_cols
list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,1,-1,-1,1,-1,-1,-1,-1,-1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['OtherCorpModel_size'] = list_obs_model
    result['OtherCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Borrower Industry'
list_dataset_name = list(df['Borrower Industry'].value_counts()[:20].index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Borrower Industry']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Other Company)"]
# 60789
df['year'].value_counts().sort_index()
'''
2008      72
2009    2248
2010    5862
2011    6061
2012    6831
2013    7019
2014    7020
2015    5648
2016    5705
2017    5440
2018    4657
2019    4226
Name: year, dtype: int64
'''
df['inBvD'].value_counts()
'''
Out[94]: 
No     35883
Yes    24906
Name: inBvD, dtype: int64
'''


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

other_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, other_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_dat_dropped_results_Other.xlsx')

list_factor = model_LC.quant_factor + model_MM.quant_factor + other_factor_cols
list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,1,-1,-1,1,-1,-1,-1,-1,-1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['OtherCorpModel_size'] = list_obs_model
    result['OtherCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Borrower Industry'
list_dataset_name = list(df['Borrower Industry'].value_counts()[:20].index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Borrower Industry']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
#df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')
df = df[df['Borrower Type']=="Corporation (Other Company)"]

def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

other_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, other_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
#writer = pd.ExcelWriter(r'SFA/CPPD_results_Other_overlaped.xlsx')
#writer = pd.ExcelWriter(r'SFA/CPPD_results_Other_dropped_overlaped_matched.xlsx')
#writer = pd.ExcelWriter(r'SFA/CPPD_results_Other_dropped_overlaped_3year.xlsx')
writer = pd.ExcelWriter(r'SFA/CPPD_results_Other_dropped_3year.xlsx')

list_factor = model_LC.quant_factor + model_MM.quant_factor + other_factor_cols
#df=df.dropna(subset=list_factor, how='any')
#df=df.query('Match==1')
df=df.query('2016<=year<=2018')


list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,1,-1,-1,1,-1,-1,-1,-1,-1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        if len(df_tmp_dropna)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        if len(dat_LC)<5:
            list_sd_LC.append(np.nan)
        else:
            list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        if len(dat_MM)<5:
            list_sd_MM.append(np.nan)
        else:
            list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        if len(dat_model)<5:
            list_sd_model.append(np.nan)
        else:
            list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))


    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['OtherCorpModel_size'] = list_obs_model
    result['OtherCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Borrower Industry'
list_dataset_name = list(df['Borrower Industry'].value_counts()[:20].index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Borrower Industry']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
from PDScorecardTool.Process import gcar_converter
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
#df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')
dat = dat[dat['Borrower Type']=="Corporation (Other Company)"]

def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

other_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


dat = build_cni_factors(dat)
dat = other_processing(dat)
gcar_converter(dat, other_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor+other_factor_cols:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))
     


list_factor = model_LC.quant_factor + model_MM.quant_factor + other_factor_cols
dat=dat.dropna(subset=list_factor, how='any')
#dat=dat.query('Match==1')
dat=dat.query('2016<=year<=2018')


list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,1,-1,-1,1,-1,-1,-1,-1,-1]



#%% Build ratios
cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']


#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios

dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']



factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + other_factor_cols+ [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))




#%%

#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in factor_list:
    df_tmp_dropna = dat.dropna(subset=[factor])
    list_quality.append(len(df_tmp_dropna)/ len(dat))
    list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor])))

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = factor_list
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'EXP/CPPD_SFA_other_dropped_overlaped_3years.xlsx')




#%% pick SD > 0.20
list_short = result.query('SomersD>=0.2 and DataQuality>=0.7').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)



#%% after manual selection
list_final=[
'Current Financial Data #7',
'Current Financial Data #6',
'Current Financial Data #8',
'Current Financial Data #3',
'prof@EBITDA_to_TA',
'Current Financial Data #1',
'LC_UBEBITDA_to_IE', 
'LC_UBEBITDA_to_NS',
'new@CLCoverageRatio', 
'BvD_P/L for period [=Net income] ', 
'Current Financial Data #4', 
'new@CashFlowRatio',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_Capt',
'Current Financial Data #5',
'new@LTDCoverageRatio']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(np.abs(dat[list_final].corr()), linewidths=.3, cmap='Blues', ax=ax)




for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(4,4,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(4,4,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
0,
0,
0,
0,
0,
'Current Financial Data #1',
0,0,0,
'BvD_P/L for period [=Net income] ', 
0,0,0,0,0,0]

dat['Current Financial Data #1'] = dat['Current Financial Data #1'] + np.abs(dat['Current Financial Data #1'].min()) +1
dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1




#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'Current Financial Data #7',
'Current Financial Data #6',
'Current Financial Data #8',
'Current Financial Data #3',
'prof@EBITDA_to_TA',
'LC_UBEBITDA_to_IE', 
'LC_UBEBITDA_to_NS',
'new@CLCoverageRatio', 
'Current Financial Data #4', 
'new@CashFlowRatio',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_Capt',
'Current Financial Data #5',
'new@LTDCoverageRatio']



for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

list_neg = [0,]*16





#%% MFA
dat_norm = dat[list_final+['Borrower CIF',
 'Borrower Name',
 'Borrower Country',
 'Borrower Type',
 'Borrower Type 2',
 'Borrower Office Code',
 'Borrower Office Name',
 'timestamp',
 'year',
 'Current Total Score',
 'Primary Evaluation',
 'BTMU Rating',
 'Applied Rating',
 'Applied Rating PDRR',
 'Match',
 'Borrower Industry',
 'Industry Model',
 'BvD_Proxy_Net_Sales_inUSD']].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()






#%%  model selection

writer = pd.ExcelWriter(r'EXP/CPPD_MFA_other_dropped_overlaped_3years_more.xlsx')
#for factor_num in range(4,8):
for factor_num in range(8,11):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()



#%% model performance

model =['Current Financial Data #7', 'Current Financial Data #6', 'Current Financial Data #1', 'Current Financial Data #5']


df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')

x_train = sm.add_constant(df[model], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit(disp=0)
df['fittedvalues'] = result.fittedvalues

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
#0.5454782648499367

#%%



#%% model score and perform
def calc_fun_mfa(list_dataset, list_dataset_name):
    list_obs = []
    list_sd = []
    result = pd.DataFrame()
    for dat in list_dataset:
        list_obs.append(len(dat))
        if len(dat)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(SomersD(dat['Applied Rating PDRR'], dat['fittedvalues']))

    result['DataSize'] = list_obs
    result['SomersD'] = list_sd
    result.index = list_dataset_name

    return(result)

#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun_mfa(list_dataset, list_dataset_name)

#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun_mfa(list_dataset, list_dataset_name)



#%% Industry by 'Borrower Industry'
list_dataset_name = list(df['Borrower Industry'].value_counts()[:20].index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Borrower Industry']==ind])

result3 = calc_fun_mfa(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2016')
df2 = df.query('year==2017')
df3 = df.query('year==2018')

list_dataset = [df1, df2, df3]
list_dataset_name =['2016','2017','2018']

result4 = calc_fun_mfa(list_dataset, list_dataset_name)


result = pd.concat([result1,result2,result3,result4], axis=0)


result.to_excel(r'EXP\4factor.xlsx')
import os, sys, pandas as pd, numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(palette='muted')



def cluster_corr(dat, pl_cols):
    pl_cat1 = [pl_cols[0],]
    pl_cat2 = []

    for name in pl_cols[1:]:
        if np.abs(dat[[name]+pl_cat1].corr()).iloc[0,1:].mean() > 0.75:
            pl_cat1.append(name)
        else:
            pl_cat2.append(name)
    if len(pl_cat2)==0:
        return((pl_cat1,))
    else:
        return((pl_cat1,) + cluster_corr(dat, pl_cat2))


def plot_cluster_corr(dat, cat_tuple):
    cols = []
    for cat in cat_tuple:
        cols = cols+cat

    f, ax = plt.subplots(figsize=(10*len(cols)/25, 6*len(cols)/25))
    sns.heatmap(np.abs(dat[cols].corr()), linewidths=.3, cmap='Blues', ax=ax)


def quanttrans(data, model, floor=0.05, cap=0.95):
    """
    This function calculates the floor, cap, mean and std of the data. The output
    can be used as the updated parameters for quantitative factors normailization.
    Modified in Ver. 1.4

    Parameters:

        data:   the input dataset in DataFrame. Make sure no NA in quant
                factors

        model:  PDModel class

        floor:  float, default 0.05
                quantile for floor 

        cap:    float, default 0.95
                quantile for cap 

        
    Return:
        a dictionary that saves floor, cap, doc_mean, doc_std
    
    """    
    normdata = data.copy()


    # get new cap and floor from valid observations:
    floor_list=[];  cap_list=[]

    for i, factor in enumerate(model.quant_factor):
        if not model.Invalid_Neg[i]:
            floor_list.append(normdata[factor].quantile(floor))
            cap_list.append(normdata[factor].quantile(cap))
        else:
            floor_list.append(normdata[factor][normdata[factor]>0].quantile(floor))
            cap_list.append(normdata[factor][normdata[factor]>0].quantile(cap))


    # Invalid_Neg
    for i,neg in enumerate(model.Invalid_Neg):
        if neg:
            col=model.quant_factor[i]
            normdata[col][normdata[col]<0] = cap_list[i]
           
    # cap/floor for quant factors:
    for i, col in enumerate(model.quant_factor):
        normdata[col] = np.clip(normdata[col], floor_list[i], cap_list[i])        

    # quant factors transformation:
    for i, col in enumerate(model.quant_factor):
        if model.quant_log[i]:
            normdata[col] = np.log(normdata[col])

    # get new mean and std:
    doc_mean=[];    doc_std=[]

    for i, col in enumerate(model.quant_factor):
        doc_mean.append(normdata[col].mean())     
        doc_std.append(normdata[col].std())     

    dictionary = {'floor':floor_list, 'cap':cap_list, 'doc_mean':doc_mean, 'doc_std':doc_std}
    
    return(dictionary)


import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_cif_cleaner, gcar_americas_office_code

data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019.pkl')
data = data[data['Data From']=='CD']
data = data[data['Borrower Type']=='Corporation (Other Company)']

data.dropna(subset=['Borrower CIF'], inplace=True)
data['Borrower CIF_cleaned'] =gcar_cif_cleaner(data)
#

data['timestamp'] = data['Approval Date'].mask(
                        pd.isnull(data['Approval Date']),
                        data['Approval/Agreed Date'])
data['timestamp'] = pd.to_datetime(data['timestamp'])
data.dropna(subset=['timestamp'], inplace=True)
data['year'] = [int(x.year) for x in data['timestamp'] ]

# 2018-19 other corp
dat = data.query('year>2017')
is_model = dat['Borrower Type']=='Corporation (Other Company)'

Corp_Others_ModelData = dat[is_model]
Corp_Others_ModelData.shape
# (20373, 903)


isempty_cols = Corp_Others_ModelData.filter(regex='^Current Financial Data').isnull().all() 
# Corp_Others_ModelData[isempty_cols.index[~isempty_cols]].apply(lambda x:pd.to_numeric(x.str.replace('\,|\%',''), errors='coerce'))
COModel_factor_names = Corp_Others_ModelData[isempty_cols.index[~isempty_cols].str.replace('Current Financial Data', 'Scoring Name')].apply(lambda x:x.dropna().unique()).T[0].values

# dict(zip(model_factor_names, [dict(zip(['bins','labels'],[[],[]]))]*len(COModel_factor_names)))
factor_bins = \
{'Corporation (Other Company)':
    {

     '1. Capability for Debt Repayment - (1) Cash Flow Amount': {'bins': [-pd.np.Inf, 10*10**6, 30*10**6, 50*10**6, 70*10**6, 100*10**6, 300*10**6, 500*10**6, 700*10**6, 1000*10**6, pd.np.Inf],
                                                                 'labels': [2, 3, 4, 6, 8, 9, 11, 12, 13, 15]},
     '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA': {'bins': [-pd.np.Inf, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2, 8.2, 9.2, 10.2, pd.np.Inf],
                                                                                  'labels': [10, 8, 6, 4, 2, 0, -2, -4, -6, -8, -10]},#EBITA is negative then set it to -10
     '1. Capability for Debt Repayment - (3) EBITDA / Net Interest Payment': {'bins': [-pd.np.Inf, 0.85, 0.95, 2, 3, 4, 6, 8, pd.np.Inf],
                                                                              'labels': [0, 2, 4, 6, 8, 10, 11, 12]},
     '2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio': {'bins': [-pd.np.Inf, -80, -70, -60, -50, -40, -30, -20, -10, 0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, pd.np.Inf],
                                                                                  'labels': [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
     '2. Financial Condition - 2-1. Safety - (2) Gearing Ratio': {'bins': [-pd.np.Inf, 50, 75, 100, 150, 200, pd.np.Inf],
                                                                  'labels': [10, 8, 6, 4, 2, 0]},
     '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales': {'bins': [-pd.np.Inf, 1, 2, 3, 6, 8, 10, 12, 14, 16, 20, pd.np.Inf],
                                                                                    'labels': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
     '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets': {'bins': [-pd.np.Inf, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, pd.np.Inf],
                                                                                                             'labels': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]},
     '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth': {'bins': [-pd.np.Inf, -40, -30, -20, -10, -2.5, 0, 0.5, 5, 25, 35, pd.np.Inf],
                                                                                                                    'labels': [-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10]},
     '2. Financial Condition - 2-4. Size - (1) Net Worth': {'bins': [-pd.np.Inf, 50*10**6, 100*10**6, 150*10**6, 200*10**6, 350*10**6, 500*10**6, 750*10**6, 1000*10**6, 1500*10**6, 2000*10**6, pd.np.Inf],
                                                            'labels': [0, 1, 3, 4, 6, 7, 8, 10, 11, 13, 14]},
     '2. Financial Condition - 2-4. Size - (2) Sales': {'bins': [-pd.np.Inf, 100*10**6, 500*10**6, 1000*10**6, 3000*10**6, pd.np.Inf],
                                                        'labels': [0, 1, 2, 3, 4]}

    }
}
# factor_bins[model_names[3]]



score_df = pd.DataFrame()
for idx in isempty_cols.index[~isempty_cols].str.replace('Current Financial Data #',''):
    factor_num='{}'.format(idx)
    scr_str=Corp_Others_ModelData['Scoring Name #'+factor_num].dropna().unique()[0]    
    print(scr_str)
    df=Corp_Others_ModelData['Current Financial Data #'+factor_num]    
    df_clean = pd.to_numeric(df.astype(str).apply(lambda x:x.replace('%','').replace(',','')), errors='coerce')
    
    is_right = True
    if scr_str in ['2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio',
                   '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales',
                   '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets',
                   '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth',
                   '2. Financial Condition - 2-4. Size - (1) Net Worth']:
        is_right = False
    
    binned_df=pd.cut(df_clean,
                     bins=factor_bins[model_names[3]][scr_str]['bins'], 
                     labels=factor_bins[model_names[3]][scr_str]['labels'], right=is_right)
    df_2 = pd.concat([Corp_Others_ModelData.filter(regex='^Borrower Name$|^Application No.$|^Approval Date'),
                      df,df_clean, binned_df, Corp_Others_ModelData['Current Score #'+factor_num]], axis=1)
#     display(df_2.head())
    bin_counts = binned_df.value_counts(dropna=False).sort_index()
#     display(bin_counts.to_frame())
    nan_diff = bin_counts[bin_counts.index[-1:]].values[0]-df.isna().value_counts()[True]
    if nan_diff>0:
        print('Please check Current Financial Data #{} as there are {} strings which are not NaNs:'.format(factor_num,nan_diff))
#         non_numbered_df = df[~df.isna()].astype(str).apply(lambda x:x.replace('%','').replace(',',''))   
#         print(non_numbered_df[pd.to_numeric(non_numbered_df, errors='coerce').isna()].value_counts(),'\n') 
  
    if scr_str == '1. Capability for Debt Repayment - (1) Cash Flow Amount':
        binned_df = binned_df.astype(float)
        binned_df[df_clean<0]=0
        binned_df = binned_df.astype('category')

    #Deal with negative EBITA for the factor - '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA'
    if scr_str == '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA':
        binned_df = binned_df.astype(float)
        binned_df[(df == 'Negative EBITDA')|(df == '#DIV/0!')]=-10
        binned_df = binned_df.astype('category')
#         print(df_2.iloc[[13781,12594]])        
#         print(df_2[(df == 'Negative EBITDA')|(df == '#DIV/0!')].shape)
#         print(df.isna().value_counts()) 

    if scr_str == '1. Capability for Debt Repayment - (3) EBITDA / Net Interest Payment':
        binned_df = binned_df.astype(float)
        binned_df[(df == 'EBITDA is 0 or less')|(df == 'Negative EBITDA')]=0
        binned_df[df=='#DIV/0!']=12
        binned_df = binned_df.astype('category')
        
    if scr_str in ['2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio',
                   '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets',
                   '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth']:
        binned_df = binned_df.astype(float)
        binned_df[df=='#DIV/0!']=0
        binned_df = binned_df.astype('category') #use this option right = False for binning

    if scr_str == '2. Financial Condition - 2-1. Safety - (2) Gearing Ratio':
        binned_df[(df=='#DIV/0!')|(df == 'Negative EQuity')]=0
        binned_df = binned_df.astype(float)
        binned_df[df_clean.le(0)]=0
        binned_df = binned_df.astype('category')
           
    if scr_str == '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales':
        binned_df[(df=='#DIV/0!')|(df == 'Negative Sales')]=0
    
    diff_score = binned_df.fillna(0).sub(Corp_Others_ModelData['Current Score #'+factor_num], fill_value=0.0)
#     print(df_2[(diff_score.abs()>0)].head(20))
    vCts = diff_score.value_counts(dropna=False).sort_index()
#     print(vCts)
    print('Number of mismatches:',vCts[(vCts.index!=0.0) & (~vCts.index.isna())].sum(),'\n')

    bin_counts = binned_df.value_counts(dropna=False).sort_index()
    nan_diff = bin_counts[bin_counts.index[-1:]].values[0]-df.isna().value_counts()[True]
#     print('Current Financial Data #{} after migtigating for the NaNs:{}\n'.format(factor_num,nan_diff))
    score_df=pd.concat([score_df, binned_df], axis=1)
# display(score_df.head())
score_mismatches = score_df.sum(axis=1).sub(Corp_Others_ModelData['Current Total Score'], fill_value=0.0).value_counts(dropna=False).sort_index()
print('Total matches out of {} are {} ({:.0%})'.format(score_df.shape[0],score_mismatches.loc[0.0],score_mismatches.loc[0.0]/score_df.shape[0]))list_factor=['BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
 'BvD_Number of employees',
 'LC_UBEBITDA_to_NS',
 'LC_TD_to_UBEBITDA',
 'LC_Total Assets',
 'LC_TD_to_Capt',
 'LC_UBEBITDA_to_IE',
 'MM_NOP_to_NS',
 'MM_TD_to_UBEBITDA',
 'MM_TD_to_Capt',
 'MM_ECE_to_TL',
 'MM_TangNW_to_TA',
 'cf@TD_to_ACF',
 'bs@TD_to_CA_exc_CL',
 'liq@CA_exc_CL_to_TD',
 'liq@CA_exc_CL_to_TL',
 'liq@CA_exc_CL_to_TA',
 'liq@CA_exc_CL_to_CA',
 'liq@CA_exc_CL_to_CL',
 'size@TangNW_to_TA_exc_CA',
 'bs@TL_to_TL_exc_CL',
 'cf@TD_to_GP_and_Dep_and_Amo',
 'ds@GP_and_IE_to_IE',
 'ds@GP_and_IE_to_TL',
 'prof@TangA_to_NS',
 'size@TangNW_to_TA',
 'BTMU@EBITDA_to_IE',
 'BTMU@TD_to_EBITDA',
 'BTMU@EBITDA_to_IE',
 'bs@TD_to_CA_exc_CL',
 'bs@TD_to_TA_exc_TL',
 'cf@TD_to_GP_and_Dep_and_Amo',
 'cf@TD_to_GP_and_Dep_and_Amo',
 'cf@TD_NOP',
 'BTMU@OP_to_Sales',
 'cf@TD_to_EBITDA',
 'prof@EBITDA_to_Capt',
 'prof@EBITDA_to_NS',
 'prof@EBITDA_to_TangA',
 'prof@EBITDA_to_TA',
 'act@NS_to_CL',
 'act@NS_to_TA',
 'bs@TD_to_Capt',
 'bs@TL_to_Capt',
 'bs@CL_to_Capt',
 'prof@NOP_to_NS',
 'prof@NOP_to_TangNW',
 'prof@NOP_to_TA',
 'bs@TLTD_to_TA',
 'bs@TLTD_to_TangNW',
 'bs@TLTD_to_Capt',
 'bs@TL_to_TangNW',
 'bs@TL_to_TA',
 'bs@CL_to_TL',
 'ds@TL_to_IE',
 'liq@ECE_to_TL',
 'liq@CA_to_TL',
 'bs@TD_to_CA_exc_CL',
 'liq@ECE_to_CA',
 'liq@CA_to_TA',
 'liq@ECE_to_TD',
 'liq@ECE_to_TA',
 'liq@ECE_to_CL',
 'new@COGSRatio',
 'new@GrossMarkup',
 'new@GrossProfitMargin',
 'new@GrossProfitRatio',
 'newprof@GrossMarginRatio',
 'new@NetSalesRevenue',
 'new@CashFlowRatio',
 'new@AssetEfficiencyRatio',
 'new@CLCoverageRatio',
 'new@LTDCoverageRatio',
 'new@InterestCoverageRatio',
 'new@DA_to_Sales',
 'new@CostPerEmployee',
 'new@IFATurnover',
 'new@TFATurnover',
 'Other_Current Financial Data #1',
 'SP_Current Financial Data #1',
 'Other_Current Financial Data #2',
 'SP_Current Financial Data #2',
 'Other_Current Financial Data #3',
 'SP_Current Financial Data #3',
 'Other_Current Financial Data #4',
 'SP_Current Financial Data #4',
 'Other_Current Financial Data #5',
 'SP_Current Financial Data #5',
 'Other_Current Financial Data #6',
 'SP_Current Financial Data #6',
 'Other_Current Financial Data #7',
 'SP_Current Financial Data #7',
 'Other_Current Financial Data #8',
 'SP_Current Financial Data #8',
 'Other_Current Financial Data #9',
 'SP_Current Financial Data #9',
 'Other_Current Financial Data #10',
 'SP_Current Financial Data #10',
 'Geo_Americas',
 'Geo_Asia',
 'Geo_EMEA',
 'Tokyo_Corporation (Major Company)',
 'Tokyo_Corporation (Other Company)',]
# -*- coding: utf-8 -*-
"""
Created on Tue Sep  8 10:40:51 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np
import math
import matplotlib.pyplot as plt
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_20200903.pkl')
df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')


# Benford's Law percentages for leading digits 1-9
BENFORD = [30.1, 17.6, 12.5, 9.7, 7.9, 6.7, 5.8, 5.1, 4.6]


def count_first_digit(data_str):
    mask=df[data_str]>=1.
    data=list(df[mask][data_str])
    for i in range(len(data)):
        while data[i]>=10:
            data[i]=data[i]/10
    first_digits=[int(x) for x in sorted(data)]
    data_count=[]
    for i in range(9):
        count=first_digits.count(i+1)
        data_count.append(count)
    total_count=sum(data_count)
    data_percentage=[(i/total_count)*100 for i in data_count]
    return  total_count,data_count, data_percentage


def get_expected_counts(total_count):
    """Return list of expected Benford's Law counts for total sample count."""
    return [round(p * total_count / 100) for p in BENFORD]


def chi_square_test(data_count,expected_counts):
    """Return boolean on chi-square test (8 degrees of freedom & P-val=0.05)."""
    chi_square_stat = 0  # chi square test statistic
    for data, expected in zip(data_count,expected_counts):

        chi_square = math.pow(data - expected, 2)

        chi_square_stat += chi_square / expected

    print("\nChi-squared Test Statistic = {:.3f}".format(chi_square_stat))
    print("Critical value at a P-value of 0.05 is 15.51.")    
    return chi_square_stat < 15.51



#%% Plot

def bar_chart(data_pct, name):

    """Make bar chart of observed vs expected 1st digit frequency in percent."""

    fig, ax = plt.subplots()
    index = [i + 1 for i in range(9)]  # 1st digits for x-axis
# text for labels, title and ticks
    fig.canvas.set_window_title('Percentage First Digits')
    ax.set_title(f'{name[-8:]} vs. Benford Values', fontsize=15)
    ax.set_ylabel('Frequency (%)', fontsize=16)
    ax.set_xticks(index)
    ax.set_xticklabels(index, fontsize=14)

    # build bars    

    rects = ax.bar(index, data_pct, width=0.95, color='black', label='Data')

    # attach a text label above each bar displaying its height

    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2, height,
                '{:0.1f}'.format(height), ha='center', va='bottom', 
                fontsize=13)

    # plot Benford values as red dots
    ax.scatter(index, BENFORD, s=150, c='red', zorder=2, label='Benford')
    # Hide the right and top spines & add legend
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.legend(prop={'size':15}, frameon=False)

    plt.show()




total_count,data_count, data_percentage = count_first_digit('Current Financial Data #1')
expected_counts=get_expected_counts(total_count)
chi_square_test(data_count,expected_counts)

#%%
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']
df = dat[dat['Borrower Type']=='Corporation (Major Company)'].copy()


fig, axes = plt.subplots(2,5,figsize=(18,10))
for fac, ax in zip(curr, axes.flatten()):

    total_count,data_count, data_percentage = count_first_digit(fac)
    expected_counts=get_expected_counts(total_count)
    
    #bar_chart(data_percentage, fac)
    index = [i + 1 for i in range(9)]  # 1st digits for x-axis
    # text for labels, title and ticks
    #fig.canvas.set_window_title('Percentage First Digits')
    ax.set_title(f'{fac[-8:]} vs. Benford Values', fontsize=8)
    ax.set_ylabel('Frequency (%)', fontsize=10)
    ax.set_xticks(index)
    ax.set_xticklabels(index, fontsize=9)

    # build bars    

    rects = ax.bar(index, data_percentage, width=0.95, color='black', label='Data')

    # attach a text label above each bar displaying its height

    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2, height,
                '{:0.1f}'.format(height), ha='center', va='bottom', 
                fontsize=13)

    # plot Benford values as red dots
    ax.scatter(index, BENFORD, s=150, c='red', zorder=2, label='Benford')
    # Hide the right and top spines & add legend
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.legend(prop={'size':10}, frameon=False)

    plt.show()







bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']



gcar_converter(df_bvd, bvd_cols, inplace=True)
df = df_bvd.copy()

fig, axes = plt.subplots(2,5,figsize=(18,10))
for fac, ax in zip(bvd_cols[:10], axes.flatten()):

    total_count,data_count, data_percentage = count_first_digit(fac)
    expected_counts=get_expected_counts(total_count)
    
    #bar_chart(data_percentage, fac)
    index = [i + 1 for i in range(9)]  # 1st digits for x-axis
    # text for labels, title and ticks
    #fig.canvas.set_window_title('Percentage First Digits')
    ax.set_title(f'{fac[-8:]} vs. Benford Values', fontsize=8)
    ax.set_ylabel('Frequency (%)', fontsize=10)
    ax.set_xticks(index)
    ax.set_xticklabels(index, fontsize=9)

    # build bars    

    rects = ax.bar(index, data_percentage, width=0.95, color='black', label='Data')

    # attach a text label above each bar displaying its height

    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2, height,
                '{:0.1f}'.format(height), ha='center', va='bottom', 
                fontsize=13)

    # plot Benford values as red dots
    ax.scatter(index, BENFORD, s=150, c='red', zorder=2, label='Benford')
    # Hide the right and top spines & add legend
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.legend(prop={'size':10}, frameon=False)

    plt.show()



#%% check bvd ratio
dat_LC = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat_MM = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')

df = pd.concat([dat_LC, dat_MM])

curr=[
'LC_UBEBITDA_to_NS',
'LC_TD_to_UBEBITDA',
'LC_Total Assets',
'LC_TD_to_Capt',
'LC_UBEBITDA_to_IE',
'MM_NOP_to_NS',
'MM_TD_to_UBEBITDA',
'MM_TD_to_Capt',
'MM_ECE_to_TL',
'MM_TangNW_to_TA',]

df['LC_Total Assets'] = df['LC_Total Assets'].transform(np.exp)



fig, axes = plt.subplots(2,5,figsize=(18,10))
for fac, ax in zip(curr, axes.flatten()):

    total_count,data_count, data_percentage = count_first_digit(fac)
    expected_counts=get_expected_counts(total_count)
    
    #bar_chart(data_percentage, fac)
    index = [i + 1 for i in range(9)]  # 1st digits for x-axis
    # text for labels, title and ticks
    #fig.canvas.set_window_title('Percentage First Digits')
    ax.set_title(f'{fac} vs. Benford Values', fontsize=8)
    ax.set_ylabel('Frequency (%)', fontsize=10)
    ax.set_xticks(index)
    ax.set_xticklabels(index, fontsize=9)

    # build bars    

    rects = ax.bar(index, data_percentage, width=0.95, color='black', label='Data')

    # attach a text label above each bar displaying its height

    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2, height,
                '{:0.1f}'.format(height), ha='center', va='bottom', 
                fontsize=13)

    # plot Benford values as red dots
    ax.scatter(index, BENFORD, s=150, c='red', zorder=2, label='Benford')
    # Hide the right and top spines & add legend
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.legend(prop={'size':10}, frameon=False)

    plt.show()

# -*- coding: utf-8 -*-
"""
Created on Tue Nov  3 13:25:00 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')

dfc = pd.read_pickle(r'..\data\Compustat_buildratio_20201106_addtag20201203.pkl')
dfb = pd.read_pickle(r'..\data\dat_2008-2019_cleaned_BvD_relink_addratios.pkl')



#%% bvd
dat = dfb.query('year>=2010')
dat = dat.query('inBvD=="Yes"')
dat.dropna(subset=['Applied Rating'], inplace=True)

bvd_cols=[
'BvD_Fixed assets',
'BvD_Intangible fixed assets',
'BvD_Tangible fixed assets',
'BvD_Other fixed assets',
'BvD_Current assets',
'BvD_Stock',
'BvD_Debtors',
'BvD_Other current assets',
'BvD_Cash & cash equivalent',
'BvD_Total assets',
'BvD_Shareholders funds',
'BvD_Capital',
'BvD_Other shareholders funds',
'BvD_Non-current liabilities',
'BvD_Long term debt',
'BvD_Other non-current liabilities',
'BvD_Provisions (Liabilities & Equity)',
'BvD_Current liabilities',
'BvD_Loans (Liabilities & Equity)',
'BvD_Creditors',
'BvD_Other current liabilities',
'BvD_Total shareh. funds & liab.',
'BvD_Working capital',
'BvD_Net current assets',
'BvD_Enterprise value',
'BvD_Number of employees',
'BvD_Operating revenue (Turnover)',
'BvD_Sales',
'BvD_Costs of goods sold',
'BvD_Gross profit',
'BvD_Other operating expenses',
'BvD_Operating P/L [=EBIT]',
'BvD_Financial P/L',
'BvD_Financial revenue',
'BvD_Financial expenses',
'BvD_P/L before tax',
'BvD_Taxation',
'BvD_P/L after tax',
'BvD_Extr. and other P/L',
'BvD_Extr. and other revenue',
'BvD_Extr. and other expenses',
'BvD_P/L for period [=Net income] ',
'BvD_Export revenue',
'BvD_Material costs',
'BvD_Costs of employees',
'BvD_Depreciation & Amortization',
'BvD_Other operating items',
'BvD_Interest paid',
'BvD_Research & Development expenses',
'BvD_Cash flow',
'BvD_Added value',
'BvD_EBITDA',
'BvD_Proxy_Total_Assets',
'BvD_Proxy_Interest_expense',
'BvD_Proxy_Ending_Cash_Equiv',
'BvD_Proxy_UBEBITDA',
'BvD_Proxy_Net_Sales',
'BvD_Proxy_Total_Debt',
'BvD_Proxy_Capitalization',
'BvD_Proxy_Net_Op_Profit',
'BvD_Proxy_Total_Liabilities',
'BvD_Proxy_Tangible_Net_Worth',
'BvD_Proxy_Total_Assets_inUSD',
'BvD_Proxy_Interest_expense_inUSD',
'BvD_Proxy_Ending_Cash_Equiv_inUSD',
'BvD_Proxy_UBEBITDA_inUSD',
'BvD_Proxy_Net_Sales_inUSD',
'BvD_Proxy_Total_Debt_inUSD',
'BvD_Proxy_Capitalization_inUSD',
'BvD_Proxy_Net_Op_Profit_inUSD',
'BvD_Proxy_Total_Liabilities_inUSD',
'BvD_Proxy_Tangible_Net_Worth_inUSD',
'LC_UBEBITDA_to_NS',
'LC_TD_to_UBEBITDA',
'LC_Total Assets',
'LC_TD_to_Capt',
'LC_UBEBITDA_to_IE',
'MM_NOP_to_NS',
'MM_TD_to_UBEBITDA',
'MM_TD_to_Capt',
'MM_ECE_to_TL',
'MM_TangNW_to_TA',
'size@Union Bank EBITDA',
'size@Capitalization',
'ds@Interest Expense',
'UBEBITDA',
'Capt',
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'bs@TD_to_TA_exc_TL',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']



list_sd=[]
list_sign = []
list_qua=[]
N=len(dat)
for name in bvd_cols:
    sd = np.abs(SomersD(dat['Applied Rating'], dat[name]))
    list_sign.append(np.sign(sd))
    list_sd.append(np.abs(sd))
    list_qua.append(dat[name].count()/N)

result = pd.DataFrame()
result['Factor'] = bvd_cols
result['SomersD'] = list_sd
result['Relationship'] = list_sign
result['DataQuality'] = list_qua


result.sort_values(by='SomersD',ascending=False, inplace=True)
result.reset_index(drop=True, inplace=True)

result.to_excel('__investigation_BvD_SFA.xlsx')

result2 = result.drop_duplicates(subset=['SomersD'])





#%% Compustat
dat = dfc.query('ratingYear>=2010')

int_ratings_mapping={
'AAA':1,
'AA+':2,
'AA':3,
'AA-':4, 
'A+':5,
'A':6,
'A-':7,
'BBB+':8,
'BBB':9,
'BBB-':10,
'BB+':11,
'BB':12,  
'BB-':13,
'B+':14,
'B':15,
'B-':16,
'CCC':17,
'CC':18,
'CCC-':19,
'CCC+':20,
'D':21,
'SD':21
}

dat['ExternalRating_int'] = dat['ratingSymbol']
dat['ExternalRating_int'] = dat['ExternalRating_int'].replace(int_ratings_mapping)
dat['ExternalRating_int'] = pd.to_numeric(dat['ExternalRating_int'], errors='coerce')
dat.dropna(subset=['ExternalRating_int'], inplace=True)



cs_cols=[
'CEQ',
'SEQ',
'IB',
'NI',
'EBIT',
'EBITDA',
'OIBDP',
'AT',
'RE',
'OANCF',
'LT',
'GP',
'TXT',
'LCT',
'ACT',
'RECT',
'SALE',
'REVT',
'AP',
'COGS',
'DLTT',
'CHE',
'CH',
'DP',
'INTAN',
'EMP',
'CAPX',
'DD1',
'XINT',
'INVT',
'NOPI',
'WCAP',
'ac__Net_Sales_by_Curr_Liab',
'ac__Net_Sales_by_TNW',
'ac__Net_Sales_by_Total_Assets',
'ac__Net_Sales_by_Acct_Receivable',
'ac__Net_Sales_by_Inventory',
'ac__COGS_by_Acct_Payable',
'ac__COGS',
'bs__Total_Debt_by_Total_Asset',
'bs__Total_Debt_by_TNW',
'bs__Total_Debt_by_Working_Capital',
'bs__Total_Debt_by_Capitalization_mm3_lc4',
'bs__LT_Debt_by_Adj_Capitalization',
'bs__LT_Debt_by_TNW',
'bs__Total_Liab_by_TNW',
'bs__LTD_by_LTD_and_TNW',
'bs__Total_Debt_by_Equity',
'bs__Total_Liab_by_Total_Asset',
'bs__Curr_Liab_by_Total_Asset',
'bs__Curr_Liab_by_TNW',
'bs__Curr_Liab_by_Total_Liab',
'bs__Curr_Liab_by_Capitalization',
'bs__Curr_Liab_by_Adj_Capitalization',
'bs__Total_Liab_by_Equity',
'bs__Net_Debt_by_EBITDA',
'bs__Total_Debt_by_EBITDA',
'bs__Total_Asset_by_Equity',
'bs__Total_Debt_by_TD_Equity_major6',
'bs__Total_Debt_by_TD_Equity_major9',
'bs__Net_Worth_by_Total_Asset_other4',
'cf__Total_Debt_by_NI_and_DA',
'cf__Total_Debt_by_EBITDA_mm2_lc2',
'cf__Total_Debt_by_EBIT',
'cf__Total_Debt_by_Net_Income',
'cf__Total_Debt_by_Net_Op_Cash_Flow',
'cf__Net_Debt_by_Net_Income',
'cf__Net_Debt_by_Net_Op_Cash_Flow',
'cf__LT_Debt_by_NI_and_DA',
'cf__LT_Debt_by_EBITDA',
'cf__LT_Debt_by_EBIT',
'cf__LT_Debt_by_Net_Income',
'cf__LT_Debt_by_Net_Op_Cash_Flow',
'ds__NI_and_Int_Exp_by_Int_Exp',
'ds__Net_Income_by_Int_Exp',
'ds__EBITDA_by_Int_Exp_lc5_other3',
'ds__EBIT_by_Int_Exp_major3',
'ds__Net_Op_Cash_Flow_by_Int_Exp',
'ds__Net_Income_by_Curr_Liab',
'ds__Net_Income_by_Total_Liab',
'ds__EBIT_minus_CAPEX_by_Int_Exp',
'ds__EBITDA_minus_CAPEX_by_Int_Exp',
'ds__Net_Sales_by_Int_Exp',
'ds__Total_Liab_by_Int_Exp',
'ds__Int_Exp',
'ds__FFO_Interest_Coverage_major4',
'ds__Op_Cash_by_Debt_major5',
'lq__Retained_Earn_by_Curr_Liab',
'lq__Cash_by_Total_Debt',
'lq__Cash_and_Cash_Equiv_by_Total_Debt',
'lq__Cash_and_Cash_Equiv_by_Total_Liab_mm4',
'lq__Cash_and_Cash_Equiv_by_Total_Assets',
'lq__Curr_Assets_excl_Inventory_by_Total_Debt',
'lq__Curr_Assets_excl_Inventory_by_Total_Liab',
'lq__Cash_and_Cash_Equiv_by_Curr_Liab',
'lq__Cash_and_Cash_Equiv_by_Curr_Assets',
'lq__Curr_Assets_by_Total_Liab',
'lq__Curr_Assets_minus_Liab_by_Total_Debt',
'lq__Quick_Ratio_major8',
'lq__Curr_Assets_by_Total_Assets',
'lq__Curr_Assets_minus_Curr_Liab_by_Curr_Assets',
'lq__Curr_Assets_excl_Inventory_by_Total_Assets',
'lq__Net_Sales_by_Working_Cap',
'lq__Curr_Assets_minus_Curr_Liab_by_Curr_Liab',
'lq__Current_Ratio',
'pf__ROE',
'pf__ROA',
'pf__Net_Inc_Before_Extraordinary_Items',
'pf__Net_Inc_Before_Extra_Items_by_Total_Assets',
'pf__Net_Profit_Margin',
'pf__EBIT_by_Capitalization',
'pf__EBITDA_by_Capitalization',
'pf__EBIT_by_Total_Assets',
'pf__EBITDA_by_Total_Assets',
'pf__EBITDA_by_Tangible_Assets',
'pf__EBITDA_by_Net_Sales_lc1',
'pf__Retained_Earn_by_TNW',
'pf__Net_Op_Income_by_TNW',
'pf__Net_Op_Income_by_Net_Worth',
'pf__Net_Op_Income_by_Net_Sales_mm1',
'pf__OIBD_by_Rev_major1',
'pf__Return_on_Capital_major2',
'sz__Net_Income',
'sz__Retained_Earnings',
'sz__EBIT',
'sz__EBITDA',
'sz__Net_Sales_other10',
'sz__Total_Curr_Assets',
'sz__Tangible_Assets',
'sz__Net_Op_Income',
'sz__TNW_by_Total_Assets_mm5',
'sz__TNW_by_Non_Curr_Assets',
'sz__Net_Worth_by_Non_Curr_Assets',
'sz__Tangible_Net_Worth',
'sz__Cash',
'sz__Cash_and_Cash_Equiv',
'sz__Working_Capital',
'sz__Net_Worth_other9',
'sz__Total_Assets_lc3_major7',
'sz__Total_Inventory',
'sz__Total_Debt',
'sz__Capitalization',
'sz__Capital_Expenditure',
'sz__Total_Curr_Liab',
'sz__Total_Liab',
'sz__Common_Equity',
'sz__Income_Taxes',
'sz__Revenue',
'sz__Employees',
'new__COGS_by_Net_Sales',
'new__Gross_Markup',
'new__Gross_Profit_Margin',
'new__Gross_Profit_by_Net_Sales',
'new__Gross_Margin_Ratio',
'new__Net_Sales_Revenue',
'new__Op_Cash_Flow_by_Curr_Liab',
'new__Op_Cash_Flow_by_Net_Sales',
'new__Asset_Turnover_Ratio',
'new__Depre_Amort_by_Net_Sales',
'new__Working_Capital',
'new__Funds_From_Operation']



list_sd=[]
list_sign = []
list_qua=[]
N=len(dat)
for name in cs_cols:
    sd = np.abs(SomersD(dat['ExternalRating_int'], dat[name]))
    list_sign.append(np.sign(sd))
    list_sd.append(np.abs(sd))
    list_qua.append(dat[name].count()/N)

result = pd.DataFrame()
result['Factor'] = cs_cols
result['SomersD'] = list_sd
result['Relationship'] = list_sign
result['DataQuality'] = list_qua


result.sort_values(by='SomersD',ascending=False, inplace=True)
result.reset_index(drop=True, inplace=True)

result.to_excel('__investigation_Compustat_SFA.xlsx')


#%%
cols=[
'sz__Capitalization',
'sz__Total_Assets_lc3_major7',
'ds__Total_Liab_by_Int_Exp',
'COGS',
]
list_sd=[]
list_sd_count = [];list_sd_count_LC = [];list_sd_count_MM = []

for name in cols:
    dat2 = dat.dropna(subset=['ExternalRating_int',name])
    sd = SomersD(dat2['ExternalRating_int'], dat2[name])
    list_sd.append(np.abs(sd))
    list_sd_count.append(len(dat2))
    list_sd_count_LC.append(dat2.query('portfolio=="LC"')[name].count())
    list_sd_count_MM.append(dat2.query('portfolio=="MM"')[name].count())

result = pd.DataFrame()
result['Factor'] = cols
result['SomersD'] = list_sd
result['count'] = list_sd_count
result['count_LC'] = list_sd_count_LC
result['count_MM'] = list_sd_count_MMimport os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered3.pkl')

cols=[
'Data From',
 'Application No.',
 'Borrower CIF',
 'Borrower_CIF_cleaned',
 'Borrower Name',
 'Borrower Country',
 'Borrower Type',
 'Borrower Type 2',
 'Borrower Office Code',
 'Borrower Office Name',
 'timestamp',
 'year',
 'Current Total Score',
 'Primary Evaluation',
 'Secondary Evaluation',
 'Third Evaluation',
 'Final Result of Evaluation based on Financial Substance Score',
 'Applied Rating',
 'Borrower Industry',
 'Industry Model',
 'Current Financial Data #1',
 'Current Financial Data #2',
 'Current Financial Data #3',
 'Current Financial Data #4',
 'Current Financial Data #5',
 'Current Financial Data #6',
 'Current Financial Data #7',
 'Current Financial Data #8',
 'Current Financial Data #9',
 'Current Financial Data #10',
 "External Rating (Moody's)",
 'External Rating (R&I)',
 'External Rating (S&P)'
 ]

dat=dat[cols]


#%% Drop Borrowers who have unique "CIF" but multiple(>=3) "Names"
tmp = dat.groupby('Borrower_CIF_cleaned')['Borrower Name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts(normalize=True)
'''
1    0.657148
2    0.255707
3    0.063053
4    0.017100
5    0.005476
6    0.001011
7    0.000421
9    0.000042
8    0.000042
Name: Num_CusName, dtype: float64

'''

list_toremove = tmp[tmp.Num_CusName>=3].index.tolist()
mask = dat.Borrower_CIF_cleaned.isin(list_toremove)
dat = dat[~mask].reset_index(drop=True)




#%% tag Geo info
dat['Geo'] = 'Asia'
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(gcar_Americas_office_code), 'Americas')
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(gcar_EMEA_office_code), 'EMEA')


dat['BTMU Rating'] = dat['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat['BTMU Rating'].value_counts().sort_index() 




#%% convert to numeric value
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

gcar_converter(dat, curr, inplace=True)





df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')


bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


df_bvd['Fiscal year'].value_counts().sort_index()


dat['year-1'] = dat['year']-1






#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% relink: Year (N-1)'s BvD links to Year N's GCARS data
df_merged = pd.merge(dat, df_bvd, left_on=['Borrower_CIF_cleaned','year-1'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_merged['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_merged['BvD_CIF']]
df_merged['inBvD'].value_counts()


#%% Drop Browers who have unique "BvD_CIF" but multiple(>=3) "Names"
tmp = df_merged.groupby('BvD_CIF')['BvD_Company name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts().sort_index()
'''
0    2495
1    5646
2     113
3       7
Name: Num_CusName, dtype: int64

'''

list_toremove = tmp[tmp.Num_CusName>=3].index.tolist()
mask = df_merged.BvD_CIF.isin(list_toremove)
df_merged = df_merged[~mask].reset_index(drop=True)



import pickle
lmdb = 0.3
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']




dat = df_merged.copy()



cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']



#%% build CnI ratios
dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
# for invalid_neg
dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']


# MM new transformation
dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb



#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios
dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']



factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))



#%% build current tokyo model factors:
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']
# fill inf
for factor in curr:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))


# rename current factor

for name in curr:
    dat['Other_'+name] = np.nan
    dat['SP_'+name] = np.nan

    dat['Other_'+name] = dat['Other_'+name].mask(dat['Borrower Type']=="Corporation (Other Company)", dat[name])
    dat['SP_'+name] = dat['SP_'+name].mask(dat['Borrower Type']!="Corporation (Other Company)", dat[name])

dat.drop(columns=curr, inplace=True)


#%% label geo info
dat = pd.concat([dat, pd.get_dummies(dat['Geo'], prefix='Geo')], axis=1)

#%% label tokyo model info
dat = pd.concat([dat, pd.get_dummies(dat['Borrower Type'], prefix='Tokyo')], axis=1)




#%% add portfolio info
dat['portfolio'] = 'unknown'
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']>1e9, 'LC')
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']<=1e9, 'MM')

dat['portfolio'].value_counts()

















#%%
#####################################################################
dat = dat.query('year>=2010')
dat2 = dat.query('inBvD=="Yes"')
dat2.dropna(subset=['Applied Rating'], inplace=True)

bvd_cols=[
'BvD_Fixed assets',
'BvD_Intangible fixed assets',
'BvD_Tangible fixed assets',
'BvD_Other fixed assets',
'BvD_Current assets',
'BvD_Stock',
'BvD_Debtors',
'BvD_Other current assets',
'BvD_Cash & cash equivalent',
'BvD_Total assets',
'BvD_Shareholders funds',
'BvD_Capital',
'BvD_Other shareholders funds',
'BvD_Non-current liabilities',
'BvD_Long term debt',
'BvD_Other non-current liabilities',
'BvD_Provisions (Liabilities & Equity)',
'BvD_Current liabilities',
'BvD_Loans (Liabilities & Equity)',
'BvD_Creditors',
'BvD_Other current liabilities',
'BvD_Total shareh. funds & liab.',
'BvD_Working capital',
'BvD_Net current assets',
'BvD_Enterprise value',
'BvD_Number of employees',
'BvD_Operating revenue (Turnover)',
'BvD_Sales',
'BvD_Costs of goods sold',
'BvD_Gross profit',
'BvD_Other operating expenses',
'BvD_Operating P/L [=EBIT]',
'BvD_Financial P/L',
'BvD_Financial revenue',
'BvD_Financial expenses',
'BvD_P/L before tax',
'BvD_Taxation',
'BvD_P/L after tax',
'BvD_Extr. and other P/L',
'BvD_Extr. and other revenue',
'BvD_Extr. and other expenses',
'BvD_P/L for period [=Net income] ',
'BvD_Export revenue',
'BvD_Material costs',
'BvD_Costs of employees',
'BvD_Depreciation & Amortization',
'BvD_Other operating items',
'BvD_Interest paid',
'BvD_Research & Development expenses',
'BvD_Cash flow',
'BvD_Added value',
'BvD_EBITDA',
'BvD_Proxy_Total_Assets',
'BvD_Proxy_Interest_expense',
'BvD_Proxy_Ending_Cash_Equiv',
'BvD_Proxy_UBEBITDA',
'BvD_Proxy_Net_Sales',
'BvD_Proxy_Total_Debt',
'BvD_Proxy_Capitalization',
'BvD_Proxy_Net_Op_Profit',
'BvD_Proxy_Total_Liabilities',
'BvD_Proxy_Tangible_Net_Worth',
'BvD_Proxy_Total_Assets_inUSD',
'BvD_Proxy_Interest_expense_inUSD',
'BvD_Proxy_Ending_Cash_Equiv_inUSD',
'BvD_Proxy_UBEBITDA_inUSD',
'BvD_Proxy_Net_Sales_inUSD',
'BvD_Proxy_Total_Debt_inUSD',
'BvD_Proxy_Capitalization_inUSD',
'BvD_Proxy_Net_Op_Profit_inUSD',
'BvD_Proxy_Total_Liabilities_inUSD',
'BvD_Proxy_Tangible_Net_Worth_inUSD',
'LC_UBEBITDA_to_NS',
'LC_TD_to_UBEBITDA',
'LC_Total Assets',
'LC_TD_to_Capt',
'LC_UBEBITDA_to_IE',
'MM_NOP_to_NS',
'MM_TD_to_UBEBITDA',
'MM_TD_to_Capt',
'MM_ECE_to_TL',
'MM_TangNW_to_TA',
'size@Union Bank EBITDA',
'size@Capitalization',
'ds@Interest Expense',
'UBEBITDA',
'Capt',
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'bs@TD_to_TA_exc_TL',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']



list_sd=[]
list_sign = []
list_qua=[]
N=len(dat)
for name in bvd_cols:
    sd = np.abs(SomersD(dat['Applied Rating'], dat[name]))
    list_sign.append(np.sign(sd))
    list_sd.append(np.abs(sd))
    list_qua.append(dat[name].count()/N)

result = pd.DataFrame()
result['Factor'] = bvd_cols
result['SomersD'] = list_sd
result['Relationship'] = list_sign
result['DataQuality'] = list_qua


result.sort_values(by='SomersD',ascending=False, inplace=True)
result.reset_index(drop=True, inplace=True)

result.to_excel('__investigation_BvD_SFA.xlsx')

result2 = result.drop_duplicates(subset=['SomersD'])



#%%
from PDScorecardTool.Process import SomersD

cols=[
'BvD_Capital',
'BvD_Total assets',
'ds@TL_to_IE',
'new@COGSRatio',]

int_ratings_mapping={
'AAA':1,
'AA+':2,
'AA':3,
'AA-':4, 
'A+':5,
'A':6,
'A-':7,
'BBB+':8,
'BBB':9,
'BBB-':10,
'BB+':11,
'BB':12,  
'BB-':13,
'B+':14,
'B':15,
'B-':16,
'CCC+~':17,
}

dat2['ExternalRating_int'] = dat2['External Rating (S&P)']
dat2['ExternalRating_int'] = dat2['ExternalRating_int'].replace(int_ratings_mapping)
dat2['ExternalRating_int'] = pd.to_numeric(dat2['ExternalRating_int'], errors='coerce')
dat2['ExternalRating_int'].value_counts()


list_sd=[]
list_sd2=[]
list_sd_count = [];list_sd_count_LC = [];list_sd_count_MM = []
list_sd2_count = [];list_sd2_count_LC = [];list_sd2_count_MM = []


for name in cols:
    sd = np.abs(SomersD(dat2['Applied Rating'], dat2[name]))
    list_sd.append(np.abs(sd))
    list_sd_count.append(dat2[name].count())
    list_sd_count_LC.append(dat2.query('portfolio=="LC"')[name].count())
    list_sd_count_MM.append(dat2.query('portfolio=="MM"')[name].count())

    dat3 = dat2.dropna(subset=['ExternalRating_int',name])
    list_sd2.append(np.abs(SomersD(dat3['ExternalRating_int'], dat3[name]))) 
    list_sd2_count.append(dat3[name].count())
    list_sd2_count_LC.append(dat3.query('portfolio=="LC"')[name].count())
    list_sd2_count_MM.append(dat3.query('portfolio=="MM"')[name].count())



result = pd.DataFrame()
result['Factor'] = cols
result['SomersD'] = list_sd
result['count'] = list_sd_count
result['count_LC'] = list_sd_count_LC
result['count_MM'] = list_sd_count_MM
result['SomersD2'] = list_sd2
result['count2'] = list_sd2_count
result['count2_LC'] = list_sd2_count_LC
result['count2_MM'] = list_sd2_count_MM



#%%

df_all = dat.query('inBvD=="Yes"').dropna(subset=['External Rating (S&P)'])


df_lc = df_all.query('portfolio=="LC"')
df_mm = df_all.query('portfolio=="MM"')

df_all['Borrower_CIF_cleaned'].nunique()  
# 887
df_lc['Borrower_CIF_cleaned'].nunique()  
# 496
df_mm['Borrower_CIF_cleaned'].nunique()  
# 240

dat['Borrower_CIF_cleaned'].nunique()  
# 21456


#%%
dat['Borrower Name'].nunique()  
 26257

'''
GCARS data have about 21,456 unique customers (by CIF).
Among them, 887 unique customers have both ‘BvD financials’ and ‘External Rating S&P’.
The pct is 887/21456 = 4.1%.

In other words, only 4.1% of current 2 Tokyo models’ customers have BvD financials and external rating. 
Considering the data quality of BvD financial and constructed ratios, the pct will be lower than 4% or 
even lower than 3%, which makes it weak in representativeness for current portfolio.

And I also found the Compustat data can cover about 2.3% of GCARS customer names(which is not far less than 3-4% ). 
Note that, Compustat data is mainly for US/CAN companies.



'''
# -*- coding: utf-8 -*-
"""
Created on Tue Sep  8 10:40:51 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')

#%% CBM
#df_cb = pd.read_excel(r'C:\Users\ub71894\Documents\Data\CreditBenchmark Ratings\60001325_31AUG20_07SEP20_ClientHistory_Tokyo_Universe.xlsx')
#df_cb.to_pickle(r'C:\Users\ub71894\Documents\Data\CreditBenchmark Ratings\60001325_31AUG20_07SEP20_ClientHistory_Tokyo_Universe.pkl')
df_cb = pd.read_pickle(r'C:\Users\ub71894\Documents\Data\CreditBenchmark Ratings\60001325_31AUG20_07SEP20_ClientHistory_Tokyo_Universe.pkl')
df_cb['CIF_cleaned'] = gcar_cif_cleaner(df_cb, col='CIF_cleaned')
df_cb['timestamp'] = pd.to_datetime(df_cb[ 'CB_Effective_Date'])
df_cb['year'] = [x.year for x in df_cb.timestamp]
df_cb.drop_duplicates(subset=['CIF_cleaned','year'], inplace=True)

dict_cb={
'aaa':1,     
'aa+':2,    
'aa':3,      
'aa-':4,     
'a+':5,      
'a':6,       
'a-':7,      
'bbb+':8,    
'bbb':9,     
'bbb-':10,    
'bb+':11,     
'bb':12,      
'bb-':13,     
'b+':14,      
'b':15,       
'b-':16,      
'ccc+':17,    
'ccc':18,     
'ccc-':19,    
'cc':20,      
}
df_cb['rating_in_notch'] = df_cb['CB_Consensus'].replace(dict_cb)



#%% LC
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat = pd.merge(df, df_cb, left_on=['Borrower_CIF_cleaned','year'], right_on=['CIF_cleaned','year'], how='inner')


#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()
'''
0    620
1    158
Name: Gua_override, dtype: int64
'''

mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

#dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
#dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])
#
#dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,102:82})
#dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['LC_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['LC_PD'] = list_PD
    dat['LC_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_LC_norm = get_proxy_rating(dat_LC_norm, model_LC)
# 507


print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['LC_PDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Primary Evaluation']))
#print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['PrimaryEvaluationPDRR']))
#print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Applied Rating PDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Applied Rating']))
0.3287446699844405
0.3826575189423779
0.5671849203615502


print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['LC_PDRR']))
print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['Primary Evaluation']))
#print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['PrimaryEvaluationPDRR']))
#print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['Applied Rating PDRR']))
print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['Applied Rating']))

0.26526590230233105
0.32259259716681693
0.4549746979665993

dat_LC_norm['Borrower Type'].value_counts()

'''
Corporation (Major Company)    399
Corporation (Other Company)    108
Name: Borrower Type, dtype: int64
'''


#  2020/12/28  to response Hera
cols=[
'Borrower Name','Borrower_CIF_cleaned','year','Primary Evaluation','Applied Rating',
'CB_Consensus','rating_in_notch','LC_PDRR']

df_out = dat_LC_norm[cols].copy()
print(SomersD(df_out['rating_in_notch'], df_out['LC_PDRR']))
print(SomersD(df_out['rating_in_notch'], df_out['Primary Evaluation']))

df_out.to_excel('LC_portfolio.xlsx')



#%% MM
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')

dat = pd.merge(df, df_cb, left_on=['Borrower_CIF_cleaned','year'], right_on=['CIF_cleaned','year'], how='inner')


#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()
'''
0    540
1    202
Name: Gua_override, dtype: int64
'''

mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

#dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
#dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])
#
#dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,102:82})
#dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['MM_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['MM_PD'] = list_PD
    dat['MM_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_MM_norm = get_proxy_rating(dat_MM_norm, model_MM)
# 486


print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['MM_PDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Primary Evaluation']))
#print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Applied Rating PDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Applied Rating']))
0.22849042962060961
0.1673844624813649
0.3057384223143955


print(SomersD(dat_MM_norm['Risk_Entity_PD_Median_Proxy'], dat_MM_norm['MM_PDRR']))
print(SomersD(dat_MM_norm['Risk_Entity_PD_Median_Proxy'], dat_MM_norm['Primary Evaluation']))
#print(SomersD(dat_MM_norm['Risk_Entity_PD_Median_Proxy'], dat_MM_norm['Applied Rating PDRR']))
print(SomersD(dat_MM_norm['Risk_Entity_PD_Median_Proxy'], dat_MM_norm['Applied Rating']))

0.19218383828165833
0.16983275018776678
0.3114662890091548


dat_MM_norm['Borrower Type'].value_counts()
'''
Corporation (Other Company)    430
Corporation (Major Company)     56
Name: Borrower Type, dtype: int64

'''

#  2020/12/28  to response Hera
cols=[
'Borrower Name','Borrower_CIF_cleaned','year','Primary Evaluation','Applied Rating',
'CB_Consensus','rating_in_notch','MM_PDRR']

df_out = dat_MM_norm[cols].copy()
print(SomersD(df_out['rating_in_notch'], df_out['MM_PDRR']))
print(SomersD(df_out['rating_in_notch'], df_out['Primary Evaluation']))

df_out.to_excel('MM_portfolio.xlsx')



#%%
import seaborn as sns
import matplotlib.pyplot as plt

df_cb['PD_Contribution_Count'] = df_cb['PD_Contribution_Count'].replace({'MIN':4})
dat_LC_norm['PD_Contribution_Count'] = dat_LC_norm['PD_Contribution_Count'].replace({'MIN':4})
dat_MM_norm['PD_Contribution_Count'] = dat_MM_norm['PD_Contribution_Count'].replace({'MIN':4})

#  RLA plot
fig, ax = plt.subplots(1, 1, figsize=(8,6))
ax = sns.countplot(data=df_cb, x='PD_Contribution_Count', palette="crest")
values=df_cb['PD_Contribution_Count'].value_counts().sort_index().values
for i, p in enumerate(ax.patches):
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha="center")

ax.set_xticklabels([int(float(x.get_text())) for x in ax.get_xticklabels()])
ax.set_xlabel("Count of Contributors")


#  LC plot
fig, ax = plt.subplots(1, 1, figsize=(8,6))
ax = sns.countplot(data=dat_LC_norm, x='PD_Contribution_Count', palette="crest")
values=dat_LC_norm['PD_Contribution_Count'].value_counts().sort_index().values
for i, p in enumerate(ax.patches):
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha="center")

ax.set_xticklabels([int(float(x.get_text())) for x in ax.get_xticklabels()])
ax.set_xlabel("Count of Contributors")
plt.savefig('CBM_LC.png')


#  MM plot
fig, ax = plt.subplots(1, 1, figsize=(8,6))
ax = sns.countplot(data=dat_MM_norm, x='PD_Contribution_Count', palette="crest")
values=dat_MM_norm['PD_Contribution_Count'].value_counts().sort_index().values
for i, p in enumerate(ax.patches):
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha="center")

ax.set_xticklabels([int(float(x.get_text())) for x in ax.get_xticklabels()])
ax.set_xlabel("Count of Contributors")
plt.savefig('CBM_MM.png')

# -*- coding: utf-8 -*-
"""
Created on Mon Oct  5 12:15:24 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
import seaborn as sns
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')

df_full = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_BvD_relink.pkl')
df_3yrmm = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')



#%% full data
dat = df_full.copy()

bvd_cols=[
 'BvD_Current assets',
 'BvD_Cash & cash equivalent',
 'BvD_Total assets',
 'BvD_Capital',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Sales',
 'BvD_Gross profit',
 'BvD_Operating P/L [=EBIT]',
 'BvD_P/L for period [=Net income] ',
 'BvD_EBITDA']


# change to USD
for name in bvd_cols:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']




dat.dropna(subset=bvd_cols, how='all', inplace=True)
dat1 = dat[dat['Borrower Type']=='Corporation (Other Company)'].sample(2000)
#dat1 = dat[dat['Borrower Type']=='Corporation (Major Company)'].sample(2000)
dat1[name].describe(percentiles=[0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99])



for name in bvd_cols:
    dat2 = dat1[dat1[name]<dat1[name].quantile(.95)]
    sns.displot(data=dat2, x=name, hue='Borrower Type', kind='kde')




dat2 = dat1[dat1[name]<1e7]
#sns.displot(data=dat2, x=name, hue='Borrower Type', kde=True)


sns.histplot(data=dat2, x=name, hue='Borrower Type', bins=20)



name =  'BvD_Enterprise value'
# -*- coding: utf-8 -*-
"""
Created on Tue Nov  3 13:25:00 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import MAUG_mapping, NAICS_mapping
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
from PDScorecardTool.Process import SomersD
from PDScorecardTool.Process import func_sd, func_rla, func_ovd, google_color
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


df_ci_lc1 = pd.read_pickle(r'..\data\CnI_data\dat_lc_by1000_last.pkl.xz')
df_ci_mm1 = pd.read_pickle(r'..\data\CnI_data\dat_mm_by1000_last.pkl.xz')

df_ci_lc2 = pd.read_csv(r'..\data\CnI_data\dat_LC_201819.csv')
df_ci_mm2 = pd.read_csv(r'..\data\CnI_data\dat_MM_201819.csv')

df_lc = pd.concat([df_ci_lc1,df_ci_lc2], axis=0)
df_mm = pd.concat([df_ci_mm1,df_ci_mm2], axis=0)



df_lc = MAUG_mapping(df_lc)
df_lc = NAICS_mapping(df_lc)
df_lc['timestamp'] = pd.to_datetime(df_lc['archive_date'])

df_lc['Industry_by_MAUG'].value_counts()
data_lc = df_lc[df_lc['Industry_by_MAUG']=="Power & Utilities"]


df_mm = MAUG_mapping(df_mm)
df_mm = NAICS_mapping(df_mm)
df_mm['timestamp'] = pd.to_datetime(df_mm['archive_date'])

df_mm['Industry_by_MAUG'].value_counts()
data_mm = df_mm[df_mm['Industry_by_MAUG']=="Power & Utilities"]


#%%
df = pd.read_pickle(r'..\data\GCARS 2008-2019_matches3.pkl')
aa = df['Borrower Industry'].unique().tolist()


mask = (df['Borrower Industry']=='Electric Utility Services')|(df['Borrower Industry']=='Gas Utility Services')|(df['Borrower Industry']=='Heating Utilities')
data = df[mask].query('year==2018')


data['Borrower Industry'].value_counts()
'''

Electric Utility Services    319
Gas Utility Services          81
Heating Utilities              2
Name: Borrower Industry, dtype: int64
'''
data['Borrower Industry'].value_counts() / len(df.query('year==2018'))
'''
Electric Utility Services    0.026156
Gas Utility Services         0.006642
Heating Utilities            0.000164
Name: Borrower Industry, dtype: float64
'''

data['Borrower Type'].value_counts()
'''
Out[47]: 
Corporation (Other Company)    278
Corporation (Major Company)    124
Name: Borrower Type, dtype: int64
'''


data.to_excel(r'NPPU_GCARS_2018.xlsx')


#%% Oilg Gas

cols=[
 'Oil and Gas Extraction',
  'Liquid Propane Gas Distributor',
  'Gasoline Stations',
  'Natural Gas Stations',
  'Plumbing, Gas Facilities, Sewage Facilities Construction']

mask = df['Borrower Industry'].isin(cols)
data = df[mask].query('year==2018')


data['Borrower Industry'].value_counts()
'''
Out[52]: 
Oil and Gas Extraction                                      165
Gasoline Stations                                             8
Liquid Propane Gas Distributor                                8
Plumbing, Gas Facilities, Sewage Facilities Construction      5
Name: Borrower Industry, dtype: int64
'''


data['Borrower Industry'].value_counts() / len(df.query('year==2018'))
'''
Oil and Gas Extraction                                      0.013529
Gasoline Stations                                           0.000656
Liquid Propane Gas Distributor                              0.000656
Plumbing, Gas Facilities, Sewage Facilities Construction    0.000410
Name: Borrower Industry, dtype: float64
'''


data['Borrower Type'].value_counts() 
'''
Corporation (Other Company)    132
Corporation (Major Company)     54
Name: Borrower Type, dtype: int64
'''



data.to_excel(r'O&G_GCARS_2018.xlsx')
#%% Master Default
import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


md = pd.read_excel(r'C:\Users\ub71894\Documents\Data\MasterDefault\Master_Def_202006_clean.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered3.pkl')



df_md = md[['L_CIF_NUMBER', 'L_OBLIGOR_NAME', 'L_DATE_OF_DEFAULT', 'Scorecard']] 
df_md.dropna(subset=['L_CIF_NUMBER'], inplace=True)
#df_md['CIF'] = [str(int(x))[:8] for x in df_md.L_CIF_NUMBER]
#df_md['CIF'] = [str(int(x))[-8:] for x in df_md.L_CIF_NUMBER]
df_md['CIF'] = [str(int(x)) for x in df_md.L_CIF_NUMBER]
df_md.drop_duplicates(subset=['CIF'], inplace=True)


cols=['Borrower CIF',
 'Borrower Name',
 'Borrower Industry',
 'Borrower Country',
 'Borrower Type',]
df = dat[cols]
df.drop_duplicates(subset=['Borrower CIF'], inplace=True)
df['CIF'] = gcar_cif_cleaner(df, col='Borrower CIF')


new = pd.merge(df, df_md, on='CIF', how='inner')




#%% GCARS Default
import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')

dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered3.pkl')
mask = dat['Applied Rating'].isin([83, 90, 101, 102])

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(mask, 1)


dat['def_flag'].sum()
# 415
dat.query('def_flag==1')['Borrower Type'].value_counts()

'''
Corporation (Other Company)    316
Corporation (Major Company)     99
Name: Borrower Type, dtype: int64
'''
dat.query('def_flag==0')['Borrower Type'].value_counts()
'''
Corporation (Other Company)    95465
Corporation (Major Company)    24504
Name: Borrower Type, dtype: int64
'''



dat.query('def_flag==1')['Borrower CIF'].count()
# 415
dat.query('def_flag==1')['Borrower CIF'].nunique()
# 261

dat.query('def_flag==1')['year'].value_counts().sort_index()
'''
2008     1
2009    16
2010    79
2011    41
2012    35
2013    30
2014    25
2015    24
2016    50
2017    38
2018    34
2019    42
Name: year, dtype: int64
'''


df = dat.query('def_flag==1').drop_duplicates(subset=['Borrower CIF'])
df.query('def_flag==1')['Borrower Type'].value_counts()

'''
Corporation (Other Company)    197
Corporation (Major Company)     64
Name: Borrower Type, dtype: int64
'''




#%%  add BvD
df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


dat['year-1'] = dat['year']-1
len(set(dat['Borrower CIF_cleaned'].unique()))
# 23849

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(dat['Borrower CIF_cleaned'].unique())&set(df_bvd['CIF'].unique()))
# 10821



#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% relink: Year (N-1)'s BvD links to Year N's GCARS data
df_merged = pd.merge(dat, df_bvd, left_on=['Borrower CIF_cleaned','year-1'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_merged['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_merged['BvD_CIF']]
df_merged['inBvD'].value_counts()

'''
No     69458
Yes    54295
Name: inBvD, dtype: int64
'''



#%% add portfolio info
df_merged['portfolio'] = 'unknown'
df_merged['portfolio'] = df_merged['portfolio'].mask(df_merged['BvD_Proxy_Net_Sales_inUSD']>1e9, 'LC')
df_merged['portfolio'] = df_merged['portfolio'].mask(df_merged['BvD_Proxy_Net_Sales_inUSD']<=1e9, 'MM')

df_merged['portfolio'].value_counts()
'''
unknown    77417
MM         40106
LC          6230
Name: portfolio, dtype: int64

'''

#%% split
dat_LC = df_merged.query('portfolio=="LC"')
dat_MM = df_merged.query('portfolio=="MM"')


dat_LC['def_flag'].sum()
21
len(dat_LC)
6230

dat_LC.query('def_flag==1')['year'].value_counts().sort_index()
'''
2010    2
2011    1
2012    1
2014    2
2015    2
2016    4
2017    3
2018    3
2019    3
Name: year, dtype: int64

'''
dat_LC['year'].value_counts().sort_index()
'''
2008      4
2009     79
2010    459
2011    592
2012    606
2013    635
2014    699
2015    719
2016    722
2017    592
2018    626
2019    497
Name: year, dtype: int64
'''




dat_MM['def_flag'].sum()
101
len(dat_MM)
40106

dat_MM.query('def_flag==1')['year'].value_counts().sort_index()
'''
2010    10
2011     6
2012     9
2013    11
2014     9
2015     5
2016    23
2017    10
2018     9
2019     9
Name: year, dtype: int64
'''


dat_MM['year'].value_counts().sort_index()
'''
2008       1
2009     484
2010    2340
2011    2626
2012    3548
2013    3787
2014    4385
2015    5178
2016    4820
2017    5037
2018    4665
2019    3235
Name: year, dtype: int64
'''

5037+4665+3235# -*- coding: utf-8 -*-
"""
Created on Tue Nov  3 13:25:00 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')

# 2020 data
list_file=[
r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCAR_BR_Approved\br download_2020Q1.tsv',
r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCAR_BR_Approved\br download_20200405.tsv',
r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCAR_BR_Approved\br download_202006.tsv']
dt0=pd.DataFrame()
for e in list_file:
    tmp=pd.read_csv(e, sep='\t', header=0,encoding = "ISO-8859-1")
    dt0=dt0.append(tmp,ignore_index=True)

dt0['timestamp'] = pd.to_datetime(dt0['Approval/Agreed Date'])
df = dt0.dropna(subset=['timestamp'])



#%% filter data
# model
mask = df['Borrower Type'].isin(['Corporation (Major Company)', 'Corporation (Other Company)'])
df = df[mask]

# Source
df_BL = df.loc[(df['Data From'].isin(['BL']))]
df_BL = df_BL[df_BL['Ringi/Sairyo']=='Sairyo'] 
df_CD = df.loc[(df['Data From'].isin(['CD']))]

df = pd.concat([df_CD, df_BL])
df = df.sort_values(by=['Data From'])
df = df.drop_duplicates(subset=['Application No.'], keep='last')

# Drop NA that exists in ratings
df = df.dropna(subset=['Applied Rating'], how='any')

# Drop duplicates
df['Borrower_CIF_cleaned'] = gcar_cif_cleaner(df, col='Borrower CIF')
df.drop_duplicates(subset=['Borrower_CIF_cleaned', 'timestamp'], inplace=True)



# tag year info
df['year'] = [int(x.year) for x in df['timestamp'] ]
df['year'].value_counts().sort_index() 
'''
2020    7447
Name: year, dtype: int64
'''




dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCARS 2008-2019_matches4.pkl')




cols=[
 'Borrower CIF',
 'Borrower_CIF_cleaned',
 'Borrower Name',
 'Borrower Country',
 'Borrower Office Code',
 'timestamp',
 'year',
 'Primary Evaluation',
 'Secondary Evaluation',
 'Third Evaluation',
 'Final Result of Evaluation based on Financial Substance Score',
 'Applied Rating',
 'Borrower Industry',
 'Borrower Type',
  'External Rating (S&P)',
  'Current Financial Data #1',
 'Current Financial Data #2',
 'Current Financial Data #3',
 'Current Financial Data #4',
 'Current Financial Data #5',
 'Current Financial Data #6',
 'Current Financial Data #7',
 'Current Financial Data #8',
 'Current Financial Data #9',
 'Current Financial Data #10',

]

# combine
df = pd.concat([dat[cols], df[cols]], axis=0)




# tag Geo info
df['Geo'] = 'Asia'
df['Geo'] = df['Geo'].mask(df['Borrower Office Code'].isin(gcar_Americas_office_code), 'Americas')
df['Geo'] = df['Geo'].mask(df['Borrower Office Code'].isin(gcar_EMEA_office_code), 'EMEA')
df['Geo'].value_counts()

'''
Asia        74148
EMEA        27010
Americas    26673
Name: Geo, dtype: int64
'''

# tag Guarantor info
df['Gua_override'] = 1
mask = df['Secondary Evaluation']==df['Third Evaluation']
df.loc[mask,'Gua_override']=0
df['Gua_override'].value_counts()
'''
0    75948
1    51883
Name: Gua_override, dtype: int64
'''

# replace Final Evaluation as Secondary for all Guarantor impacted records
df['BTMU_Rating'] = df['Applied Rating'].mask(df['Gua_override']==1, df['Secondary Evaluation'])
# get string version
df['BTMU_Rating_str'] = df['BTMU_Rating'].transform(lambda x: str(x)[:-1]+'-'+str(x)[-1])
#df['BTMU_Rating_str'].value_counts().sort_index() 

dff = df.query('year>=2017')
dff.to_pickle(r'..\data\__investigation_GCARS_stability_2017-2020.pkl')



import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCARS 2008-2019_matches.pkl')



df = dat[dat['Borrower Type'] == 'Corporation (Other Company)']

df_1 = df.query('Match==1')
df_0 = df.query('Match==0')

cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Match'
]

df_1.year.value_counts(normalize=True).sort_index()
df_0.year.value_counts(normalize=True).sort_index()


df.groupby('year')['Match'].mean
'''
year
2008    0.084746
2009    0.043767
2010    0.049397
2011    0.042160
2012    0.062644
2013    0.342165
2014    0.362398
2015    0.400951
2016    0.438614
2017    0.636697
2018    0.597648
2019    0.585712
Name: Match, dtype: float64
'''








def check_diff(col):
    print(f'Non-machted sample, {col}: # unique = {df_0[col].nunique()}')
    print(f'    Machted sample, {col}: # unique = {df_1[col].nunique()}')


def diff_ratio(col):

    return (df_0[col].nunique()+1)/(1+df_1[col].nunique())

N = len(df)
list_col=[]

for i,col in enumerate(list(df)):
    rr = df[col].count()/N
    if rr>=0.25:
        list_col.append(col)



list_ratio=[]
for col in list_col:
    list_ratio.append(diff_ratio(col))


result = pd.DataFrame()
result['ratio'] = list_ratio
result['field']= list_col




check_diff('(MUB) Primary Evaluation from RA')

check_diff('Modification Corporate Bond')


col = 'Format'
check_diff(col)

df[col].count()


col = 'Borrower Situation'
check_diff(col)

df[col].count()



list_col=[
 '(MUB) (Primary Evaluation from RA) Archive ID',
 '(MUB) (Primary Evaluation from RA) Captured Time (PST)',
 '(MUB) (Primary Evaluation from RA) Current User',
 '(MUB) (Primary Evaluation from RA) Customer Name',
 '(MUB) (Primary Evaluation from RA) Imported Time (JST)',
 '(MUB) (Primary Evaluation from RA) MUFG Bank Primary Rating',
 '(MUB) (Primary Evaluation from RA) RCIF',
 '(MUB) (Primary Evaluation from RA) Statement Date',
 '(MUB) Archive ID',
 '(MUB) Borrower Grade',
 '(MUB) MUFG Bank Rating',
 '(MUB) PDRR',
 '(MUB) Primary Evaluation from RA',
 '(MUB) RCIF']


N = len(df)
for i,col in enumerate(list_col):
    print((df[col].count()/N))



df['(MUB) (Primary Evaluation from RA) BTMU Primary Rating'].count()



df_0[col].value_counts()
Out[68]: 
MUFG Bank    51517
Name: Format, dtype: int64

df_1[col].value_counts()
Out[69]: 
MUFG Bank    25983
MUB            539
Name: Format, dtype: int64# -*- coding: utf-8 -*-
"""
Created on Tue Sep  8 10:40:51 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')




#%% LC
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')


#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1038
1     379
Name: Gua_override, dtype: int64
'''

dat = dat.query('Gua_override==0').reset_index(drop=True)


#%% locate outliers
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])


#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['LC_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['LC_PD'] = list_PD
    dat['LC_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_LC_norm = get_proxy_rating(dat_LC_norm, model_LC)

#%%
df = dat_LC_norm[dat_LC_norm['Borrower Type']=='Corporation (Other Company)'].copy()
df.dropna(subset=['Applied Rating','Primary Evaluation'], how='any', inplace=True)

df['diff'] = np.abs(df['LC_PDRR'] -  df['Applied Rating PDRR'])
df = df.drop_duplicates(subset=['Borrower_CIF_cleaned', 'diff'])
df['diff'].value_counts().sort_index()


df2 = df.query('diff<=1').sample(5).sort_values('Borrower Name')
df3 = df.query('diff>4').sample(5).sort_values('Borrower Name')
df_LC = pd.concat([df2,df3], axis=0)

df_LC['Borrower Name'].nunique()
# 10



#%% MM
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')


#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()



dat = dat.query('Gua_override==0').reset_index(drop=True)


#%% locate outliers
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['MM_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['MM_PD'] = list_PD
    dat['MM_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_MM_norm = get_proxy_rating(dat_MM_norm, model_MM)



df = dat_MM_norm[dat_MM_norm['Borrower Type']=='Corporation (Other Company)'].copy()
df.dropna(subset=['Applied Rating','Primary Evaluation'], how='any', inplace=True)

df['diff'] = np.abs(df['MM_PDRR'] -  df['Applied Rating PDRR'])
df = df.drop_duplicates(subset=['Borrower_CIF_cleaned', 'diff'])
df['diff'].value_counts().sort_index()


df4 = df.query('diff<=1').sample(5).sort_values('Borrower Name')
df5 = df.query('diff>7').sample(5).sort_values('Borrower Name')
df_MM = pd.concat([df4,df5], axis=0)

df_MM['Borrower Name'].nunique()
# 9


writer = pd.ExcelWriter(r'SFAMFA\sample_forGCARS_reports_2017-2019.xlsx')

df_LC.to_excel(writer, 'LC model')
df_MM.to_excel(writer, 'MM model')
writer.save()# -*- coding: utf-8 -*-
"""
Created on Tue Sep  8 10:40:51 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')



#%% SP
df_sp_raw = pd.read_csv(r'C:\Users\ub71894\Documents\Data\Compustat Pull 082520\S&P Rating Mapping Data 2019-09-24.csv')
data_gcar1 = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\yiming\name matching.xlsx')
data_gcar2 = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\yiming\name matching 2.xlsx')
data_gcar = pd.concat([data_gcar1 ,data_gcar2], axis=0)


data_gcar = data_gcar.query('correct==1')
data_gcar['CONM'] = data_gcar['diffmatchname_90'].mask(
    pd.isnull(data_gcar['diffmatchname_90']),
    data_gcar['diffmatchname_80'])

df_mapping = data_gcar[[ 'name_tokyo', 'CONM']].drop_duplicates(subset=['name_tokyo']).reset_index(drop=True)


df_sp = pd.merge(df_mapping, df_sp_raw, on='CONM', how='left')
df_sp.sort_values(by='ratingDate', inplace=True)
df_sp = df_sp.query('ratingSymbol!="NR"')
dict_sp={
'AAA':1,
'AA+':2,
'AA':3,
'AA-':4,
'A+':5,
'A':6,
'A-':7,
'BBB+':8, 
'BBB':9,     
'BBB-':10,        
'BB+':11,   
'BB':12,      
'BB-':13,     
'B+':14,   
'B':15,   
'B-':16,
'CCC':17, 
'D':18,
'SD':18,
}  
    
df_sp['rating_in_notch'] = df_sp['ratingSymbol'].replace(dict_sp)
df_sp = df_sp[['name_tokyo','CONM','ratingDate','ratingSymbol','rating_in_notch']]
df_sp['timestamp'] = pd.to_datetime(df_sp['ratingDate'])
df_sp['year'] = [x.year for x in df_sp.timestamp]
df_sp.drop_duplicates(subset=['name_tokyo','year'], inplace=True)



#%% LC
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat = pd.merge(df, df_sp, left_on=['Borrower Name','year'], right_on=['name_tokyo','year'], how='inner')

#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    46
1     4
Name: Gua_override, dtype: int64
'''

dat = dat.query('Gua_override==0').reset_index(drop=True)


#%% locate outliers
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])


#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['LC_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['LC_PD'] = list_PD
    dat['LC_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_LC_norm = get_proxy_rating(dat_LC_norm, model_LC)
# 37

print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['LC_PDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['PrimaryEvaluationPDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Applied Rating PDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Applied Rating']))
-0.05982905982905983
0.4551282051282051
0.6303418803418803
0.6025641025641025


dat_LC_norm['Borrower Type'].value_counts()
'''

Corporation (Major Company)    32
Corporation (Other Company)     2
Name: Borrower Type, dtype: int64
'''

#%%


#%% MM
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')
dat = pd.merge(df, df_sp, left_on=['Borrower Name','year'], right_on=['name_tokyo','year'], how='inner')

#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()



dat = dat.query('Gua_override==0').reset_index(drop=True)


#%% locate outliers
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])


#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['MM_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['MM_PD'] = list_PD
    dat['MM_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_MM_norm = get_proxy_rating(dat_MM_norm, model_MM)
# 8


print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['MM_PDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['PrimaryEvaluationPDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Applied Rating PDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Applied Rating']))
0.5789473684210527
0.5789473684210527
1.0
1.0



dat_MM_norm['Borrower Type'].value_counts()
'''
Corporation (Other Company)    4
Corporation (Major Company)    3
Name: Borrower Type, dtype: int64

'''
['01_prepare_data.py', '02_analysis.py', '03_withCompuStat.py', '03_withCompuStat_SPmodel.py', '04_SNL_prepare.py', '05_checkdata.py', '06_check2019data.py', '06_check2019data_updated_BL.py', '07_filterandcreateAmNonAm.py', '07_filterandcreateAmNonAm_dropsomerating.py', '08_SFA_archived_BvD_preparedata.py', '08_SFA_archived_BvD_somersd.py', '08_SFA_archived_BvD_somersd_quant.py', '09_EXP_archived_BvD_buildratio.py', '09_EXP_archived_BvD_MM_AM+EU_SFAMFA.py', '09_EXP_archived_BvD_MM_am_SFAMFA.py', '09_EXP_archived_BvD_MM_nam_SFAMFA.py', '10_SFAMFA_01_preparedata.py', '10_SFAMFA_02_merge_with_BVD.py', '10_SFAMFA_03_buildratio.py', '10_SFAMFA_04_LC _noGua.py', '10_SFAMFA_04_LC _noGua_nooutliers.py', '10_SFAMFA_04_LC.py', '10_SFAMFA_05_MM.py', '11_relink_01_preparedata.py', '11_relink_02_merge_with_BVD.py', '11_relink_03_buildratio.py', '11_relink_04_CPPD_LC.py', '11_relink_04_CPPD_LC_original.py', '11_relink_04_CPPD_MM.py', '11_relink_04_CPPD_MM_original.py', '11_relink_archived_04_LC_noGua_nooutliers.py', '11_relink_archived_04_LC_noGua_nooutliers_exp.py', '11_relink_archived_04_MM_noGua_nooutliers.py', 'construct_cty_risk_label.py', 'CPPD_07_filtering.py', 'CPPD_08_SFA_BvD_preparedata.py', 'CPPD_08_SFA_Major.py', 'CPPD_08_SFA_Major_dropped.py', 'CPPD_08_SFA_Major_test.py', 'CPPD_08_SFA_Other.py', 'CPPD_08_SFA_Other_dropped.py', 'CPPD_08_SFA_Other_test.py', 'CPPD_09_MFA_Other.py', 'newfunc.py', 'prdrvw_01_othercorp.py', 'varlib.py', '__investigation_Benford.py', '__investigation_BvDmissImportant.py', '__investigation_BvDmissImportant2.py', '__investigation_CBM.py', '__investigation_checkbreakpoint_GCARS.py', '__investigation_CPPD_O&G_NPPU.py', '__investigation_default.py', '__investigation_GCARS_stability.py', '__investigation_othercorp_replication.py', '__investigation_sample.py', '__investigation_SP.py']
[232, 341, 418, 496, 854, 952, 1069, 1148, 1299, 1465, 1688, 1794, 2106, 2358, 2657, 3000, 3345, 3474, 3648, 4064, 4856, 5600, 6384, 7162, 7316, 7540, 7844, 8484, 8807, 9401, 9740, 10505, 10916, 11682, 11802, 11941, 12086, 12336, 12584, 12814, 13050, 13286, 13518, 14133, 14230, 14374, 14517, 14826, 15277, 16082, 16417, 16482, 16607, 16914, 17050, 17196, 17402, 17648]

#%% consider year
cty = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\Country Rates Merged 01-29-2020.xlsx')
cty['Cty_Name'] = [x.upper() for x in cty['GCARS Country Name']]
cty["Country Ratings"] = cty["Country Ratings"].astype('category')
cty["Cty_Rating_Label"] = cty["Country Ratings"].cat.codes
cty.drop_duplicates(subset=['Cty_Name','Period'], inplace=True)

dat = pd.merge(dat, cty, left_on=['Borrower Country','year'], right_on=['Cty_Name','Period'], how='left')
dat['Cty_Rating_Label'].fillna(2, inplace=True)


dat['Cty_Rating_Label2'] = dat['Cty_Rating_Label']
dat['Cty_Rating_Label2'].replace({
0: 9.376918,
1: 8.162304,
2: 8.601478,
3: 9.028239,
4:10.050725,
5:10.818182,
6:13.375000,
    },
    inplace=True)


dat['Cty_Rating_Label3'] = [0 if x<4 else 1 for x in dat['Cty_Rating_Label']]
dat['Cty_Rating_Label4'] = [0 if x<5 else 1 for x in dat['Cty_Rating_Label']]

dat = pd.concat([dat, pd.get_dummies(dat['Cty_Rating_Label'], prefix='Cty_Rating')], axis=1)




'''
#%% not consider year
cty = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\Country Rates Merged 01-29-2020.xlsx')
cty_name = cty.query('Period==2019')['GCARS Country Name'].unique().tolist()
cty_name = [x.upper() for x in cty_name]

dat_name  = dat['Borrower Country'].unique().tolist()
for name in dat_name:
    if name in cty_name:
        continue
    else:
        print(f'{name} is not in cty')





df = cty.query('Period==2019')
df["Country Ratings"] = df["Country Ratings"].astype('category')
df["Cty_Rating_Label"] = df["Country Ratings"].cat.codes
df['Cty_Name'] = [x.upper() for x in df['GCARS Country Name']]
dict_code = dict(zip(df['Cty_Name'], df['Cty_Rating_Label']))
dict_code.update({
    'MARSHALL ISLANDS':2,
    'LIBERIA':2,
    'JERSEY,C.I.':2,
    'Br. Virgin Is.':2, 
    'REPUBLIC OF MONTENEG':2, 
    'Lao P.D.R':2, 
    })
dat['Cty_Rating_Label'] = dat['Borrower Country'].copy()
dat['Cty_Rating_Label'] = dat['Cty_Rating_Label'].replace(dict_code)
dat['Cty_Rating_Label'].fillna(2, inplace=True)


dat['Cty_Rating_Label'].value_counts()
'''

#%%


SomersD(dat['Applied Rating PDRR'], dat['Cty_Rating_Label'])


SomersD(dat['Applied Rating PDRR'], dat['Cty_Rating_0.0'])





dat.groupby('Cty_Rating_Label')['Applied Rating PDRR'].mean()

SomersD(dat['Applied Rating PDRR'], dat['Cty_Rating_Label2'])



dat[['Applied Rating PDRR','Cty_Rating_Label']].corr()




#%%



#%% Optimize SD Geo weights
x_train = sm.add_constant(df[['score', 'Cty_Rating_Label4']], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit()
result.summary()

df['fittedvalues'] = result.fittedvalues
df['fittedvalues_round'] = [round(x) for x in df['fittedvalues']]

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
# 0.5545549365285304
SomersD(df['Applied Rating PDRR'], df['fittedvalues_round'])
# 0.5325370780228447




from PDScorecardTool.Process import google_color
df2 = df[df.Cty_Rating_Label.isin([0,1,2,3])]

sns.relplot(x='score', y='Applied Rating PDRR', data=df2, hue="Cty_Rating_Label", alpha=0.7, 
    palette=google_color, height=9)# -*- coding: utf-8 -*-
"""
Created on Tue Jan 28 13:20:53 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCARS 2008-2019_matches2.pkl')


#%% Drop defaulters
def_mask = (dat['Applied Rating']==101) | (dat['Applied Rating']==102)
dat = dat[~def_mask]

country_str = 'Borrower Country'
country_count = dat[country_str].value_counts()
dat['year'].value_counts().sort_index()



#%% Drop rating 2-0, 2-3, 3-0, 8-3, 9-0
mask = dat['Applied Rating'].isin([20,23,30,83,90])
dat_dropped = dat[~mask]


# cast integer rating to string
dat_dropped['BTMU Rating'] = dat_dropped['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat_dropped['BTMU Rating'].value_counts().sort_index() 
'''
1-0      533
2-1      134
2-2      369
3-1      837
3-2     1540
3-3     1474
4-0    23073
5-1    10455
5-2     5634
6-1     4883
6-2     6239
7-0    11167
8-1     5979
8-2     4955
Name: BTMU Rating, dtype: int64
'''

# map to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

dat_dropped['Applied Rating PDRR'] = dat_dropped['Applied Rating'].transform(lambda x: pd_mapping[x])
cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Match',
'Borrower Industry',
'Industry Model',
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']



dat_dropped[cols].to_pickle(r'..\data\CPPD_dat_dropped_2008-2019.pkl')


#%%  keep all rating

# cast integer rating to string
dat['BTMU Rating'] = dat['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat['BTMU Rating'].value_counts().sort_index() 
'''
1-0      533
2-0     7231
2-1      134
2-2      369
2-3      302
3-0    18852
3-1      837
3-2     1540
3-3     1474
4-0    23073
5-1    10455
5-2     5634
6-1     4883
6-2     6239
7-0    11167
8-1     5979
8-2     4955
8-3       67
9-0      252
Name: BTMU Rating, dtype: int64
'''


cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Type 2',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Match',
'Borrower Industry',
'Industry Model',
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']



dat[cols].to_pickle(r'..\data\CPPD_dat_2008-2019.pkl')
# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner

df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019.pkl')
df2 = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019.pkl')


bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df2, col='Borrower CIF', inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)
df_bvd.drop_duplicates(subset=['CIF','Fiscal year'], inplace=True)

len(set(df['Borrower CIF'].unique()))
# 23190

len(set(df2['Borrower CIF'].unique()))
# 19742

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(df['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 10745

len(set(df2['Borrower CIF'].unique())&set(df_bvd['CIF'].unique()))
# 9187


#%%

#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% merge
df = pd.merge(df, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df['BvD_CIF']]
df['inBvD'].value_counts()
'''
Out[11]: 
No     61708
Yes    42268
Name: inBvD, dtype: int64

'''

df2 = pd.merge(df2, df_bvd, left_on=['Borrower CIF','year'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df2['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df2['BvD_CIF']]
df2['inBvD'].value_counts()
'''
No     47410
Yes    29862
Name: inBvD, dtype: int64
'''



df.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')
df2.to_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')

# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Major Company)"]

# 21972
df['year'].value_counts().sort_index()
'''
2008      98
2009     785
2010    2056
2011    2306
2012    2206
2013    2412
2014    2680
2015    2522
2016    2437
2017    1785
2018    1419
2019    1266
Name: year, dtype: int64
'''
df['inBvD'].value_counts()
'''
No     14307
Yes     7665
Name: inBvD, dtype: int64
'''


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

Major_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, Major_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_results_Major.xlsx')


list_factor = model_LC.quant_factor + model_MM.quant_factor + Major_factor_cols
list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,-1,-1,-1,-1,1,-1,-1,-1,1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        if len(df_tmp_dropna)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        if len(dat_LC)<5:
            list_sd_LC.append(np.nan)
        else:
            list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        if len(dat_MM)<5:
            list_sd_MM.append(np.nan)
        else:
            list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        if len(dat_model)<5:
            list_sd_model.append(np.nan)
        else:
            list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['MajorCorpModel_size'] = list_obs_model
    result['MajorCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Industry Model'
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Major Company)"]

# 16483
df['year'].value_counts().sort_index()
'''
2008      62
2009     498
2010    1651
2011    1837
2012    1767
2013    1966
2014    2175
2015    2009
2016    1913
2017    1117
2018     788
2019     700
Name: year, dtype: int64
'''
df['inBvD'].value_counts()
'''
No     11527
Yes     4956
Name: inBvD, dtype: int64
'''


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

Major_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, Major_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_dat_dropped_results_Major.xlsx')

list_factor = model_LC.quant_factor + model_MM.quant_factor + Major_factor_cols
list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,-1,-1,-1,-1,1,-1,-1,-1,1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        if len(df_tmp_dropna)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        if len(dat_LC)<5:
            list_sd_LC.append(np.nan)
        else:
            list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        if len(dat_MM)<5:
            list_sd_MM.append(np.nan)
        else:
            list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        if len(dat_model)<5:
            list_sd_model.append(np.nan)
        else:
            list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['MajorCorpModel_size'] = list_obs_model
    result['MajorCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Industry Model'
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Major Company)"]



def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

Major_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, Major_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_results_Major_dropped_3year.xlsx')

list_factor = model_LC.quant_factor + model_MM.quant_factor + Major_factor_cols
#df=df.dropna(subset=model_LC.quant_factor+model_MM.quant_factor+['Applied Rating','Primary Evaluation'], how='any')

df=df.query('2016<=year<=2018')


list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,-1,-1,-1,-1,1,-1,-1,-1,1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        if len(df_tmp_dropna)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        if len(dat_LC)<5:
            list_sd_LC.append(np.nan)
        else:
            list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        if len(dat_MM)<5:
            list_sd_MM.append(np.nan)
        else:
            list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        if len(dat_model)<5:
            list_sd_model.append(np.nan)
        else:
            list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['MajorCorpModel_size'] = list_obs_model
    result['MajorCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Industry Model'
list_dataset_name = list(df['Industry Model'].value_counts().index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Industry Model']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Other Company)"]
# 82004
df['year'].value_counts().sort_index()
'''
2008      97
2009    2800
2010    7323
2011    7838
2012    8728
2013    8830
2014    9011
2015    7455
2016    7447
2017    7929
2018    7529
2019    7017
Name: year, dtype: int64
'''
df['inBvD'].value_counts()
'''
No     47401
Yes    34603
Name: inBvD, dtype: int64
'''


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

other_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, other_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_results_Other.xlsx')


list_factor = model_LC.quant_factor + model_MM.quant_factor + other_factor_cols
list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,1,-1,-1,1,-1,-1,-1,-1,-1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['OtherCorpModel_size'] = list_obs_model
    result['OtherCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Borrower Industry'
list_dataset_name = list(df['Borrower Industry'].value_counts()[:20].index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Borrower Industry']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')

df = df[df['Borrower Type']=="Corporation (Other Company)"]
# 60789
df['year'].value_counts().sort_index()
'''
2008      72
2009    2248
2010    5862
2011    6061
2012    6831
2013    7019
2014    7020
2015    5648
2016    5705
2017    5440
2018    4657
2019    4226
Name: year, dtype: int64
'''
df['inBvD'].value_counts()
'''
Out[94]: 
No     35883
Yes    24906
Name: inBvD, dtype: int64
'''


def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

other_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, other_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
writer = pd.ExcelWriter(r'SFA/CPPD_dat_dropped_results_Other.xlsx')

list_factor = model_LC.quant_factor + model_MM.quant_factor + other_factor_cols
list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,1,-1,-1,1,-1,-1,-1,-1,-1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))

    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['OtherCorpModel_size'] = list_obs_model
    result['OtherCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Borrower Industry'
list_dataset_name = list(df['Borrower Industry'].value_counts()[:20].index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Borrower Industry']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.Process import gcar_converter
import pickle
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
#df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')
df = df[df['Borrower Type']=="Corporation (Other Company)"]

def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

other_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


df = build_cni_factors(df)
df = other_processing(df)
gcar_converter(df, other_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor:                       
        df[factor] = df[factor].clip(
            np.nanmin(df[factor][df[factor] != -np.inf]), 
            np.nanmax(df[factor][df[factor] != np.inf]))
    
 
   



#%% factor level SFA
#writer = pd.ExcelWriter(r'SFA/CPPD_results_Other_overlaped.xlsx')
#writer = pd.ExcelWriter(r'SFA/CPPD_results_Other_dropped_overlaped_matched.xlsx')
#writer = pd.ExcelWriter(r'SFA/CPPD_results_Other_dropped_overlaped_3year.xlsx')
writer = pd.ExcelWriter(r'SFA/CPPD_results_Other_dropped_3year.xlsx')

list_factor = model_LC.quant_factor + model_MM.quant_factor + other_factor_cols
#df=df.dropna(subset=list_factor, how='any')
#df=df.query('Match==1')
df=df.query('2016<=year<=2018')


list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,1,-1,-1,1,-1,-1,-1,-1,-1]

list_year = ['==2012','==2013','==2014','==2015','==2016','==2017','==2018','==2019','<=2019']
list_colname = ['2012_SD','2013_SD','2014_SD','2015_SD','2016_SD','2017_SD','2018_SD','2019_SD','ALL_SD']
list_colname2 = ['2012_obs','2013_obs','2014_obs','2015_obs','2016_obs','2017_obs','2018_obs','2019_obs','ALL_obs']

result = pd.DataFrame()
for p,cond in enumerate(list_year):
    df_tmp = df.query(f'year{cond}')
    list_sd = []
    list_obs = []
    for q, factor in enumerate(list_factor):
        df_tmp_dropna = df_tmp.dropna(subset=[factor,'Applied Rating'])
        list_obs.append(len(df_tmp_dropna))
        if len(df_tmp_dropna)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(list_multiplier[q]*SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor]))
    result[list_colname[p]] = list_sd
    result[list_colname2[p]] = list_obs

result.index = list_factor

result.to_excel(writer, 'SFA')



#%% model score and perform
def calc_fun(list_dataset, list_dataset_name):

    list_obs_LC = []
    list_sd_LC = []
    list_obs_MM = []
    list_sd_MM = []
    list_obs_model = []
    list_sd_model = []
    result = pd.DataFrame()

    for dat in list_dataset:

        dat_LC = dat.dropna(subset=model_LC.quant_factor, how='any')
        dat_LC = normalization(dat_LC, model_LC, quant_only=True, missing='median')
        dat_LC['quant_score'] = (model_LC.quant_weight*dat_LC[model_LC.quant_factor]).sum(axis=1)
        list_obs_LC.append(len(dat_LC))
        if len(dat_LC)<5:
            list_sd_LC.append(np.nan)
        else:
            list_sd_LC.append(SomersD(dat_LC['Applied Rating'], dat_LC['quant_score']))

        dat_MM = dat.dropna(subset=model_MM.quant_factor, how='any')
        dat_MM = normalization(dat_MM, model_MM, quant_only=True, missing='median')
        dat_MM['quant_score'] = (model_MM.quant_weight*dat_MM[model_MM.quant_factor]).sum(axis=1)
        list_obs_MM.append(len(dat_MM))
        if len(dat_MM)<5:
            list_sd_MM.append(np.nan)
        else:
            list_sd_MM.append(SomersD(dat_MM['Applied Rating'], dat_MM['quant_score']))

        dat_model =  dat.dropna(subset=['Applied Rating','Primary Evaluation'], how='any')
        list_obs_model.append(len(dat_model))
        if len(dat_model)<5:
            list_sd_model.append(np.nan)
        else:
            list_sd_model.append(SomersD(dat_model['Applied Rating'], dat_model['Primary Evaluation']))


    result['LC_size'] = list_obs_LC
    result['LC_SD'] = list_sd_LC
    result['MM_size'] = list_obs_MM
    result['MM_SD'] = list_sd_MM
    result['OtherCorpModel_size'] = list_obs_model
    result['OtherCorpModel_SD'] = list_sd_model

    result.index = list_dataset_name

    return(result)


#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun(list_dataset, list_dataset_name)


#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun(list_dataset, list_dataset_name)


#%% Industry by 'Borrower Industry'
list_dataset_name = list(df['Borrower Industry'].value_counts()[:20].index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Borrower Industry']==ind])

result3 = calc_fun(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2015')
df2 = df.query('year==2016')
df3 = df.query('year==2017')
df4 = df.query('year==2018')
df5 = df.query('year==2019')
list_dataset = [df1, df2, df3, df4, df5]
list_dataset_name =['2015', '2016','2017','2018','2019']

result4 = calc_fun(list_dataset, list_dataset_name)



#%% Sum
result = pd.concat([result1,result2,result3,result4], axis=0)

result.to_excel(writer, 'Analysis')



writer.save()# -*- coding: utf-8 -*-
"""
Created on Fri May 15 11:29:47 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import SomersD, normalization
from PDScorecardTool.CreateBenchmarkMatrix import TMstats
from PDScorecardTool.Process import gcar_converter
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm

model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
#df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_2008-2019_SFA_BvD.pkl')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD_dat_dropped_2008-2019_SFA_BvD.pkl')
dat = dat[dat['Borrower Type']=="Corporation (Other Company)"]

def build_cni_factors(dat):
    dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
    dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
    dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
    dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
    dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
    dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
    dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
    dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
    return (dat)    

def other_processing(dat):
    # for invalid_neg
    dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
    dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
    dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
    dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']

    # MM new transformation
    dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
    dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb
    return (dat) 

other_factor_cols=[
'Current Financial Data #1',
'Current Financial Data #2',
'Current Financial Data #3',
'Current Financial Data #4',
'Current Financial Data #5',
'Current Financial Data #6',
'Current Financial Data #7',
'Current Financial Data #8',
'Current Financial Data #9',
'Current Financial Data #10',]


dat = build_cni_factors(dat)
dat = other_processing(dat)
gcar_converter(dat, other_factor_cols, inplace=True)

# fill inf
for factor in model_LC.quant_factor+model_MM.quant_factor+other_factor_cols:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))
     


list_factor = model_LC.quant_factor + model_MM.quant_factor + other_factor_cols
dat=dat.dropna(subset=list_factor, how='any')
#dat=dat.query('Match==1')
dat=dat.query('2016<=year<=2018')


list_multiplier = model_LC.quant_multiplier + list(model_MM.quant_multiplier) + [-1,1,-1,-1,1,-1,-1,-1,-1,-1]



#%% Build ratios
cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']


#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios

dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']



factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + other_factor_cols+ [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))




#%%

#%% SFA
result = pd.DataFrame()
list_quality=[]
list_sd=[]
for factor in factor_list:
    df_tmp_dropna = dat.dropna(subset=[factor])
    list_quality.append(len(df_tmp_dropna)/ len(dat))
    list_sd.append(np.abs(SomersD(df_tmp_dropna['Applied Rating'], df_tmp_dropna[factor])))

result['SomersD'] = list_sd
result['DataQuality'] = list_quality 
result.index = factor_list
result.sort_values('SomersD', ascending=False, inplace=True)
result.to_excel(r'EXP/CPPD_SFA_other_dropped_overlaped_3years.xlsx')




#%% pick SD > 0.20
list_short = result.query('SomersD>=0.2 and DataQuality>=0.7').index.tolist()


# initial 
f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(dat[list_short].corr(), linewidths=.3, cmap='Blues', ax=ax)
# after clustering
cat=cluster_corr(dat, list_short)
plot_cluster_corr(dat, cat)



#%% after manual selection
list_final=[
'Current Financial Data #7',
'Current Financial Data #6',
'Current Financial Data #8',
'Current Financial Data #3',
'prof@EBITDA_to_TA',
'Current Financial Data #1',
'LC_UBEBITDA_to_IE', 
'LC_UBEBITDA_to_NS',
'new@CLCoverageRatio', 
'BvD_P/L for period [=Net income] ', 
'Current Financial Data #4', 
'new@CashFlowRatio',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_Capt',
'Current Financial Data #5',
'new@LTDCoverageRatio']

f, ax = plt.subplots(figsize=(20, 12))
sns.heatmap(np.abs(dat[list_final].corr()), linewidths=.3, cmap='Blues', ax=ax)




for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    aa = tmp.groupby('Applied Rating PDRR').mean()
    p=i+1
    plt.subplot(4,4,p)
    plt.plot(aa[factor],aa.index)
    plt.title(factor)
    print(f'min of {factor} is {tmp[factor].min()}')


for i, factor in enumerate(list_final): 
    tmp = dat[[factor, 'Applied Rating PDRR']].copy()
    tmp[factor] = tmp[factor].clip(tmp[factor].quantile(0.05), tmp[factor].quantile(0.95))
    tmp = tmp.dropna(subset=[factor])
    p=i+1
    plt.subplot(4,4,p)
    sns.distplot(tmp[factor])
    plt.title(factor)



list_transform = [
0,
0,
0,
0,
0,
'Current Financial Data #1',
0,0,0,
'BvD_P/L for period [=Net income] ', 
0,0,0,0,0,0]

dat['Current Financial Data #1'] = dat['Current Financial Data #1'] + np.abs(dat['Current Financial Data #1'].min()) +1
dat['BvD_P/L for period [=Net income] '] = dat['BvD_P/L for period [=Net income] '] + np.abs(dat['BvD_P/L for period [=Net income] '].min()) +1




#%% auto invalid negtive identification:
pl_invalid_neg = []
list_neg = [
'Current Financial Data #7',
'Current Financial Data #6',
'Current Financial Data #8',
'Current Financial Data #3',
'prof@EBITDA_to_TA',
'LC_UBEBITDA_to_IE', 
'LC_UBEBITDA_to_NS',
'new@CLCoverageRatio', 
'Current Financial Data #4', 
'new@CashFlowRatio',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_Capt',
'Current Financial Data #5',
'new@LTDCoverageRatio']



for factor in list_neg:
    df = dat[[factor, 'Applied Rating PDRR']].copy()
    # rename
    df.rename(columns={factor:'ratio'}, inplace=True)
    df_pos = df.query('ratio>0')
    floor_mean = df_pos.query('ratio<={}'.format(df_pos.ratio.quantile(0.05)))['Applied Rating PDRR'].mean()   
    cap_mean = df_pos.query('ratio>={}'.format(df_pos.ratio.quantile(0.95)))['Applied Rating PDRR'].mean()   
    neg_mean = df.query('ratio<=0')['Applied Rating PDRR'].mean()
    mid_point = np.sqrt(cap_mean*floor_mean)

    if floor_mean>cap_mean and neg_mean<mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    elif floor_mean<cap_mean and neg_mean>mid_point: # means it's closer to cap
        pl_invalid_neg.append(factor)
    else:
        pl_invalid_neg.append(0)

list_neg = [0,]*16





#%% MFA
dat_norm = dat[list_final+['Borrower CIF',
 'Borrower Name',
 'Borrower Country',
 'Borrower Type',
 'Borrower Type 2',
 'Borrower Office Code',
 'Borrower Office Name',
 'timestamp',
 'year',
 'Current Total Score',
 'Primary Evaluation',
 'BTMU Rating',
 'Applied Rating',
 'Applied Rating PDRR',
 'Match',
 'Borrower Industry',
 'Industry Model',
 'BvD_Proxy_Net_Sales_inUSD']].copy()

list_sd_sign = []
for i, factor in enumerate(list_final): 
        sign = np.sign(SomersD(dat_norm['Applied Rating PDRR'], dat_norm[factor])) 
        list_sd_sign.append(sign)
        # invalid negtive                   
        if list_neg[i]:            
            dat_norm[factor][ (dat_norm[factor]<0) & (dat_norm[list_neg[i]]<0) ] = dat_norm[factor].quantile(0.95)
            # treat NA in 'neg_source' as negative value
            dat_norm[factor][ (dat_norm[factor]<0) & pd.isnull(dat_norm[list_neg[i]]) ] = dat_norm[factor].quantile(0.95)
        # cap/floor
        dat_norm[factor] = dat_norm[factor].clip(dat_norm[factor].quantile(0.05), dat_norm[factor].quantile(0.95))
        # transformation
        if list_transform[i]:
            dat_norm[factor] = np.log(dat_norm[factor])

        # quick normalization
        dat_norm[factor] = sign*50*(dat_norm[factor] - dat_norm[factor].mean()) / dat_norm[factor].std()






#%%  model selection

writer = pd.ExcelWriter(r'EXP/CPPD_MFA_other_dropped_overlaped_3years_more.xlsx')
#for factor_num in range(4,8):
for factor_num in range(8,11):
    pl_models=[];  pl_R2=[];  pl_SD=[];   pl_pvals=[]; pl_sign=[]
    pl_obs=[];  pl_wts=[]
    for setting in combinations(list_final, factor_num):
        model = list(setting)
        df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')
        pl_obs.append(len(df))
        x_train = sm.add_constant(df[model], prepend = True)
        linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
        result = linear.fit(disp=0)
        pl_pvals.append(result.pvalues.max())
        if (np.sign(result.params[1:]).sum()==factor_num):
            pl_sign.append('correct')
        else:
            pl_sign.append('wrong')
        
        pl_models.append(model)
        pl_wts.append(list(result.params[1:] / result.params[1:].sum()))
        pl_R2.append(result.rsquared)
        pl_SD.append(SomersD(df['Applied Rating PDRR'], result.fittedvalues))
    
    
    models_result = pd.DataFrame()
    models_result['models'] = pl_models
    models_result['weights'] = pl_wts
    models_result['#OBS'] = pl_obs
    models_result['R2'] = pl_R2
    models_result['SomersD'] = pl_SD
    models_result['max_pvalue'] = pl_pvals
    models_result['sign'] = pl_sign


    models_result.query('sign=="correct"').sort_values(
        'SomersD', ascending=False).to_excel(writer, f'FactorNum={factor_num}')

writer.save()



#%% model performance

model =['Current Financial Data #7', 'Current Financial Data #6', 'Current Financial Data #1', 'Current Financial Data #5']


df = dat_norm.dropna(subset=model+['Applied Rating PDRR'], how='any')

x_train = sm.add_constant(df[model], prepend = True)
linear = sm.OLS(df['Applied Rating PDRR'], x_train, missing='drop')
result = linear.fit(disp=0)
df['fittedvalues'] = result.fittedvalues

SomersD(df['Applied Rating PDRR'], df['fittedvalues'])
#0.5454782648499367

#%%



#%% model score and perform
def calc_fun_mfa(list_dataset, list_dataset_name):
    list_obs = []
    list_sd = []
    result = pd.DataFrame()
    for dat in list_dataset:
        list_obs.append(len(dat))
        if len(dat)<5:
            list_sd.append(np.nan)
        else:
            list_sd.append(SomersD(dat['Applied Rating PDRR'], dat['fittedvalues']))

    result['DataSize'] = list_obs
    result['SomersD'] = list_sd
    result.index = list_dataset_name

    return(result)

#%% Size
df1 = df.query('BvD_Proxy_Net_Sales_inUSD>1e9')
df2 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e9')
df3 = df.query('BvD_Proxy_Net_Sales_inUSD<=1e8')
list_dataset = [df1, df2, df3]
list_dataset_name =['Sales 1Bn or more', 'Sales 1Bn or below','Sales 100M or below']

result1 = calc_fun_mfa(list_dataset, list_dataset_name)

#%% Geo
list_americas_office_code = [3103,3104,3108,3110,3116,3120,3134,\
3138,3147,3149,3153,3157,3158,3160,3161,3165,3170,3191,3220,3500,3770,
3281,3282,3286,
3250,3260
]
list_eu_office_code=list(set([
3301,3364,3461,3321,3234,3361,3237,3348,3523,3357,3424,3561,3524,3238,
3525,3526,3239,3591,3377,3305,3225,3351,3226,3354,3231,3310,3233,3356,
3232,3340,3341,3343,3349,3235,3235,3235,3235,3304,3309,3219,3219,3371,
3342,3345,33422,3236,3236,3550,3240,3337,3330,3332,6412,6246,3525,6249,
6762,6434,3305,6359,3361,3321,3461,3348,6430,6334,6338,6363,6277,6360,
6431,6335,6415,6444,6418,6276,6423,6990,6207,6227,3321,3305,3340,3461,
3525,3301,3301,3301,3301,6337,6447,6457,6362,7232,7233,
]))

list_code = list_americas_office_code + list_eu_office_code

df1 = df.loc[(df['Borrower Office Code']).isin(list_americas_office_code)] 
df2 = df.loc[(df['Borrower Office Code']).isin(list_eu_office_code)] 
df3 = df.loc[~(df['Borrower Office Code']).isin(list_code)] 
list_dataset = [df1, df2, df3]
list_dataset_name =['Americas','EMEA','Asia']

result2 = calc_fun_mfa(list_dataset, list_dataset_name)



#%% Industry by 'Borrower Industry'
list_dataset_name = list(df['Borrower Industry'].value_counts()[:20].index)
list_dataset=[]
for ind in list_dataset_name:
    list_dataset.append(df[df['Borrower Industry']==ind])

result3 = calc_fun_mfa(list_dataset, list_dataset_name)


#%% Year
df1 = df.query('year==2016')
df2 = df.query('year==2017')
df3 = df.query('year==2018')

list_dataset = [df1, df2, df3]
list_dataset_name =['2016','2017','2018']

result4 = calc_fun_mfa(list_dataset, list_dataset_name)


result = pd.concat([result1,result2,result3,result4], axis=0)


result.to_excel(r'EXP\4factor.xlsx')
import os, sys, pandas as pd, numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(palette='muted')



def cluster_corr(dat, pl_cols):
    pl_cat1 = [pl_cols[0],]
    pl_cat2 = []

    for name in pl_cols[1:]:
        if np.abs(dat[[name]+pl_cat1].corr()).iloc[0,1:].mean() > 0.75:
            pl_cat1.append(name)
        else:
            pl_cat2.append(name)
    if len(pl_cat2)==0:
        return((pl_cat1,))
    else:
        return((pl_cat1,) + cluster_corr(dat, pl_cat2))


def plot_cluster_corr(dat, cat_tuple):
    cols = []
    for cat in cat_tuple:
        cols = cols+cat

    f, ax = plt.subplots(figsize=(10*len(cols)/25, 6*len(cols)/25))
    sns.heatmap(np.abs(dat[cols].corr()), linewidths=.3, cmap='Blues', ax=ax)


def quanttrans(data, model, floor=0.05, cap=0.95):
    """
    This function calculates the floor, cap, mean and std of the data. The output
    can be used as the updated parameters for quantitative factors normailization.
    Modified in Ver. 1.4

    Parameters:

        data:   the input dataset in DataFrame. Make sure no NA in quant
                factors

        model:  PDModel class

        floor:  float, default 0.05
                quantile for floor 

        cap:    float, default 0.95
                quantile for cap 

        
    Return:
        a dictionary that saves floor, cap, doc_mean, doc_std
    
    """    
    normdata = data.copy()


    # get new cap and floor from valid observations:
    floor_list=[];  cap_list=[]

    for i, factor in enumerate(model.quant_factor):
        if not model.Invalid_Neg[i]:
            floor_list.append(normdata[factor].quantile(floor))
            cap_list.append(normdata[factor].quantile(cap))
        else:
            floor_list.append(normdata[factor][normdata[factor]>0].quantile(floor))
            cap_list.append(normdata[factor][normdata[factor]>0].quantile(cap))


    # Invalid_Neg
    for i,neg in enumerate(model.Invalid_Neg):
        if neg:
            col=model.quant_factor[i]
            normdata[col][normdata[col]<0] = cap_list[i]
           
    # cap/floor for quant factors:
    for i, col in enumerate(model.quant_factor):
        normdata[col] = np.clip(normdata[col], floor_list[i], cap_list[i])        

    # quant factors transformation:
    for i, col in enumerate(model.quant_factor):
        if model.quant_log[i]:
            normdata[col] = np.log(normdata[col])

    # get new mean and std:
    doc_mean=[];    doc_std=[]

    for i, col in enumerate(model.quant_factor):
        doc_mean.append(normdata[col].mean())     
        doc_std.append(normdata[col].std())     

    dictionary = {'floor':floor_list, 'cap':cap_list, 'doc_mean':doc_mean, 'doc_std':doc_std}
    
    return(dictionary)


import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from PDScorecardTool.Process import gcar_cif_cleaner, gcar_americas_office_code

data = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019.pkl')
data = data[data['Data From']=='CD']
data = data[data['Borrower Type']=='Corporation (Other Company)']

data.dropna(subset=['Borrower CIF'], inplace=True)
data['Borrower CIF_cleaned'] =gcar_cif_cleaner(data)
#

data['timestamp'] = data['Approval Date'].mask(
                        pd.isnull(data['Approval Date']),
                        data['Approval/Agreed Date'])
data['timestamp'] = pd.to_datetime(data['timestamp'])
data.dropna(subset=['timestamp'], inplace=True)
data['year'] = [int(x.year) for x in data['timestamp'] ]

# 2018-19 other corp
dat = data.query('year>2017')
is_model = dat['Borrower Type']=='Corporation (Other Company)'

Corp_Others_ModelData = dat[is_model]
Corp_Others_ModelData.shape
# (20373, 903)


isempty_cols = Corp_Others_ModelData.filter(regex='^Current Financial Data').isnull().all() 
# Corp_Others_ModelData[isempty_cols.index[~isempty_cols]].apply(lambda x:pd.to_numeric(x.str.replace('\,|\%',''), errors='coerce'))
COModel_factor_names = Corp_Others_ModelData[isempty_cols.index[~isempty_cols].str.replace('Current Financial Data', 'Scoring Name')].apply(lambda x:x.dropna().unique()).T[0].values

# dict(zip(model_factor_names, [dict(zip(['bins','labels'],[[],[]]))]*len(COModel_factor_names)))
factor_bins = \
{'Corporation (Other Company)':
    {

     '1. Capability for Debt Repayment - (1) Cash Flow Amount': {'bins': [-pd.np.Inf, 10*10**6, 30*10**6, 50*10**6, 70*10**6, 100*10**6, 300*10**6, 500*10**6, 700*10**6, 1000*10**6, pd.np.Inf],
                                                                 'labels': [2, 3, 4, 6, 8, 9, 11, 12, 13, 15]},
     '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA': {'bins': [-pd.np.Inf, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2, 8.2, 9.2, 10.2, pd.np.Inf],
                                                                                  'labels': [10, 8, 6, 4, 2, 0, -2, -4, -6, -8, -10]},#EBITA is negative then set it to -10
     '1. Capability for Debt Repayment - (3) EBITDA / Net Interest Payment': {'bins': [-pd.np.Inf, 0.85, 0.95, 2, 3, 4, 6, 8, pd.np.Inf],
                                                                              'labels': [0, 2, 4, 6, 8, 10, 11, 12]},
     '2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio': {'bins': [-pd.np.Inf, -80, -70, -60, -50, -40, -30, -20, -10, 0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, pd.np.Inf],
                                                                                  'labels': [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
     '2. Financial Condition - 2-1. Safety - (2) Gearing Ratio': {'bins': [-pd.np.Inf, 50, 75, 100, 150, 200, pd.np.Inf],
                                                                  'labels': [10, 8, 6, 4, 2, 0]},
     '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales': {'bins': [-pd.np.Inf, 1, 2, 3, 6, 8, 10, 12, 14, 16, 20, pd.np.Inf],
                                                                                    'labels': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},
     '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets': {'bins': [-pd.np.Inf, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, pd.np.Inf],
                                                                                                             'labels': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]},
     '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth': {'bins': [-pd.np.Inf, -40, -30, -20, -10, -2.5, 0, 0.5, 5, 25, 35, pd.np.Inf],
                                                                                                                    'labels': [-10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10]},
     '2. Financial Condition - 2-4. Size - (1) Net Worth': {'bins': [-pd.np.Inf, 50*10**6, 100*10**6, 150*10**6, 200*10**6, 350*10**6, 500*10**6, 750*10**6, 1000*10**6, 1500*10**6, 2000*10**6, pd.np.Inf],
                                                            'labels': [0, 1, 3, 4, 6, 7, 8, 10, 11, 13, 14]},
     '2. Financial Condition - 2-4. Size - (2) Sales': {'bins': [-pd.np.Inf, 100*10**6, 500*10**6, 1000*10**6, 3000*10**6, pd.np.Inf],
                                                        'labels': [0, 1, 2, 3, 4]}

    }
}
# factor_bins[model_names[3]]



score_df = pd.DataFrame()
for idx in isempty_cols.index[~isempty_cols].str.replace('Current Financial Data #',''):
    factor_num='{}'.format(idx)
    scr_str=Corp_Others_ModelData['Scoring Name #'+factor_num].dropna().unique()[0]    
    print(scr_str)
    df=Corp_Others_ModelData['Current Financial Data #'+factor_num]    
    df_clean = pd.to_numeric(df.astype(str).apply(lambda x:x.replace('%','').replace(',','')), errors='coerce')
    
    is_right = True
    if scr_str in ['2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio',
                   '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales',
                   '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets',
                   '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth',
                   '2. Financial Condition - 2-4. Size - (1) Net Worth']:
        is_right = False
    
    binned_df=pd.cut(df_clean,
                     bins=factor_bins[model_names[3]][scr_str]['bins'], 
                     labels=factor_bins[model_names[3]][scr_str]['labels'], right=is_right)
    df_2 = pd.concat([Corp_Others_ModelData.filter(regex='^Borrower Name$|^Application No.$|^Approval Date'),
                      df,df_clean, binned_df, Corp_Others_ModelData['Current Score #'+factor_num]], axis=1)
#     display(df_2.head())
    bin_counts = binned_df.value_counts(dropna=False).sort_index()
#     display(bin_counts.to_frame())
    nan_diff = bin_counts[bin_counts.index[-1:]].values[0]-df.isna().value_counts()[True]
    if nan_diff>0:
        print('Please check Current Financial Data #{} as there are {} strings which are not NaNs:'.format(factor_num,nan_diff))
#         non_numbered_df = df[~df.isna()].astype(str).apply(lambda x:x.replace('%','').replace(',',''))   
#         print(non_numbered_df[pd.to_numeric(non_numbered_df, errors='coerce').isna()].value_counts(),'\n') 
  
    if scr_str == '1. Capability for Debt Repayment - (1) Cash Flow Amount':
        binned_df = binned_df.astype(float)
        binned_df[df_clean<0]=0
        binned_df = binned_df.astype('category')

    #Deal with negative EBITA for the factor - '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA'
    if scr_str == '1. Capability for Debt Repayment - (2) Gross Debt (funded debt) / EBITDA':
        binned_df = binned_df.astype(float)
        binned_df[(df == 'Negative EBITDA')|(df == '#DIV/0!')]=-10
        binned_df = binned_df.astype('category')
#         print(df_2.iloc[[13781,12594]])        
#         print(df_2[(df == 'Negative EBITDA')|(df == '#DIV/0!')].shape)
#         print(df.isna().value_counts()) 

    if scr_str == '1. Capability for Debt Repayment - (3) EBITDA / Net Interest Payment':
        binned_df = binned_df.astype(float)
        binned_df[(df == 'EBITDA is 0 or less')|(df == 'Negative EBITDA')]=0
        binned_df[df=='#DIV/0!']=12
        binned_df = binned_df.astype('category')
        
    if scr_str in ['2. Financial Condition - 2-1. Safety - (1) Net Worth / Total Asset Ratio',
                   '2. Financial Condition - 2-2. Profitability - (2) (Net Profit - Extraordinary Items) / Total Assets',
                   '2. Financial Condition - 2-3. Growth Potential - (1) Increase or Decrease of Retained Earnings / Net Worth']:
        binned_df = binned_df.astype(float)
        binned_df[df=='#DIV/0!']=0
        binned_df = binned_df.astype('category') #use this option right = False for binning

    if scr_str == '2. Financial Condition - 2-1. Safety - (2) Gearing Ratio':
        binned_df[(df=='#DIV/0!')|(df == 'Negative EQuity')]=0
        binned_df = binned_df.astype(float)
        binned_df[df_clean.le(0)]=0
        binned_df = binned_df.astype('category')
           
    if scr_str == '2. Financial Condition - 2-2. Profitability - (1) Operating Profit / Sales':
        binned_df[(df=='#DIV/0!')|(df == 'Negative Sales')]=0
    
    diff_score = binned_df.fillna(0).sub(Corp_Others_ModelData['Current Score #'+factor_num], fill_value=0.0)
#     print(df_2[(diff_score.abs()>0)].head(20))
    vCts = diff_score.value_counts(dropna=False).sort_index()
#     print(vCts)
    print('Number of mismatches:',vCts[(vCts.index!=0.0) & (~vCts.index.isna())].sum(),'\n')

    bin_counts = binned_df.value_counts(dropna=False).sort_index()
    nan_diff = bin_counts[bin_counts.index[-1:]].values[0]-df.isna().value_counts()[True]
#     print('Current Financial Data #{} after migtigating for the NaNs:{}\n'.format(factor_num,nan_diff))
    score_df=pd.concat([score_df, binned_df], axis=1)
# display(score_df.head())
score_mismatches = score_df.sum(axis=1).sub(Corp_Others_ModelData['Current Total Score'], fill_value=0.0).value_counts(dropna=False).sort_index()
print('Total matches out of {} are {} ({:.0%})'.format(score_df.shape[0],score_mismatches.loc[0.0],score_mismatches.loc[0.0]/score_df.shape[0]))list_factor=['BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
 'BvD_Number of employees',
 'LC_UBEBITDA_to_NS',
 'LC_TD_to_UBEBITDA',
 'LC_Total Assets',
 'LC_TD_to_Capt',
 'LC_UBEBITDA_to_IE',
 'MM_NOP_to_NS',
 'MM_TD_to_UBEBITDA',
 'MM_TD_to_Capt',
 'MM_ECE_to_TL',
 'MM_TangNW_to_TA',
 'cf@TD_to_ACF',
 'bs@TD_to_CA_exc_CL',
 'liq@CA_exc_CL_to_TD',
 'liq@CA_exc_CL_to_TL',
 'liq@CA_exc_CL_to_TA',
 'liq@CA_exc_CL_to_CA',
 'liq@CA_exc_CL_to_CL',
 'size@TangNW_to_TA_exc_CA',
 'bs@TL_to_TL_exc_CL',
 'cf@TD_to_GP_and_Dep_and_Amo',
 'ds@GP_and_IE_to_IE',
 'ds@GP_and_IE_to_TL',
 'prof@TangA_to_NS',
 'size@TangNW_to_TA',
 'BTMU@EBITDA_to_IE',
 'BTMU@TD_to_EBITDA',
 'BTMU@EBITDA_to_IE',
 'bs@TD_to_CA_exc_CL',
 'bs@TD_to_TA_exc_TL',
 'cf@TD_to_GP_and_Dep_and_Amo',
 'cf@TD_to_GP_and_Dep_and_Amo',
 'cf@TD_NOP',
 'BTMU@OP_to_Sales',
 'cf@TD_to_EBITDA',
 'prof@EBITDA_to_Capt',
 'prof@EBITDA_to_NS',
 'prof@EBITDA_to_TangA',
 'prof@EBITDA_to_TA',
 'act@NS_to_CL',
 'act@NS_to_TA',
 'bs@TD_to_Capt',
 'bs@TL_to_Capt',
 'bs@CL_to_Capt',
 'prof@NOP_to_NS',
 'prof@NOP_to_TangNW',
 'prof@NOP_to_TA',
 'bs@TLTD_to_TA',
 'bs@TLTD_to_TangNW',
 'bs@TLTD_to_Capt',
 'bs@TL_to_TangNW',
 'bs@TL_to_TA',
 'bs@CL_to_TL',
 'ds@TL_to_IE',
 'liq@ECE_to_TL',
 'liq@CA_to_TL',
 'bs@TD_to_CA_exc_CL',
 'liq@ECE_to_CA',
 'liq@CA_to_TA',
 'liq@ECE_to_TD',
 'liq@ECE_to_TA',
 'liq@ECE_to_CL',
 'new@COGSRatio',
 'new@GrossMarkup',
 'new@GrossProfitMargin',
 'new@GrossProfitRatio',
 'newprof@GrossMarginRatio',
 'new@NetSalesRevenue',
 'new@CashFlowRatio',
 'new@AssetEfficiencyRatio',
 'new@CLCoverageRatio',
 'new@LTDCoverageRatio',
 'new@InterestCoverageRatio',
 'new@DA_to_Sales',
 'new@CostPerEmployee',
 'new@IFATurnover',
 'new@TFATurnover',
 'Other_Current Financial Data #1',
 'SP_Current Financial Data #1',
 'Other_Current Financial Data #2',
 'SP_Current Financial Data #2',
 'Other_Current Financial Data #3',
 'SP_Current Financial Data #3',
 'Other_Current Financial Data #4',
 'SP_Current Financial Data #4',
 'Other_Current Financial Data #5',
 'SP_Current Financial Data #5',
 'Other_Current Financial Data #6',
 'SP_Current Financial Data #6',
 'Other_Current Financial Data #7',
 'SP_Current Financial Data #7',
 'Other_Current Financial Data #8',
 'SP_Current Financial Data #8',
 'Other_Current Financial Data #9',
 'SP_Current Financial Data #9',
 'Other_Current Financial Data #10',
 'SP_Current Financial Data #10',
 'Geo_Americas',
 'Geo_Asia',
 'Geo_EMEA',
 'Tokyo_Corporation (Major Company)',
 'Tokyo_Corporation (Other Company)',]
# -*- coding: utf-8 -*-
"""
Created on Tue Sep  8 10:40:51 2020

@author: ub71894
"""


import os, sys, pandas as pd, numpy as np
import math
import matplotlib.pyplot as plt
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_20200903.pkl')
df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')


# Benford's Law percentages for leading digits 1-9
BENFORD = [30.1, 17.6, 12.5, 9.7, 7.9, 6.7, 5.8, 5.1, 4.6]


def count_first_digit(data_str):
    mask=df[data_str]>=1.
    data=list(df[mask][data_str])
    for i in range(len(data)):
        while data[i]>=10:
            data[i]=data[i]/10
    first_digits=[int(x) for x in sorted(data)]
    data_count=[]
    for i in range(9):
        count=first_digits.count(i+1)
        data_count.append(count)
    total_count=sum(data_count)
    data_percentage=[(i/total_count)*100 for i in data_count]
    return  total_count,data_count, data_percentage


def get_expected_counts(total_count):
    """Return list of expected Benford's Law counts for total sample count."""
    return [round(p * total_count / 100) for p in BENFORD]


def chi_square_test(data_count,expected_counts):
    """Return boolean on chi-square test (8 degrees of freedom & P-val=0.05)."""
    chi_square_stat = 0  # chi square test statistic
    for data, expected in zip(data_count,expected_counts):

        chi_square = math.pow(data - expected, 2)

        chi_square_stat += chi_square / expected

    print("\nChi-squared Test Statistic = {:.3f}".format(chi_square_stat))
    print("Critical value at a P-value of 0.05 is 15.51.")    
    return chi_square_stat < 15.51



#%% Plot

def bar_chart(data_pct, name):

    """Make bar chart of observed vs expected 1st digit frequency in percent."""

    fig, ax = plt.subplots()
    index = [i + 1 for i in range(9)]  # 1st digits for x-axis
# text for labels, title and ticks
    fig.canvas.set_window_title('Percentage First Digits')
    ax.set_title(f'{name[-8:]} vs. Benford Values', fontsize=15)
    ax.set_ylabel('Frequency (%)', fontsize=16)
    ax.set_xticks(index)
    ax.set_xticklabels(index, fontsize=14)

    # build bars    

    rects = ax.bar(index, data_pct, width=0.95, color='black', label='Data')

    # attach a text label above each bar displaying its height

    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2, height,
                '{:0.1f}'.format(height), ha='center', va='bottom', 
                fontsize=13)

    # plot Benford values as red dots
    ax.scatter(index, BENFORD, s=150, c='red', zorder=2, label='Benford')
    # Hide the right and top spines & add legend
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.legend(prop={'size':15}, frameon=False)

    plt.show()




total_count,data_count, data_percentage = count_first_digit('Current Financial Data #1')
expected_counts=get_expected_counts(total_count)
chi_square_test(data_count,expected_counts)

#%%
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']
df = dat[dat['Borrower Type']=='Corporation (Major Company)'].copy()


fig, axes = plt.subplots(2,5,figsize=(18,10))
for fac, ax in zip(curr, axes.flatten()):

    total_count,data_count, data_percentage = count_first_digit(fac)
    expected_counts=get_expected_counts(total_count)
    
    #bar_chart(data_percentage, fac)
    index = [i + 1 for i in range(9)]  # 1st digits for x-axis
    # text for labels, title and ticks
    #fig.canvas.set_window_title('Percentage First Digits')
    ax.set_title(f'{fac[-8:]} vs. Benford Values', fontsize=8)
    ax.set_ylabel('Frequency (%)', fontsize=10)
    ax.set_xticks(index)
    ax.set_xticklabels(index, fontsize=9)

    # build bars    

    rects = ax.bar(index, data_percentage, width=0.95, color='black', label='Data')

    # attach a text label above each bar displaying its height

    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2, height,
                '{:0.1f}'.format(height), ha='center', va='bottom', 
                fontsize=13)

    # plot Benford values as red dots
    ax.scatter(index, BENFORD, s=150, c='red', zorder=2, label='Benford')
    # Hide the right and top spines & add legend
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.legend(prop={'size':10}, frameon=False)

    plt.show()







bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']



gcar_converter(df_bvd, bvd_cols, inplace=True)
df = df_bvd.copy()

fig, axes = plt.subplots(2,5,figsize=(18,10))
for fac, ax in zip(bvd_cols[:10], axes.flatten()):

    total_count,data_count, data_percentage = count_first_digit(fac)
    expected_counts=get_expected_counts(total_count)
    
    #bar_chart(data_percentage, fac)
    index = [i + 1 for i in range(9)]  # 1st digits for x-axis
    # text for labels, title and ticks
    #fig.canvas.set_window_title('Percentage First Digits')
    ax.set_title(f'{fac[-8:]} vs. Benford Values', fontsize=8)
    ax.set_ylabel('Frequency (%)', fontsize=10)
    ax.set_xticks(index)
    ax.set_xticklabels(index, fontsize=9)

    # build bars    

    rects = ax.bar(index, data_percentage, width=0.95, color='black', label='Data')

    # attach a text label above each bar displaying its height

    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2, height,
                '{:0.1f}'.format(height), ha='center', va='bottom', 
                fontsize=13)

    # plot Benford values as red dots
    ax.scatter(index, BENFORD, s=150, c='red', zorder=2, label='Benford')
    # Hide the right and top spines & add legend
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.legend(prop={'size':10}, frameon=False)

    plt.show()



#%% check bvd ratio
dat_LC = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat_MM = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')

df = pd.concat([dat_LC, dat_MM])

curr=[
'LC_UBEBITDA_to_NS',
'LC_TD_to_UBEBITDA',
'LC_Total Assets',
'LC_TD_to_Capt',
'LC_UBEBITDA_to_IE',
'MM_NOP_to_NS',
'MM_TD_to_UBEBITDA',
'MM_TD_to_Capt',
'MM_ECE_to_TL',
'MM_TangNW_to_TA',]

df['LC_Total Assets'] = df['LC_Total Assets'].transform(np.exp)



fig, axes = plt.subplots(2,5,figsize=(18,10))
for fac, ax in zip(curr, axes.flatten()):

    total_count,data_count, data_percentage = count_first_digit(fac)
    expected_counts=get_expected_counts(total_count)
    
    #bar_chart(data_percentage, fac)
    index = [i + 1 for i in range(9)]  # 1st digits for x-axis
    # text for labels, title and ticks
    #fig.canvas.set_window_title('Percentage First Digits')
    ax.set_title(f'{fac} vs. Benford Values', fontsize=8)
    ax.set_ylabel('Frequency (%)', fontsize=10)
    ax.set_xticks(index)
    ax.set_xticklabels(index, fontsize=9)

    # build bars    

    rects = ax.bar(index, data_percentage, width=0.95, color='black', label='Data')

    # attach a text label above each bar displaying its height

    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2, height,
                '{:0.1f}'.format(height), ha='center', va='bottom', 
                fontsize=13)

    # plot Benford values as red dots
    ax.scatter(index, BENFORD, s=150, c='red', zorder=2, label='Benford')
    # Hide the right and top spines & add legend
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    ax.legend(prop={'size':10}, frameon=False)

    plt.show()

# -*- coding: utf-8 -*-
"""
Created on Tue Nov  3 13:25:00 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')

dfc = pd.read_pickle(r'..\data\Compustat_buildratio_20201106_addtag20201203.pkl')
dfb = pd.read_pickle(r'..\data\dat_2008-2019_cleaned_BvD_relink_addratios.pkl')



#%% bvd
dat = dfb.query('year>=2010')
dat = dat.query('inBvD=="Yes"')
dat.dropna(subset=['Applied Rating'], inplace=True)

bvd_cols=[
'BvD_Fixed assets',
'BvD_Intangible fixed assets',
'BvD_Tangible fixed assets',
'BvD_Other fixed assets',
'BvD_Current assets',
'BvD_Stock',
'BvD_Debtors',
'BvD_Other current assets',
'BvD_Cash & cash equivalent',
'BvD_Total assets',
'BvD_Shareholders funds',
'BvD_Capital',
'BvD_Other shareholders funds',
'BvD_Non-current liabilities',
'BvD_Long term debt',
'BvD_Other non-current liabilities',
'BvD_Provisions (Liabilities & Equity)',
'BvD_Current liabilities',
'BvD_Loans (Liabilities & Equity)',
'BvD_Creditors',
'BvD_Other current liabilities',
'BvD_Total shareh. funds & liab.',
'BvD_Working capital',
'BvD_Net current assets',
'BvD_Enterprise value',
'BvD_Number of employees',
'BvD_Operating revenue (Turnover)',
'BvD_Sales',
'BvD_Costs of goods sold',
'BvD_Gross profit',
'BvD_Other operating expenses',
'BvD_Operating P/L [=EBIT]',
'BvD_Financial P/L',
'BvD_Financial revenue',
'BvD_Financial expenses',
'BvD_P/L before tax',
'BvD_Taxation',
'BvD_P/L after tax',
'BvD_Extr. and other P/L',
'BvD_Extr. and other revenue',
'BvD_Extr. and other expenses',
'BvD_P/L for period [=Net income] ',
'BvD_Export revenue',
'BvD_Material costs',
'BvD_Costs of employees',
'BvD_Depreciation & Amortization',
'BvD_Other operating items',
'BvD_Interest paid',
'BvD_Research & Development expenses',
'BvD_Cash flow',
'BvD_Added value',
'BvD_EBITDA',
'BvD_Proxy_Total_Assets',
'BvD_Proxy_Interest_expense',
'BvD_Proxy_Ending_Cash_Equiv',
'BvD_Proxy_UBEBITDA',
'BvD_Proxy_Net_Sales',
'BvD_Proxy_Total_Debt',
'BvD_Proxy_Capitalization',
'BvD_Proxy_Net_Op_Profit',
'BvD_Proxy_Total_Liabilities',
'BvD_Proxy_Tangible_Net_Worth',
'BvD_Proxy_Total_Assets_inUSD',
'BvD_Proxy_Interest_expense_inUSD',
'BvD_Proxy_Ending_Cash_Equiv_inUSD',
'BvD_Proxy_UBEBITDA_inUSD',
'BvD_Proxy_Net_Sales_inUSD',
'BvD_Proxy_Total_Debt_inUSD',
'BvD_Proxy_Capitalization_inUSD',
'BvD_Proxy_Net_Op_Profit_inUSD',
'BvD_Proxy_Total_Liabilities_inUSD',
'BvD_Proxy_Tangible_Net_Worth_inUSD',
'LC_UBEBITDA_to_NS',
'LC_TD_to_UBEBITDA',
'LC_Total Assets',
'LC_TD_to_Capt',
'LC_UBEBITDA_to_IE',
'MM_NOP_to_NS',
'MM_TD_to_UBEBITDA',
'MM_TD_to_Capt',
'MM_ECE_to_TL',
'MM_TangNW_to_TA',
'size@Union Bank EBITDA',
'size@Capitalization',
'ds@Interest Expense',
'UBEBITDA',
'Capt',
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'bs@TD_to_TA_exc_TL',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']



list_sd=[]
list_sign = []
list_qua=[]
N=len(dat)
for name in bvd_cols:
    sd = np.abs(SomersD(dat['Applied Rating'], dat[name]))
    list_sign.append(np.sign(sd))
    list_sd.append(np.abs(sd))
    list_qua.append(dat[name].count()/N)

result = pd.DataFrame()
result['Factor'] = bvd_cols
result['SomersD'] = list_sd
result['Relationship'] = list_sign
result['DataQuality'] = list_qua


result.sort_values(by='SomersD',ascending=False, inplace=True)
result.reset_index(drop=True, inplace=True)

result.to_excel('__investigation_BvD_SFA.xlsx')

result2 = result.drop_duplicates(subset=['SomersD'])





#%% Compustat
dat = dfc.query('ratingYear>=2010')

int_ratings_mapping={
'AAA':1,
'AA+':2,
'AA':3,
'AA-':4, 
'A+':5,
'A':6,
'A-':7,
'BBB+':8,
'BBB':9,
'BBB-':10,
'BB+':11,
'BB':12,  
'BB-':13,
'B+':14,
'B':15,
'B-':16,
'CCC':17,
'CC':18,
'CCC-':19,
'CCC+':20,
'D':21,
'SD':21
}

dat['ExternalRating_int'] = dat['ratingSymbol']
dat['ExternalRating_int'] = dat['ExternalRating_int'].replace(int_ratings_mapping)
dat['ExternalRating_int'] = pd.to_numeric(dat['ExternalRating_int'], errors='coerce')
dat.dropna(subset=['ExternalRating_int'], inplace=True)



cs_cols=[
'CEQ',
'SEQ',
'IB',
'NI',
'EBIT',
'EBITDA',
'OIBDP',
'AT',
'RE',
'OANCF',
'LT',
'GP',
'TXT',
'LCT',
'ACT',
'RECT',
'SALE',
'REVT',
'AP',
'COGS',
'DLTT',
'CHE',
'CH',
'DP',
'INTAN',
'EMP',
'CAPX',
'DD1',
'XINT',
'INVT',
'NOPI',
'WCAP',
'ac__Net_Sales_by_Curr_Liab',
'ac__Net_Sales_by_TNW',
'ac__Net_Sales_by_Total_Assets',
'ac__Net_Sales_by_Acct_Receivable',
'ac__Net_Sales_by_Inventory',
'ac__COGS_by_Acct_Payable',
'ac__COGS',
'bs__Total_Debt_by_Total_Asset',
'bs__Total_Debt_by_TNW',
'bs__Total_Debt_by_Working_Capital',
'bs__Total_Debt_by_Capitalization_mm3_lc4',
'bs__LT_Debt_by_Adj_Capitalization',
'bs__LT_Debt_by_TNW',
'bs__Total_Liab_by_TNW',
'bs__LTD_by_LTD_and_TNW',
'bs__Total_Debt_by_Equity',
'bs__Total_Liab_by_Total_Asset',
'bs__Curr_Liab_by_Total_Asset',
'bs__Curr_Liab_by_TNW',
'bs__Curr_Liab_by_Total_Liab',
'bs__Curr_Liab_by_Capitalization',
'bs__Curr_Liab_by_Adj_Capitalization',
'bs__Total_Liab_by_Equity',
'bs__Net_Debt_by_EBITDA',
'bs__Total_Debt_by_EBITDA',
'bs__Total_Asset_by_Equity',
'bs__Total_Debt_by_TD_Equity_major6',
'bs__Total_Debt_by_TD_Equity_major9',
'bs__Net_Worth_by_Total_Asset_other4',
'cf__Total_Debt_by_NI_and_DA',
'cf__Total_Debt_by_EBITDA_mm2_lc2',
'cf__Total_Debt_by_EBIT',
'cf__Total_Debt_by_Net_Income',
'cf__Total_Debt_by_Net_Op_Cash_Flow',
'cf__Net_Debt_by_Net_Income',
'cf__Net_Debt_by_Net_Op_Cash_Flow',
'cf__LT_Debt_by_NI_and_DA',
'cf__LT_Debt_by_EBITDA',
'cf__LT_Debt_by_EBIT',
'cf__LT_Debt_by_Net_Income',
'cf__LT_Debt_by_Net_Op_Cash_Flow',
'ds__NI_and_Int_Exp_by_Int_Exp',
'ds__Net_Income_by_Int_Exp',
'ds__EBITDA_by_Int_Exp_lc5_other3',
'ds__EBIT_by_Int_Exp_major3',
'ds__Net_Op_Cash_Flow_by_Int_Exp',
'ds__Net_Income_by_Curr_Liab',
'ds__Net_Income_by_Total_Liab',
'ds__EBIT_minus_CAPEX_by_Int_Exp',
'ds__EBITDA_minus_CAPEX_by_Int_Exp',
'ds__Net_Sales_by_Int_Exp',
'ds__Total_Liab_by_Int_Exp',
'ds__Int_Exp',
'ds__FFO_Interest_Coverage_major4',
'ds__Op_Cash_by_Debt_major5',
'lq__Retained_Earn_by_Curr_Liab',
'lq__Cash_by_Total_Debt',
'lq__Cash_and_Cash_Equiv_by_Total_Debt',
'lq__Cash_and_Cash_Equiv_by_Total_Liab_mm4',
'lq__Cash_and_Cash_Equiv_by_Total_Assets',
'lq__Curr_Assets_excl_Inventory_by_Total_Debt',
'lq__Curr_Assets_excl_Inventory_by_Total_Liab',
'lq__Cash_and_Cash_Equiv_by_Curr_Liab',
'lq__Cash_and_Cash_Equiv_by_Curr_Assets',
'lq__Curr_Assets_by_Total_Liab',
'lq__Curr_Assets_minus_Liab_by_Total_Debt',
'lq__Quick_Ratio_major8',
'lq__Curr_Assets_by_Total_Assets',
'lq__Curr_Assets_minus_Curr_Liab_by_Curr_Assets',
'lq__Curr_Assets_excl_Inventory_by_Total_Assets',
'lq__Net_Sales_by_Working_Cap',
'lq__Curr_Assets_minus_Curr_Liab_by_Curr_Liab',
'lq__Current_Ratio',
'pf__ROE',
'pf__ROA',
'pf__Net_Inc_Before_Extraordinary_Items',
'pf__Net_Inc_Before_Extra_Items_by_Total_Assets',
'pf__Net_Profit_Margin',
'pf__EBIT_by_Capitalization',
'pf__EBITDA_by_Capitalization',
'pf__EBIT_by_Total_Assets',
'pf__EBITDA_by_Total_Assets',
'pf__EBITDA_by_Tangible_Assets',
'pf__EBITDA_by_Net_Sales_lc1',
'pf__Retained_Earn_by_TNW',
'pf__Net_Op_Income_by_TNW',
'pf__Net_Op_Income_by_Net_Worth',
'pf__Net_Op_Income_by_Net_Sales_mm1',
'pf__OIBD_by_Rev_major1',
'pf__Return_on_Capital_major2',
'sz__Net_Income',
'sz__Retained_Earnings',
'sz__EBIT',
'sz__EBITDA',
'sz__Net_Sales_other10',
'sz__Total_Curr_Assets',
'sz__Tangible_Assets',
'sz__Net_Op_Income',
'sz__TNW_by_Total_Assets_mm5',
'sz__TNW_by_Non_Curr_Assets',
'sz__Net_Worth_by_Non_Curr_Assets',
'sz__Tangible_Net_Worth',
'sz__Cash',
'sz__Cash_and_Cash_Equiv',
'sz__Working_Capital',
'sz__Net_Worth_other9',
'sz__Total_Assets_lc3_major7',
'sz__Total_Inventory',
'sz__Total_Debt',
'sz__Capitalization',
'sz__Capital_Expenditure',
'sz__Total_Curr_Liab',
'sz__Total_Liab',
'sz__Common_Equity',
'sz__Income_Taxes',
'sz__Revenue',
'sz__Employees',
'new__COGS_by_Net_Sales',
'new__Gross_Markup',
'new__Gross_Profit_Margin',
'new__Gross_Profit_by_Net_Sales',
'new__Gross_Margin_Ratio',
'new__Net_Sales_Revenue',
'new__Op_Cash_Flow_by_Curr_Liab',
'new__Op_Cash_Flow_by_Net_Sales',
'new__Asset_Turnover_Ratio',
'new__Depre_Amort_by_Net_Sales',
'new__Working_Capital',
'new__Funds_From_Operation']



list_sd=[]
list_sign = []
list_qua=[]
N=len(dat)
for name in cs_cols:
    sd = np.abs(SomersD(dat['ExternalRating_int'], dat[name]))
    list_sign.append(np.sign(sd))
    list_sd.append(np.abs(sd))
    list_qua.append(dat[name].count()/N)

result = pd.DataFrame()
result['Factor'] = cs_cols
result['SomersD'] = list_sd
result['Relationship'] = list_sign
result['DataQuality'] = list_qua


result.sort_values(by='SomersD',ascending=False, inplace=True)
result.reset_index(drop=True, inplace=True)

result.to_excel('__investigation_Compustat_SFA.xlsx')


#%%
cols=[
'sz__Capitalization',
'sz__Total_Assets_lc3_major7',
'ds__Total_Liab_by_Int_Exp',
'COGS',
]
list_sd=[]
list_sd_count = [];list_sd_count_LC = [];list_sd_count_MM = []

for name in cols:
    dat2 = dat.dropna(subset=['ExternalRating_int',name])
    sd = SomersD(dat2['ExternalRating_int'], dat2[name])
    list_sd.append(np.abs(sd))
    list_sd_count.append(len(dat2))
    list_sd_count_LC.append(dat2.query('portfolio=="LC"')[name].count())
    list_sd_count_MM.append(dat2.query('portfolio=="MM"')[name].count())

result = pd.DataFrame()
result['Factor'] = cols
result['SomersD'] = list_sd
result['count'] = list_sd_count
result['count_LC'] = list_sd_count_LC
result['count_MM'] = list_sd_count_MMimport os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered3.pkl')

cols=[
'Data From',
 'Application No.',
 'Borrower CIF',
 'Borrower_CIF_cleaned',
 'Borrower Name',
 'Borrower Country',
 'Borrower Type',
 'Borrower Type 2',
 'Borrower Office Code',
 'Borrower Office Name',
 'timestamp',
 'year',
 'Current Total Score',
 'Primary Evaluation',
 'Secondary Evaluation',
 'Third Evaluation',
 'Final Result of Evaluation based on Financial Substance Score',
 'Applied Rating',
 'Borrower Industry',
 'Industry Model',
 'Current Financial Data #1',
 'Current Financial Data #2',
 'Current Financial Data #3',
 'Current Financial Data #4',
 'Current Financial Data #5',
 'Current Financial Data #6',
 'Current Financial Data #7',
 'Current Financial Data #8',
 'Current Financial Data #9',
 'Current Financial Data #10',
 "External Rating (Moody's)",
 'External Rating (R&I)',
 'External Rating (S&P)'
 ]

dat=dat[cols]


#%% Drop Borrowers who have unique "CIF" but multiple(>=3) "Names"
tmp = dat.groupby('Borrower_CIF_cleaned')['Borrower Name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts(normalize=True)
'''
1    0.657148
2    0.255707
3    0.063053
4    0.017100
5    0.005476
6    0.001011
7    0.000421
9    0.000042
8    0.000042
Name: Num_CusName, dtype: float64

'''

list_toremove = tmp[tmp.Num_CusName>=3].index.tolist()
mask = dat.Borrower_CIF_cleaned.isin(list_toremove)
dat = dat[~mask].reset_index(drop=True)




#%% tag Geo info
dat['Geo'] = 'Asia'
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(gcar_Americas_office_code), 'Americas')
dat['Geo'] = dat['Geo'].mask(dat['Borrower Office Code'].isin(gcar_EMEA_office_code), 'EMEA')


dat['BTMU Rating'] = dat['Applied Rating'].transform(lambda x: str(x)[0]+'-'+str(x)[1])
dat['BTMU Rating'].value_counts().sort_index() 




#%% convert to numeric value
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']

gcar_converter(dat, curr, inplace=True)





df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')


bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


df_bvd['Fiscal year'].value_counts().sort_index()


dat['year-1'] = dat['year']-1






#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% relink: Year (N-1)'s BvD links to Year N's GCARS data
df_merged = pd.merge(dat, df_bvd, left_on=['Borrower_CIF_cleaned','year-1'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_merged['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_merged['BvD_CIF']]
df_merged['inBvD'].value_counts()


#%% Drop Browers who have unique "BvD_CIF" but multiple(>=3) "Names"
tmp = df_merged.groupby('BvD_CIF')['BvD_Company name'].nunique().to_frame('Num_CusName')
tmp['Num_CusName'].value_counts().sort_index()
'''
0    2495
1    5646
2     113
3       7
Name: Num_CusName, dtype: int64

'''

list_toremove = tmp[tmp.Num_CusName>=3].index.tolist()
mask = df_merged.BvD_CIF.isin(list_toremove)
df_merged = df_merged[~mask].reset_index(drop=True)



import pickle
lmdb = 0.3
model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']




dat = df_merged.copy()



cols_dollar=[
'BvD_Intangible fixed assets',
 'BvD_Tangible fixed assets',
 'BvD_Other fixed assets',
 'BvD_Current assets',
 'BvD_Stock',
 'BvD_Debtors',
 'BvD_Other current assets',
 'BvD_Shareholders funds',
 'BvD_Capital',
 'BvD_Other shareholders funds',
 'BvD_Non-current liabilities',
 'BvD_Long term debt',
 'BvD_Other non-current liabilities',
 'BvD_Provisions (Liabilities & Equity)',
 'BvD_Current liabilities',
 'BvD_Loans (Liabilities & Equity)',
 'BvD_Creditors',
 'BvD_Other current liabilities',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Operating revenue (Turnover)',
 'BvD_Costs of goods sold',
 'BvD_Gross profit',
 'BvD_Other operating expenses',
 'BvD_Financial P/L',
 'BvD_Financial revenue',
 'BvD_Financial expenses',
 'BvD_P/L before tax',
 'BvD_Taxation',
 'BvD_P/L after tax',
 'BvD_Extr. and other P/L',
 'BvD_Extr. and other revenue',
 'BvD_Extr. and other expenses',
 'BvD_P/L for period [=Net income] ',
 'BvD_Material costs',
 'BvD_Costs of employees',
 'BvD_Depreciation & Amortization',
 'BvD_Other operating items',
 'BvD_Research & Development expenses',
 'BvD_Cash flow',
 'BvD_Added value',
]
# change to USD
for name in cols_dollar:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']



#%% build CnI ratios
dat['LC_UBEBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['LC_TD_to_UBEBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['LC_Total Assets'] = np.log(1+dat['BvD_Proxy_Total_Assets_inUSD'] )
dat['LC_TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['LC_UBEBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['MM_NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['MM_TD_to_UBEBITDA'] = dat['LC_TD_to_UBEBITDA']
dat['MM_TD_to_Capt'] = dat['LC_TD_to_Capt'] 
dat['MM_ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['MM_TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
# for invalid_neg
dat['size@Union Bank EBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['size@Capitalization'] = dat['BvD_Proxy_Capitalization_inUSD']
dat['ds@Interest Expense'] = dat['BvD_Proxy_Interest_expense_inUSD']
dat['UBEBITDA'] = dat['BvD_Proxy_UBEBITDA_inUSD']
dat['Capt'] = dat['BvD_Proxy_Capitalization_inUSD']


# MM new transformation
dat['MM_ECE_to_TL'][dat['MM_ECE_to_TL'] < 0 ] = 0
dat['MM_ECE_to_TL'] = (pow(dat['MM_ECE_to_TL'], lmdb) - 1)/lmdb



#%% build other CnI ratios
dat['cf@TD_to_ACF'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Cash flow']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['liq@CA_exc_CL_to_TD'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Debt_inUSD']
dat['liq@CA_exc_CL_to_TL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_exc_CL_to_TA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@CA_exc_CL_to_CA'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current assets']
dat['liq@CA_exc_CL_to_CL'] = (dat['BvD_Current assets'] - dat['BvD_Current liabilities']) / dat['BvD_Current liabilities']
dat['size@TangNW_to_TA_exc_CA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / (dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Current assets'])
dat['bs@TL_to_TL_exc_CL'] =  dat['BvD_Proxy_Total_Liabilities_inUSD']/(dat['BvD_Proxy_Total_Liabilities_inUSD']-dat['BvD_Current liabilities'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['ds@GP_and_IE_to_IE'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Interest_expense_inUSD']
dat['ds@GP_and_IE_to_TL'] = (dat['BvD_Gross profit'] + dat['BvD_Proxy_Interest_expense_inUSD']) / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['prof@TangA_to_NS'] = dat['BvD_Tangible fixed assets'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['size@TangNW_to_TA'] = dat['BvD_Proxy_Tangible_Net_Worth_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['BTMU@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['BTMU@EBITDA_to_IE'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Current assets']-dat['BvD_Current liabilities'])
dat['bs@TD_to_TA_exc_TL'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Proxy_Total_Assets_inUSD']-dat['BvD_Proxy_Total_Liabilities_inUSD'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] = dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_to_GP_and_Dep_and_Amo'] =  dat['BvD_Proxy_Total_Debt_inUSD']/(dat['BvD_Gross profit']+dat['BvD_Depreciation & Amortization'])
dat['cf@TD_NOP'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Net_Op_Profit_inUSD']
dat['BTMU@OP_to_Sales'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['cf@TD_to_EBITDA'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_UBEBITDA_inUSD']
dat['prof@EBITDA_to_Capt'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@EBITDA_to_NS'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@EBITDA_to_TangA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Tangible fixed assets']
dat['prof@EBITDA_to_TA'] = dat['BvD_Proxy_UBEBITDA_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['act@NS_to_CL'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Current liabilities']
dat['act@NS_to_TA'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TD_to_Capt'] = dat['BvD_Proxy_Total_Debt_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_Capt'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@CL_to_Capt'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['prof@NOP_to_NS'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['prof@NOP_to_TangNW'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['prof@NOP_to_TA'] = dat['BvD_Proxy_Net_Op_Profit_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TA'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@TLTD_to_TangNW'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TLTD_to_Capt'] = dat['BvD_Long term debt'] / dat['BvD_Proxy_Capitalization_inUSD']
dat['bs@TL_to_TangNW'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Tangible_Net_Worth_inUSD']
dat['bs@TL_to_TA'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['bs@CL_to_TL'] = dat['BvD_Current liabilities'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['ds@TL_to_IE'] = dat['BvD_Proxy_Total_Liabilities_inUSD'] / dat['BvD_Proxy_Interest_expense_inUSD']
dat['liq@ECE_to_TL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['liq@CA_to_TL'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Liabilities_inUSD']
dat['bs@TD_to_CA_exc_CL'] = dat['BvD_Proxy_Total_Debt_inUSD'] / (dat['BvD_Current assets'] - dat['BvD_Current liabilities'])
dat['liq@ECE_to_CA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current assets']
dat['liq@CA_to_TA'] = dat['BvD_Current assets'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_TD'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Debt_inUSD']    
dat['liq@ECE_to_TA'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Proxy_Total_Assets_inUSD']
dat['liq@ECE_to_CL'] = dat['BvD_Proxy_Ending_Cash_Equiv_inUSD'] / dat['BvD_Current liabilities']


#%% build new ratios
dat['new@COGSRatio'] = dat['BvD_Costs of goods sold'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossMarkup'] = dat['BvD_Gross profit'] / dat['new@COGSRatio']
dat['new@GrossProfitMargin'] = (dat['BvD_Proxy_Net_Sales_inUSD']-dat['BvD_Costs of goods sold']) / dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@GrossProfitRatio'] = dat['BvD_Gross profit'] / dat['BvD_Proxy_Net_Sales_inUSD']
dat['newprof@GrossMarginRatio'] = (dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']) / dat['BvD_Gross profit']
dat['new@NetSalesRevenue'] = dat['BvD_Gross profit'] - dat['BvD_Costs of goods sold']
dat['new@CashFlowRatio'] = dat['BvD_Cash flow']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@AssetEfficiencyRatio'] = dat['BvD_Cash flow']/ dat['LC_Total Assets']
dat['new@CLCoverageRatio'] = dat['BvD_Cash flow']/ dat['BvD_Current liabilities']
dat['new@LTDCoverageRatio'] = dat['BvD_Cash flow']/dat['BvD_Long term debt']
dat['new@InterestCoverageRatio'] = (dat['BvD_Cash flow'] + dat['BvD_Taxation']+dat['ds@Interest Expense']) /dat['ds@Interest Expense']
dat['new@DA_to_Sales'] = dat['BvD_Depreciation & Amortization']/ dat['BvD_Proxy_Net_Sales_inUSD']
dat['new@CostPerEmployee'] = dat['BvD_Costs of employees']/dat['BvD_Number of employees']
dat['new@IFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Intangible fixed assets']
dat['new@TFATurnover'] = dat['BvD_Proxy_Net_Sales_inUSD'] / dat['BvD_Tangible fixed assets']



factor_list = cols_dollar + ['BvD_Number of employees'] + \
model_LC.quant_factor + model_MM.quant_factor + [
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'BTMU@EBITDA_to_IE',
'bs@TD_to_CA_exc_CL',
'bs@TD_to_TA_exc_TL',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_to_GP_and_Dep_and_Amo',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'bs@TD_to_CA_exc_CL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',] + [
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']

# fill inf
for factor in factor_list:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))



#%% build current tokyo model factors:
curr=[
'Current Financial Data #1', 
'Current Financial Data #2', 'Current Financial Data #3',
'Current Financial Data #4', 'Current Financial Data #5',
'Current Financial Data #6', 'Current Financial Data #7',
'Current Financial Data #8', 'Current Financial Data #9',
'Current Financial Data #10']
# fill inf
for factor in curr:                       
        dat[factor] = dat[factor].clip(
            np.nanmin(dat[factor][dat[factor] != -np.inf]), 
            np.nanmax(dat[factor][dat[factor] != np.inf]))


# rename current factor

for name in curr:
    dat['Other_'+name] = np.nan
    dat['SP_'+name] = np.nan

    dat['Other_'+name] = dat['Other_'+name].mask(dat['Borrower Type']=="Corporation (Other Company)", dat[name])
    dat['SP_'+name] = dat['SP_'+name].mask(dat['Borrower Type']!="Corporation (Other Company)", dat[name])

dat.drop(columns=curr, inplace=True)


#%% label geo info
dat = pd.concat([dat, pd.get_dummies(dat['Geo'], prefix='Geo')], axis=1)

#%% label tokyo model info
dat = pd.concat([dat, pd.get_dummies(dat['Borrower Type'], prefix='Tokyo')], axis=1)




#%% add portfolio info
dat['portfolio'] = 'unknown'
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']>1e9, 'LC')
dat['portfolio'] = dat['portfolio'].mask(dat['BvD_Proxy_Net_Sales_inUSD']<=1e9, 'MM')

dat['portfolio'].value_counts()

















#%%
#####################################################################
dat = dat.query('year>=2010')
dat2 = dat.query('inBvD=="Yes"')
dat2.dropna(subset=['Applied Rating'], inplace=True)

bvd_cols=[
'BvD_Fixed assets',
'BvD_Intangible fixed assets',
'BvD_Tangible fixed assets',
'BvD_Other fixed assets',
'BvD_Current assets',
'BvD_Stock',
'BvD_Debtors',
'BvD_Other current assets',
'BvD_Cash & cash equivalent',
'BvD_Total assets',
'BvD_Shareholders funds',
'BvD_Capital',
'BvD_Other shareholders funds',
'BvD_Non-current liabilities',
'BvD_Long term debt',
'BvD_Other non-current liabilities',
'BvD_Provisions (Liabilities & Equity)',
'BvD_Current liabilities',
'BvD_Loans (Liabilities & Equity)',
'BvD_Creditors',
'BvD_Other current liabilities',
'BvD_Total shareh. funds & liab.',
'BvD_Working capital',
'BvD_Net current assets',
'BvD_Enterprise value',
'BvD_Number of employees',
'BvD_Operating revenue (Turnover)',
'BvD_Sales',
'BvD_Costs of goods sold',
'BvD_Gross profit',
'BvD_Other operating expenses',
'BvD_Operating P/L [=EBIT]',
'BvD_Financial P/L',
'BvD_Financial revenue',
'BvD_Financial expenses',
'BvD_P/L before tax',
'BvD_Taxation',
'BvD_P/L after tax',
'BvD_Extr. and other P/L',
'BvD_Extr. and other revenue',
'BvD_Extr. and other expenses',
'BvD_P/L for period [=Net income] ',
'BvD_Export revenue',
'BvD_Material costs',
'BvD_Costs of employees',
'BvD_Depreciation & Amortization',
'BvD_Other operating items',
'BvD_Interest paid',
'BvD_Research & Development expenses',
'BvD_Cash flow',
'BvD_Added value',
'BvD_EBITDA',
'BvD_Proxy_Total_Assets',
'BvD_Proxy_Interest_expense',
'BvD_Proxy_Ending_Cash_Equiv',
'BvD_Proxy_UBEBITDA',
'BvD_Proxy_Net_Sales',
'BvD_Proxy_Total_Debt',
'BvD_Proxy_Capitalization',
'BvD_Proxy_Net_Op_Profit',
'BvD_Proxy_Total_Liabilities',
'BvD_Proxy_Tangible_Net_Worth',
'BvD_Proxy_Total_Assets_inUSD',
'BvD_Proxy_Interest_expense_inUSD',
'BvD_Proxy_Ending_Cash_Equiv_inUSD',
'BvD_Proxy_UBEBITDA_inUSD',
'BvD_Proxy_Net_Sales_inUSD',
'BvD_Proxy_Total_Debt_inUSD',
'BvD_Proxy_Capitalization_inUSD',
'BvD_Proxy_Net_Op_Profit_inUSD',
'BvD_Proxy_Total_Liabilities_inUSD',
'BvD_Proxy_Tangible_Net_Worth_inUSD',
'LC_UBEBITDA_to_NS',
'LC_TD_to_UBEBITDA',
'LC_Total Assets',
'LC_TD_to_Capt',
'LC_UBEBITDA_to_IE',
'MM_NOP_to_NS',
'MM_TD_to_UBEBITDA',
'MM_TD_to_Capt',
'MM_ECE_to_TL',
'MM_TangNW_to_TA',
'size@Union Bank EBITDA',
'size@Capitalization',
'ds@Interest Expense',
'UBEBITDA',
'Capt',
'cf@TD_to_ACF',
'bs@TD_to_CA_exc_CL',
'liq@CA_exc_CL_to_TD',
'liq@CA_exc_CL_to_TL',
'liq@CA_exc_CL_to_TA',
'liq@CA_exc_CL_to_CA',
'liq@CA_exc_CL_to_CL',
'size@TangNW_to_TA_exc_CA',
'bs@TL_to_TL_exc_CL',
'cf@TD_to_GP_and_Dep_and_Amo',
'ds@GP_and_IE_to_IE',
'ds@GP_and_IE_to_TL',
'prof@TangA_to_NS',
'size@TangNW_to_TA',
'BTMU@EBITDA_to_IE',
'BTMU@TD_to_EBITDA',
'bs@TD_to_TA_exc_TL',
'cf@TD_NOP',
'BTMU@OP_to_Sales',
'cf@TD_to_EBITDA',
'prof@EBITDA_to_Capt',
'prof@EBITDA_to_NS',
'prof@EBITDA_to_TangA',
'prof@EBITDA_to_TA',
'act@NS_to_CL',
'act@NS_to_TA',
'bs@TD_to_Capt',
'bs@TL_to_Capt',
'bs@CL_to_Capt',
'prof@NOP_to_NS',
'prof@NOP_to_TangNW',
'prof@NOP_to_TA',
'bs@TLTD_to_TA',
'bs@TLTD_to_TangNW',
'bs@TLTD_to_Capt',
'bs@TL_to_TangNW',
'bs@TL_to_TA',
'bs@CL_to_TL',
'ds@TL_to_IE',
'liq@ECE_to_TL',
'liq@CA_to_TL',
'liq@ECE_to_CA',
'liq@CA_to_TA',
'liq@ECE_to_TD',
'liq@ECE_to_TA',
'liq@ECE_to_CL',
'new@COGSRatio',
'new@GrossMarkup',
'new@GrossProfitMargin',
'new@GrossProfitRatio',
'newprof@GrossMarginRatio',
'new@NetSalesRevenue',
'new@CashFlowRatio',
'new@AssetEfficiencyRatio',
'new@CLCoverageRatio',
'new@LTDCoverageRatio',
'new@InterestCoverageRatio',
'new@DA_to_Sales',
'new@CostPerEmployee',
'new@IFATurnover',
'new@TFATurnover']



list_sd=[]
list_sign = []
list_qua=[]
N=len(dat)
for name in bvd_cols:
    sd = np.abs(SomersD(dat['Applied Rating'], dat[name]))
    list_sign.append(np.sign(sd))
    list_sd.append(np.abs(sd))
    list_qua.append(dat[name].count()/N)

result = pd.DataFrame()
result['Factor'] = bvd_cols
result['SomersD'] = list_sd
result['Relationship'] = list_sign
result['DataQuality'] = list_qua


result.sort_values(by='SomersD',ascending=False, inplace=True)
result.reset_index(drop=True, inplace=True)

result.to_excel('__investigation_BvD_SFA.xlsx')

result2 = result.drop_duplicates(subset=['SomersD'])



#%%
from PDScorecardTool.Process import SomersD

cols=[
'BvD_Capital',
'BvD_Total assets',
'ds@TL_to_IE',
'new@COGSRatio',]

int_ratings_mapping={
'AAA':1,
'AA+':2,
'AA':3,
'AA-':4, 
'A+':5,
'A':6,
'A-':7,
'BBB+':8,
'BBB':9,
'BBB-':10,
'BB+':11,
'BB':12,  
'BB-':13,
'B+':14,
'B':15,
'B-':16,
'CCC+~':17,
}

dat2['ExternalRating_int'] = dat2['External Rating (S&P)']
dat2['ExternalRating_int'] = dat2['ExternalRating_int'].replace(int_ratings_mapping)
dat2['ExternalRating_int'] = pd.to_numeric(dat2['ExternalRating_int'], errors='coerce')
dat2['ExternalRating_int'].value_counts()


list_sd=[]
list_sd2=[]
list_sd_count = [];list_sd_count_LC = [];list_sd_count_MM = []
list_sd2_count = [];list_sd2_count_LC = [];list_sd2_count_MM = []


for name in cols:
    sd = np.abs(SomersD(dat2['Applied Rating'], dat2[name]))
    list_sd.append(np.abs(sd))
    list_sd_count.append(dat2[name].count())
    list_sd_count_LC.append(dat2.query('portfolio=="LC"')[name].count())
    list_sd_count_MM.append(dat2.query('portfolio=="MM"')[name].count())

    dat3 = dat2.dropna(subset=['ExternalRating_int',name])
    list_sd2.append(np.abs(SomersD(dat3['ExternalRating_int'], dat3[name]))) 
    list_sd2_count.append(dat3[name].count())
    list_sd2_count_LC.append(dat3.query('portfolio=="LC"')[name].count())
    list_sd2_count_MM.append(dat3.query('portfolio=="MM"')[name].count())



result = pd.DataFrame()
result['Factor'] = cols
result['SomersD'] = list_sd
result['count'] = list_sd_count
result['count_LC'] = list_sd_count_LC
result['count_MM'] = list_sd_count_MM
result['SomersD2'] = list_sd2
result['count2'] = list_sd2_count
result['count2_LC'] = list_sd2_count_LC
result['count2_MM'] = list_sd2_count_MM



#%%

df_all = dat.query('inBvD=="Yes"').dropna(subset=['External Rating (S&P)'])


df_lc = df_all.query('portfolio=="LC"')
df_mm = df_all.query('portfolio=="MM"')

df_all['Borrower_CIF_cleaned'].nunique()  
# 887
df_lc['Borrower_CIF_cleaned'].nunique()  
# 496
df_mm['Borrower_CIF_cleaned'].nunique()  
# 240

dat['Borrower_CIF_cleaned'].nunique()  
# 21456


#%%
dat['Borrower Name'].nunique()  
 26257

'''
GCARS data have about 21,456 unique customers (by CIF).
Among them, 887 unique customers have both ‘BvD financials’ and ‘External Rating S&P’.
The pct is 887/21456 = 4.1%.

In other words, only 4.1% of current 2 Tokyo models’ customers have BvD financials and external rating. 
Considering the data quality of BvD financial and constructed ratios, the pct will be lower than 4% or 
even lower than 3%, which makes it weak in representativeness for current portfolio.

And I also found the Compustat data can cover about 2.3% of GCARS customer names(which is not far less than 3-4% ). 
Note that, Compustat data is mainly for US/CAN companies.



'''
# -*- coding: utf-8 -*-
"""
Created on Tue Sep  8 10:40:51 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')

#%% CBM
#df_cb = pd.read_excel(r'C:\Users\ub71894\Documents\Data\CreditBenchmark Ratings\60001325_31AUG20_07SEP20_ClientHistory_Tokyo_Universe.xlsx')
#df_cb.to_pickle(r'C:\Users\ub71894\Documents\Data\CreditBenchmark Ratings\60001325_31AUG20_07SEP20_ClientHistory_Tokyo_Universe.pkl')
df_cb = pd.read_pickle(r'C:\Users\ub71894\Documents\Data\CreditBenchmark Ratings\60001325_31AUG20_07SEP20_ClientHistory_Tokyo_Universe.pkl')
df_cb['CIF_cleaned'] = gcar_cif_cleaner(df_cb, col='CIF_cleaned')
df_cb['timestamp'] = pd.to_datetime(df_cb[ 'CB_Effective_Date'])
df_cb['year'] = [x.year for x in df_cb.timestamp]
df_cb.drop_duplicates(subset=['CIF_cleaned','year'], inplace=True)

dict_cb={
'aaa':1,     
'aa+':2,    
'aa':3,      
'aa-':4,     
'a+':5,      
'a':6,       
'a-':7,      
'bbb+':8,    
'bbb':9,     
'bbb-':10,    
'bb+':11,     
'bb':12,      
'bb-':13,     
'b+':14,      
'b':15,       
'b-':16,      
'ccc+':17,    
'ccc':18,     
'ccc-':19,    
'cc':20,      
}
df_cb['rating_in_notch'] = df_cb['CB_Consensus'].replace(dict_cb)



#%% LC
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat = pd.merge(df, df_cb, left_on=['Borrower_CIF_cleaned','year'], right_on=['CIF_cleaned','year'], how='inner')


#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()
'''
0    620
1    158
Name: Gua_override, dtype: int64
'''

mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

#dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
#dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])
#
#dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,102:82})
#dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])


#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['LC_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['LC_PD'] = list_PD
    dat['LC_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_LC_norm = get_proxy_rating(dat_LC_norm, model_LC)
# 507


print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['LC_PDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Primary Evaluation']))
#print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['PrimaryEvaluationPDRR']))
#print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Applied Rating PDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Applied Rating']))
0.3287446699844405
0.3826575189423779
0.5671849203615502


print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['LC_PDRR']))
print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['Primary Evaluation']))
#print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['PrimaryEvaluationPDRR']))
#print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['Applied Rating PDRR']))
print(SomersD(dat_LC_norm['Risk_Entity_PD_Median_Proxy'], dat_LC_norm['Applied Rating']))

0.26526590230233105
0.32259259716681693
0.4549746979665993

dat_LC_norm['Borrower Type'].value_counts()

'''
Corporation (Major Company)    399
Corporation (Other Company)    108
Name: Borrower Type, dtype: int64
'''


#  2020/12/28  to response Hera
cols=[
'Borrower Name','Borrower_CIF_cleaned','year','Primary Evaluation','Applied Rating',
'CB_Consensus','rating_in_notch','LC_PDRR']

df_out = dat_LC_norm[cols].copy()
print(SomersD(df_out['rating_in_notch'], df_out['LC_PDRR']))
print(SomersD(df_out['rating_in_notch'], df_out['Primary Evaluation']))

df_out.to_excel('LC_portfolio.xlsx')



#%% MM
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')

dat = pd.merge(df, df_cb, left_on=['Borrower_CIF_cleaned','year'], right_on=['CIF_cleaned','year'], how='inner')


#%% treat Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()
'''
0    540
1    202
Name: Gua_override, dtype: int64
'''

mask = dat.Gua_override==1
dat['Applied Rating'] = dat['Applied Rating'].mask(mask, dat['Secondary Evaluation'] )



#%% Mapping BTMU rating to PDRR
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))

#dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].replace({80:81})
#dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])
#
#dat['Applied Rating PDRR'] = dat['Applied Rating'].replace({83:82, 90:82,102:82})
#dat['Applied Rating PDRR'] = dat['Applied Rating PDRR'].transform(lambda x: pd_mapping[x])





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['MM_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['MM_PD'] = list_PD
    dat['MM_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_MM_norm = get_proxy_rating(dat_MM_norm, model_MM)
# 486


print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['MM_PDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Primary Evaluation']))
#print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Applied Rating PDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Applied Rating']))
0.22849042962060961
0.1673844624813649
0.3057384223143955


print(SomersD(dat_MM_norm['Risk_Entity_PD_Median_Proxy'], dat_MM_norm['MM_PDRR']))
print(SomersD(dat_MM_norm['Risk_Entity_PD_Median_Proxy'], dat_MM_norm['Primary Evaluation']))
#print(SomersD(dat_MM_norm['Risk_Entity_PD_Median_Proxy'], dat_MM_norm['Applied Rating PDRR']))
print(SomersD(dat_MM_norm['Risk_Entity_PD_Median_Proxy'], dat_MM_norm['Applied Rating']))

0.19218383828165833
0.16983275018776678
0.3114662890091548


dat_MM_norm['Borrower Type'].value_counts()
'''
Corporation (Other Company)    430
Corporation (Major Company)     56
Name: Borrower Type, dtype: int64

'''

#  2020/12/28  to response Hera
cols=[
'Borrower Name','Borrower_CIF_cleaned','year','Primary Evaluation','Applied Rating',
'CB_Consensus','rating_in_notch','MM_PDRR']

df_out = dat_MM_norm[cols].copy()
print(SomersD(df_out['rating_in_notch'], df_out['MM_PDRR']))
print(SomersD(df_out['rating_in_notch'], df_out['Primary Evaluation']))

df_out.to_excel('MM_portfolio.xlsx')



#%%
import seaborn as sns
import matplotlib.pyplot as plt

df_cb['PD_Contribution_Count'] = df_cb['PD_Contribution_Count'].replace({'MIN':4})
dat_LC_norm['PD_Contribution_Count'] = dat_LC_norm['PD_Contribution_Count'].replace({'MIN':4})
dat_MM_norm['PD_Contribution_Count'] = dat_MM_norm['PD_Contribution_Count'].replace({'MIN':4})

#  RLA plot
fig, ax = plt.subplots(1, 1, figsize=(8,6))
ax = sns.countplot(data=df_cb, x='PD_Contribution_Count', palette="crest")
values=df_cb['PD_Contribution_Count'].value_counts().sort_index().values
for i, p in enumerate(ax.patches):
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha="center")

ax.set_xticklabels([int(float(x.get_text())) for x in ax.get_xticklabels()])
ax.set_xlabel("Count of Contributors")


#  LC plot
fig, ax = plt.subplots(1, 1, figsize=(8,6))
ax = sns.countplot(data=dat_LC_norm, x='PD_Contribution_Count', palette="crest")
values=dat_LC_norm['PD_Contribution_Count'].value_counts().sort_index().values
for i, p in enumerate(ax.patches):
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha="center")

ax.set_xticklabels([int(float(x.get_text())) for x in ax.get_xticklabels()])
ax.set_xlabel("Count of Contributors")
plt.savefig('CBM_LC.png')


#  MM plot
fig, ax = plt.subplots(1, 1, figsize=(8,6))
ax = sns.countplot(data=dat_MM_norm, x='PD_Contribution_Count', palette="crest")
values=dat_MM_norm['PD_Contribution_Count'].value_counts().sort_index().values
for i, p in enumerate(ax.patches):
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2., height + 0.1, values[i],ha="center")

ax.set_xticklabels([int(float(x.get_text())) for x in ax.get_xticklabels()])
ax.set_xlabel("Count of Contributors")
plt.savefig('CBM_MM.png')

# -*- coding: utf-8 -*-
"""
Created on Mon Oct  5 12:15:24 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np
import seaborn as sns
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')

df_full = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\dat_2008-2019_cleaned_BvD_relink.pkl')
df_3yrmm = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')



#%% full data
dat = df_full.copy()

bvd_cols=[
 'BvD_Current assets',
 'BvD_Cash & cash equivalent',
 'BvD_Total assets',
 'BvD_Capital',
 'BvD_Working capital',
 'BvD_Net current assets',
 'BvD_Enterprise value',
 'BvD_Sales',
 'BvD_Gross profit',
 'BvD_Operating P/L [=EBIT]',
 'BvD_P/L for period [=Net income] ',
 'BvD_EBITDA']


# change to USD
for name in bvd_cols:
    dat[name] = dat[name] * dat['BvD_Exchange rate from original currency']




dat.dropna(subset=bvd_cols, how='all', inplace=True)
dat1 = dat[dat['Borrower Type']=='Corporation (Other Company)'].sample(2000)
#dat1 = dat[dat['Borrower Type']=='Corporation (Major Company)'].sample(2000)
dat1[name].describe(percentiles=[0.01,0.05,0.1,0.25,0.5,0.75,0.9,0.95,0.99])



for name in bvd_cols:
    dat2 = dat1[dat1[name]<dat1[name].quantile(.95)]
    sns.displot(data=dat2, x=name, hue='Borrower Type', kind='kde')




dat2 = dat1[dat1[name]<1e7]
#sns.displot(data=dat2, x=name, hue='Borrower Type', kde=True)


sns.histplot(data=dat2, x=name, hue='Borrower Type', bins=20)



name =  'BvD_Enterprise value'
# -*- coding: utf-8 -*-
"""
Created on Tue Nov  3 13:25:00 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import MAUG_mapping, NAICS_mapping
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
from PDScorecardTool.Process import SomersD
from PDScorecardTool.Process import func_sd, func_rla, func_ovd, google_color
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


df_ci_lc1 = pd.read_pickle(r'..\data\CnI_data\dat_lc_by1000_last.pkl.xz')
df_ci_mm1 = pd.read_pickle(r'..\data\CnI_data\dat_mm_by1000_last.pkl.xz')

df_ci_lc2 = pd.read_csv(r'..\data\CnI_data\dat_LC_201819.csv')
df_ci_mm2 = pd.read_csv(r'..\data\CnI_data\dat_MM_201819.csv')

df_lc = pd.concat([df_ci_lc1,df_ci_lc2], axis=0)
df_mm = pd.concat([df_ci_mm1,df_ci_mm2], axis=0)



df_lc = MAUG_mapping(df_lc)
df_lc = NAICS_mapping(df_lc)
df_lc['timestamp'] = pd.to_datetime(df_lc['archive_date'])

df_lc['Industry_by_MAUG'].value_counts()
data_lc = df_lc[df_lc['Industry_by_MAUG']=="Power & Utilities"]


df_mm = MAUG_mapping(df_mm)
df_mm = NAICS_mapping(df_mm)
df_mm['timestamp'] = pd.to_datetime(df_mm['archive_date'])

df_mm['Industry_by_MAUG'].value_counts()
data_mm = df_mm[df_mm['Industry_by_MAUG']=="Power & Utilities"]


#%%
df = pd.read_pickle(r'..\data\GCARS 2008-2019_matches3.pkl')
aa = df['Borrower Industry'].unique().tolist()


mask = (df['Borrower Industry']=='Electric Utility Services')|(df['Borrower Industry']=='Gas Utility Services')|(df['Borrower Industry']=='Heating Utilities')
data = df[mask].query('year==2018')


data['Borrower Industry'].value_counts()
'''

Electric Utility Services    319
Gas Utility Services          81
Heating Utilities              2
Name: Borrower Industry, dtype: int64
'''
data['Borrower Industry'].value_counts() / len(df.query('year==2018'))
'''
Electric Utility Services    0.026156
Gas Utility Services         0.006642
Heating Utilities            0.000164
Name: Borrower Industry, dtype: float64
'''

data['Borrower Type'].value_counts()
'''
Out[47]: 
Corporation (Other Company)    278
Corporation (Major Company)    124
Name: Borrower Type, dtype: int64
'''


data.to_excel(r'NPPU_GCARS_2018.xlsx')


#%% Oilg Gas

cols=[
 'Oil and Gas Extraction',
  'Liquid Propane Gas Distributor',
  'Gasoline Stations',
  'Natural Gas Stations',
  'Plumbing, Gas Facilities, Sewage Facilities Construction']

mask = df['Borrower Industry'].isin(cols)
data = df[mask].query('year==2018')


data['Borrower Industry'].value_counts()
'''
Out[52]: 
Oil and Gas Extraction                                      165
Gasoline Stations                                             8
Liquid Propane Gas Distributor                                8
Plumbing, Gas Facilities, Sewage Facilities Construction      5
Name: Borrower Industry, dtype: int64
'''


data['Borrower Industry'].value_counts() / len(df.query('year==2018'))
'''
Oil and Gas Extraction                                      0.013529
Gasoline Stations                                           0.000656
Liquid Propane Gas Distributor                              0.000656
Plumbing, Gas Facilities, Sewage Facilities Construction    0.000410
Name: Borrower Industry, dtype: float64
'''


data['Borrower Type'].value_counts() 
'''
Corporation (Other Company)    132
Corporation (Major Company)     54
Name: Borrower Type, dtype: int64
'''



data.to_excel(r'O&G_GCARS_2018.xlsx')
#%% Master Default
import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')


md = pd.read_excel(r'C:\Users\ub71894\Documents\Data\MasterDefault\Master_Def_202006_clean.xlsx')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered3.pkl')



df_md = md[['L_CIF_NUMBER', 'L_OBLIGOR_NAME', 'L_DATE_OF_DEFAULT', 'Scorecard']] 
df_md.dropna(subset=['L_CIF_NUMBER'], inplace=True)
#df_md['CIF'] = [str(int(x))[:8] for x in df_md.L_CIF_NUMBER]
#df_md['CIF'] = [str(int(x))[-8:] for x in df_md.L_CIF_NUMBER]
df_md['CIF'] = [str(int(x)) for x in df_md.L_CIF_NUMBER]
df_md.drop_duplicates(subset=['CIF'], inplace=True)


cols=['Borrower CIF',
 'Borrower Name',
 'Borrower Industry',
 'Borrower Country',
 'Borrower Type',]
df = dat[cols]
df.drop_duplicates(subset=['Borrower CIF'], inplace=True)
df['CIF'] = gcar_cif_cleaner(df, col='Borrower CIF')


new = pd.merge(df, df_md, on='CIF', how='inner')




#%% GCARS Default
import os, sys, pandas as pd, numpy as np
sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')

dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCAR_2008-2019_filtered3.pkl')
mask = dat['Applied Rating'].isin([83, 90, 101, 102])

dat['def_flag'] = 0
dat['def_flag'] = dat['def_flag'].mask(mask, 1)


dat['def_flag'].sum()
# 415
dat.query('def_flag==1')['Borrower Type'].value_counts()

'''
Corporation (Other Company)    316
Corporation (Major Company)     99
Name: Borrower Type, dtype: int64
'''
dat.query('def_flag==0')['Borrower Type'].value_counts()
'''
Corporation (Other Company)    95465
Corporation (Major Company)    24504
Name: Borrower Type, dtype: int64
'''



dat.query('def_flag==1')['Borrower CIF'].count()
# 415
dat.query('def_flag==1')['Borrower CIF'].nunique()
# 261

dat.query('def_flag==1')['year'].value_counts().sort_index()
'''
2008     1
2009    16
2010    79
2011    41
2012    35
2013    30
2014    25
2015    24
2016    50
2017    38
2018    34
2019    42
Name: year, dtype: int64
'''


df = dat.query('def_flag==1').drop_duplicates(subset=['Borrower CIF'])
df.query('def_flag==1')['Borrower Type'].value_counts()

'''
Corporation (Other Company)    197
Corporation (Major Company)     64
Name: Borrower Type, dtype: int64
'''




#%%  add BvD
df_bvd = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\CPPD\BvD.pickle')
bvd_cols=['Fixed assets',
 'Intangible fixed assets',
 'Tangible fixed assets',
 'Other fixed assets',
 'Current assets',
 'Stock',
 'Debtors',
 'Other current assets',
 'Cash & cash equivalent',
 'Total assets',
 'Shareholders funds',
 'Capital',
 'Other shareholders funds',
 'Non-current liabilities',
 'Long term debt',
 'Other non-current liabilities',
 'Provisions (Liabilities & Equity)',
 'Current liabilities',
 'Loans (Liabilities & Equity)',
 'Creditors',
 'Other current liabilities',
 'Total shareh. funds & liab.',
 'Working capital',
 'Net current assets',
 'Enterprise value',
 'Number of employees',
 'Operating revenue (Turnover)',
 'Sales',
 'Costs of goods sold',
 'Gross profit',
 'Other operating expenses',
 'Operating P/L [=EBIT]',
 'Financial P/L',
 'Financial revenue',
 'Financial expenses',
 'P/L before tax',
 'Taxation',
 'P/L after tax',
 'Extr. and other P/L',
 'Extr. and other revenue',
 'Extr. and other expenses',
 'P/L for period [=Net income] ',
 'Export revenue',
 'Material costs',
 'Costs of employees',
 'Depreciation & Amortization',
 'Other operating items',
 'Interest paid',
 'Research & Development expenses',
 'Cash flow',
 'Added value',
 'EBITDA']

gcar_converter(df_bvd, bvd_cols, inplace=True)
gcar_cif_cleaner(df_bvd, col='CIF', inplace=True)


dat['year-1'] = dat['year']-1
len(set(dat['Borrower CIF_cleaned'].unique()))
# 23849

len(set(df_bvd['CIF'].unique()))
# 10941

len(set(dat['Borrower CIF_cleaned'].unique())&set(df_bvd['CIF'].unique()))
# 10821



#%% build BvD proxy
df_bvd['Proxy_Total_Assets'] = df_bvd['Total assets']
df_bvd['Proxy_Interest_expense'] = df_bvd['Interest paid']
df_bvd['Proxy_Ending_Cash_Equiv'] = df_bvd['Cash & cash equivalent']
df_bvd['Proxy_UBEBITDA'] = df_bvd['EBITDA']
df_bvd['Proxy_Net_Sales'] = df_bvd['Sales']
df_bvd['Proxy_Total_Debt'] = df_bvd[ 'Long term debt'] + df_bvd['Loans (Liabilities & Equity)']
df_bvd['Proxy_Capitalization'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities']+df_bvd['Proxy_Total_Debt']
df_bvd['Proxy_Net_Op_Profit'] = df_bvd['Operating P/L [=EBIT]']
df_bvd['Proxy_Total_Liabilities'] = df_bvd['Current liabilities'] + df_bvd['Non-current liabilities']
df_bvd['Proxy_Tangible_Net_Worth'] = df_bvd['Total assets'] - df_bvd['Current liabilities'] - df_bvd['Non-current liabilities'] - df_bvd[ 'Intangible fixed assets']

df_bvd['Proxy_Total_Assets_inUSD'] = df_bvd['Proxy_Total_Assets'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Interest_expense_inUSD'] = df_bvd['Proxy_Interest_expense'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Ending_Cash_Equiv_inUSD'] = df_bvd['Proxy_Ending_Cash_Equiv'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_UBEBITDA_inUSD'] = df_bvd['Proxy_UBEBITDA'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Sales_inUSD'] = df_bvd['Proxy_Net_Sales'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Debt_inUSD'] = df_bvd['Proxy_Total_Debt'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Capitalization_inUSD'] = df_bvd['Proxy_Capitalization'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Net_Op_Profit_inUSD'] = df_bvd['Proxy_Net_Op_Profit'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Total_Liabilities_inUSD'] = df_bvd['Proxy_Total_Liabilities'] * df_bvd['Exchange rate from original currency']
df_bvd['Proxy_Tangible_Net_Worth_inUSD'] = df_bvd['Proxy_Tangible_Net_Worth'] * df_bvd['Exchange rate from original currency']

df_bvd.columns = 'BvD_' + df_bvd.columns.values

#%% relink: Year (N-1)'s BvD links to Year N's GCARS data
df_merged = pd.merge(dat, df_bvd, left_on=['Borrower CIF_cleaned','year-1'], right_on=['BvD_CIF','BvD_Fiscal year'], how='left')
df_merged['inBvD'] = ['No' if pd.isnull(x) else 'Yes' for x in df_merged['BvD_CIF']]
df_merged['inBvD'].value_counts()

'''
No     69458
Yes    54295
Name: inBvD, dtype: int64
'''



#%% add portfolio info
df_merged['portfolio'] = 'unknown'
df_merged['portfolio'] = df_merged['portfolio'].mask(df_merged['BvD_Proxy_Net_Sales_inUSD']>1e9, 'LC')
df_merged['portfolio'] = df_merged['portfolio'].mask(df_merged['BvD_Proxy_Net_Sales_inUSD']<=1e9, 'MM')

df_merged['portfolio'].value_counts()
'''
unknown    77417
MM         40106
LC          6230
Name: portfolio, dtype: int64

'''

#%% split
dat_LC = df_merged.query('portfolio=="LC"')
dat_MM = df_merged.query('portfolio=="MM"')


dat_LC['def_flag'].sum()
21
len(dat_LC)
6230

dat_LC.query('def_flag==1')['year'].value_counts().sort_index()
'''
2010    2
2011    1
2012    1
2014    2
2015    2
2016    4
2017    3
2018    3
2019    3
Name: year, dtype: int64

'''
dat_LC['year'].value_counts().sort_index()
'''
2008      4
2009     79
2010    459
2011    592
2012    606
2013    635
2014    699
2015    719
2016    722
2017    592
2018    626
2019    497
Name: year, dtype: int64
'''




dat_MM['def_flag'].sum()
101
len(dat_MM)
40106

dat_MM.query('def_flag==1')['year'].value_counts().sort_index()
'''
2010    10
2011     6
2012     9
2013    11
2014     9
2015     5
2016    23
2017    10
2018     9
2019     9
Name: year, dtype: int64
'''


dat_MM['year'].value_counts().sort_index()
'''
2008       1
2009     484
2010    2340
2011    2626
2012    3548
2013    3787
2014    4385
2015    5178
2016    4820
2017    5037
2018    4665
2019    3235
Name: year, dtype: int64
'''

5037+4665+3235# -*- coding: utf-8 -*-
"""
Created on Tue Nov  3 13:25:00 2020

@author: ub71894
"""
import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.Process import gcar_Americas_office_code, gcar_EMEA_office_code
from PDScorecardTool.Process import SomersD
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')

# 2020 data
list_file=[
r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCAR_BR_Approved\br download_2020Q1.tsv',
r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCAR_BR_Approved\br download_20200405.tsv',
r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCAR_BR_Approved\br download_202006.tsv']
dt0=pd.DataFrame()
for e in list_file:
    tmp=pd.read_csv(e, sep='\t', header=0,encoding = "ISO-8859-1")
    dt0=dt0.append(tmp,ignore_index=True)

dt0['timestamp'] = pd.to_datetime(dt0['Approval/Agreed Date'])
df = dt0.dropna(subset=['timestamp'])



#%% filter data
# model
mask = df['Borrower Type'].isin(['Corporation (Major Company)', 'Corporation (Other Company)'])
df = df[mask]

# Source
df_BL = df.loc[(df['Data From'].isin(['BL']))]
df_BL = df_BL[df_BL['Ringi/Sairyo']=='Sairyo'] 
df_CD = df.loc[(df['Data From'].isin(['CD']))]

df = pd.concat([df_CD, df_BL])
df = df.sort_values(by=['Data From'])
df = df.drop_duplicates(subset=['Application No.'], keep='last')

# Drop NA that exists in ratings
df = df.dropna(subset=['Applied Rating'], how='any')

# Drop duplicates
df['Borrower_CIF_cleaned'] = gcar_cif_cleaner(df, col='Borrower CIF')
df.drop_duplicates(subset=['Borrower_CIF_cleaned', 'timestamp'], inplace=True)



# tag year info
df['year'] = [int(x.year) for x in df['timestamp'] ]
df['year'].value_counts().sort_index() 
'''
2020    7447
Name: year, dtype: int64
'''




dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\GCARS 2008-2019_matches4.pkl')




cols=[
 'Borrower CIF',
 'Borrower_CIF_cleaned',
 'Borrower Name',
 'Borrower Country',
 'Borrower Office Code',
 'timestamp',
 'year',
 'Primary Evaluation',
 'Secondary Evaluation',
 'Third Evaluation',
 'Final Result of Evaluation based on Financial Substance Score',
 'Applied Rating',
 'Borrower Industry',
 'Borrower Type',
  'External Rating (S&P)',
  'Current Financial Data #1',
 'Current Financial Data #2',
 'Current Financial Data #3',
 'Current Financial Data #4',
 'Current Financial Data #5',
 'Current Financial Data #6',
 'Current Financial Data #7',
 'Current Financial Data #8',
 'Current Financial Data #9',
 'Current Financial Data #10',

]

# combine
df = pd.concat([dat[cols], df[cols]], axis=0)




# tag Geo info
df['Geo'] = 'Asia'
df['Geo'] = df['Geo'].mask(df['Borrower Office Code'].isin(gcar_Americas_office_code), 'Americas')
df['Geo'] = df['Geo'].mask(df['Borrower Office Code'].isin(gcar_EMEA_office_code), 'EMEA')
df['Geo'].value_counts()

'''
Asia        74148
EMEA        27010
Americas    26673
Name: Geo, dtype: int64
'''

# tag Guarantor info
df['Gua_override'] = 1
mask = df['Secondary Evaluation']==df['Third Evaluation']
df.loc[mask,'Gua_override']=0
df['Gua_override'].value_counts()
'''
0    75948
1    51883
Name: Gua_override, dtype: int64
'''

# replace Final Evaluation as Secondary for all Guarantor impacted records
df['BTMU_Rating'] = df['Applied Rating'].mask(df['Gua_override']==1, df['Secondary Evaluation'])
# get string version
df['BTMU_Rating_str'] = df['BTMU_Rating'].transform(lambda x: str(x)[:-1]+'-'+str(x)[-1])
#df['BTMU_Rating_str'].value_counts().sort_index() 

dff = df.query('year>=2017')
dff.to_pickle(r'..\data\__investigation_GCARS_stability_2017-2020.pkl')



import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\Data\GCARS 2008-2019_matches.pkl')



df = dat[dat['Borrower Type'] == 'Corporation (Other Company)']

df_1 = df.query('Match==1')
df_0 = df.query('Match==0')

cols=[
'Borrower CIF',
'Borrower Name',
'Borrower Country',
'Borrower Type',
'Borrower Office Code',
'Borrower Office Name',
'timestamp',
'year',
'Current Total Score',
'Primary Evaluation',
'BTMU Rating',
'Applied Rating',
'Applied Rating PDRR',
'Match'
]

df_1.year.value_counts(normalize=True).sort_index()
df_0.year.value_counts(normalize=True).sort_index()


df.groupby('year')['Match'].mean
'''
year
2008    0.084746
2009    0.043767
2010    0.049397
2011    0.042160
2012    0.062644
2013    0.342165
2014    0.362398
2015    0.400951
2016    0.438614
2017    0.636697
2018    0.597648
2019    0.585712
Name: Match, dtype: float64
'''








def check_diff(col):
    print(f'Non-machted sample, {col}: # unique = {df_0[col].nunique()}')
    print(f'    Machted sample, {col}: # unique = {df_1[col].nunique()}')


def diff_ratio(col):

    return (df_0[col].nunique()+1)/(1+df_1[col].nunique())

N = len(df)
list_col=[]

for i,col in enumerate(list(df)):
    rr = df[col].count()/N
    if rr>=0.25:
        list_col.append(col)



list_ratio=[]
for col in list_col:
    list_ratio.append(diff_ratio(col))


result = pd.DataFrame()
result['ratio'] = list_ratio
result['field']= list_col




check_diff('(MUB) Primary Evaluation from RA')

check_diff('Modification Corporate Bond')


col = 'Format'
check_diff(col)

df[col].count()


col = 'Borrower Situation'
check_diff(col)

df[col].count()



list_col=[
 '(MUB) (Primary Evaluation from RA) Archive ID',
 '(MUB) (Primary Evaluation from RA) Captured Time (PST)',
 '(MUB) (Primary Evaluation from RA) Current User',
 '(MUB) (Primary Evaluation from RA) Customer Name',
 '(MUB) (Primary Evaluation from RA) Imported Time (JST)',
 '(MUB) (Primary Evaluation from RA) MUFG Bank Primary Rating',
 '(MUB) (Primary Evaluation from RA) RCIF',
 '(MUB) (Primary Evaluation from RA) Statement Date',
 '(MUB) Archive ID',
 '(MUB) Borrower Grade',
 '(MUB) MUFG Bank Rating',
 '(MUB) PDRR',
 '(MUB) Primary Evaluation from RA',
 '(MUB) RCIF']


N = len(df)
for i,col in enumerate(list_col):
    print((df[col].count()/N))



df['(MUB) (Primary Evaluation from RA) BTMU Primary Rating'].count()



df_0[col].value_counts()
Out[68]: 
MUFG Bank    51517
Name: Format, dtype: int64

df_1[col].value_counts()
Out[69]: 
MUFG Bank    25983
MUB            539
Name: Format, dtype: int64# -*- coding: utf-8 -*-
"""
Created on Tue Sep  8 10:40:51 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')




#%% LC
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')


#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    1038
1     379
Name: Gua_override, dtype: int64
'''

dat = dat.query('Gua_override==0').reset_index(drop=True)


#%% locate outliers
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])


#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['LC_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['LC_PD'] = list_PD
    dat['LC_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_LC_norm = get_proxy_rating(dat_LC_norm, model_LC)

#%%
df = dat_LC_norm[dat_LC_norm['Borrower Type']=='Corporation (Other Company)'].copy()
df.dropna(subset=['Applied Rating','Primary Evaluation'], how='any', inplace=True)

df['diff'] = np.abs(df['LC_PDRR'] -  df['Applied Rating PDRR'])
df = df.drop_duplicates(subset=['Borrower_CIF_cleaned', 'diff'])
df['diff'].value_counts().sort_index()


df2 = df.query('diff<=1').sample(5).sort_values('Borrower Name')
df3 = df.query('diff>4').sample(5).sort_values('Borrower Name')
df_LC = pd.concat([df2,df3], axis=0)

df_LC['Borrower Name'].nunique()
# 10



#%% MM
dat = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')


#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()



dat = dat.query('Gua_override==0').reset_index(drop=True)


#%% locate outliers
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])

df_temp = dat[['Applied Rating PDRR', 'PrimaryEvaluationPDRR']].query('PrimaryEvaluationPDRR!=999')
df_temp['diff'] = np.abs(df_temp['Applied Rating PDRR']-df_temp['PrimaryEvaluationPDRR'])





#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['MM_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['MM_PD'] = list_PD
    dat['MM_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_MM_norm = get_proxy_rating(dat_MM_norm, model_MM)



df = dat_MM_norm[dat_MM_norm['Borrower Type']=='Corporation (Other Company)'].copy()
df.dropna(subset=['Applied Rating','Primary Evaluation'], how='any', inplace=True)

df['diff'] = np.abs(df['MM_PDRR'] -  df['Applied Rating PDRR'])
df = df.drop_duplicates(subset=['Borrower_CIF_cleaned', 'diff'])
df['diff'].value_counts().sort_index()


df4 = df.query('diff<=1').sample(5).sort_values('Borrower Name')
df5 = df.query('diff>7').sample(5).sort_values('Borrower Name')
df_MM = pd.concat([df4,df5], axis=0)

df_MM['Borrower Name'].nunique()
# 9


writer = pd.ExcelWriter(r'SFAMFA\sample_forGCARS_reports_2017-2019.xlsx')

df_LC.to_excel(writer, 'LC model')
df_MM.to_excel(writer, 'MM model')
writer.save()# -*- coding: utf-8 -*-
"""
Created on Tue Sep  8 10:40:51 2020

@author: ub71894
"""

import os, sys, pandas as pd, numpy as np

sys.path.append(r'C:\Users\ub71894\Documents\DevRepo')
os.chdir(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src')
from varlib import list_factor
from PDScorecardTool.Process import SomersD, normalization, google_color
from PDScorecardTool.Process import gcar_converter, gcar_cif_cleaner
from PDScorecardTool.CreateBenchmarkMatrix import CreateBenchmarkMatrix, TMstats
import pickle
from newfunc import cluster_corr, plot_cluster_corr
import seaborn as sns
import matplotlib.pyplot as plt
from itertools import combinations, product
import statsmodels.api as sm
from sklearn.isotonic import IsotonicRegression


model_MM = pickle.load(open(r'C:\Users\ub71894\Documents\DevRepo\Files\model_MM.pkl','rb'))
model_LC = pickle.load(open(r'C:\Users\ub71894\Documents\Projects\CNI\src\UBEBITDA\model_UBEBITDA_af.pkl','rb'))
model_LC.quant_factor = ['LC_UBEBITDA_to_NS','LC_TD_to_UBEBITDA','LC_Total Assets','LC_TD_to_Capt','LC_UBEBITDA_to_IE']
model_MM.quant_factor = ['MM_NOP_to_NS','MM_TD_to_UBEBITDA','MM_TD_to_Capt','MM_ECE_to_TL','MM_TangNW_to_TA']
lmdb = 0.3
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')



#%% SP
df_sp_raw = pd.read_csv(r'C:\Users\ub71894\Documents\Data\Compustat Pull 082520\S&P Rating Mapping Data 2019-09-24.csv')
data_gcar1 = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\yiming\name matching.xlsx')
data_gcar2 = pd.read_excel(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\src\yiming\name matching 2.xlsx')
data_gcar = pd.concat([data_gcar1 ,data_gcar2], axis=0)


data_gcar = data_gcar.query('correct==1')
data_gcar['CONM'] = data_gcar['diffmatchname_90'].mask(
    pd.isnull(data_gcar['diffmatchname_90']),
    data_gcar['diffmatchname_80'])

df_mapping = data_gcar[[ 'name_tokyo', 'CONM']].drop_duplicates(subset=['name_tokyo']).reset_index(drop=True)


df_sp = pd.merge(df_mapping, df_sp_raw, on='CONM', how='left')
df_sp.sort_values(by='ratingDate', inplace=True)
df_sp = df_sp.query('ratingSymbol!="NR"')
dict_sp={
'AAA':1,
'AA+':2,
'AA':3,
'AA-':4,
'A+':5,
'A':6,
'A-':7,
'BBB+':8, 
'BBB':9,     
'BBB-':10,        
'BB+':11,   
'BB':12,      
'BB-':13,     
'B+':14,   
'B':15,   
'B-':16,
'CCC':17, 
'D':18,
'SD':18,
}  
    
df_sp['rating_in_notch'] = df_sp['ratingSymbol'].replace(dict_sp)
df_sp = df_sp[['name_tokyo','CONM','ratingDate','ratingSymbol','rating_in_notch']]
df_sp['timestamp'] = pd.to_datetime(df_sp['ratingDate'])
df_sp['year'] = [x.year for x in df_sp.timestamp]
df_sp.drop_duplicates(subset=['name_tokyo','year'], inplace=True)



#%% LC
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_LC_ready_relink.pkl')
dat = pd.merge(df, df_sp, left_on=['Borrower Name','year'], right_on=['name_tokyo','year'], how='inner')

#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()

'''
0    46
1     4
Name: Gua_override, dtype: int64
'''

dat = dat.query('Gua_override==0').reset_index(drop=True)


#%% locate outliers
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])


#%% keep original LC model
dat_LC = dat.dropna(subset=model_LC.quant_factor)
dat_LC_norm = normalization(dat_LC, model_LC, quant_only=True, missing='median')
dat_LC_norm['LC_quant_score'] = (model_LC.quant_weight*dat_LC_norm[model_LC.quant_factor]).sum(axis=1)
dat_LC_norm['LC_quant_score'] = 50*(dat_LC_norm['LC_quant_score']-model_LC.quantmean) / model_LC.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['LC_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['LC_PD'] = list_PD
    dat['LC_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_LC_norm = get_proxy_rating(dat_LC_norm, model_LC)
# 37

print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['LC_PDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['PrimaryEvaluationPDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Applied Rating PDRR']))
print(SomersD(dat_LC_norm['rating_in_notch'], dat_LC_norm['Applied Rating']))
-0.05982905982905983
0.4551282051282051
0.6303418803418803
0.6025641025641025


dat_LC_norm['Borrower Type'].value_counts()
'''

Corporation (Major Company)    32
Corporation (Other Company)     2
Name: Borrower Type, dtype: int64
'''

#%%


#%% MM
df = pd.read_pickle(r'C:\Users\ub71894\Documents\Projects\GlobalCorporation\data\SFAMFA_MM_ready_relink.pkl')
dat = pd.merge(df, df_sp, left_on=['Borrower Name','year'], right_on=['name_tokyo','year'], how='inner')

#%% remove Guarantor
dat['Secondary Evaluation'].fillna(999, inplace=True)
dat['Third Evaluation'].fillna(999, inplace=True)

dat['Gua_override'] = 1
mask = dat['Secondary Evaluation']==dat['Third Evaluation']
dat.loc[mask,'Gua_override']=0
dat['Gua_override'].value_counts()



dat = dat.query('Gua_override==0').reset_index(drop=True)


#%% locate outliers
MS = pd.read_excel(r'C:\Users\ub71894\Documents\DevRepo\Files\MasterScale_withTokyoRating.xlsx')
pd_mapping = dict(zip(MS.BTMU, MS.PDRR))
pd_mapping.update({999:999})

dat['PrimaryEvaluationPDRR'] = dat['Primary Evaluation'].fillna(999)
dat['PrimaryEvaluationPDRR'].replace({23:22,80:81}, inplace=True)
dat['PrimaryEvaluationPDRR'] = dat['PrimaryEvaluationPDRR'].transform(lambda x: pd_mapping[x])


#%% keep original MM model
dat_MM = dat.dropna(subset=model_MM.quant_factor)
dat_MM_norm = normalization(dat_MM, model_MM, quant_only=True, missing='median')
dat_MM_norm['MM_quant_score'] = (model_MM.quant_weight*dat_MM_norm[model_MM.quant_factor]).sum(axis=1)
dat_MM_norm['MM_quant_score'] = 50*(dat_MM_norm['MM_quant_score']-model_MM.quantmean) / model_MM.quantstd

def get_proxy_rating(dat, model):

    low_pd = MS['new_low']
    list_PD = []
    list_Ratings_PDRR = []
    list_Ratings_Tokyo = []

    for obs in dat.iterrows():
    
        logitPD = model.intercept1 + model.slope1*obs[1]['MM_quant_score']
        PD = np.exp(logitPD)/(1+np.exp(logitPD))
        rating_PDRR = sum(low_pd<=(PD))
        if rating_PDRR ==1:
            rating_PDRR=2
    
        list_PD.append(PD)
        list_Ratings_PDRR.append(rating_PDRR)

    
    dat['MM_PD'] = list_PD
    dat['MM_PDRR'] = list_Ratings_PDRR

    return (dat)

dat_MM_norm = get_proxy_rating(dat_MM_norm, model_MM)
# 8


print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['MM_PDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['PrimaryEvaluationPDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Applied Rating PDRR']))
print(SomersD(dat_MM_norm['rating_in_notch'], dat_MM_norm['Applied Rating']))
0.5789473684210527
0.5789473684210527
1.0
1.0



dat_MM_norm['Borrower Type'].value_counts()
'''
Corporation (Other Company)    4
Corporation (Major Company)    3
Name: Borrower Type, dtype: int64

'''
['01_prepare_data.py', '02_analysis.py', '03_withCompuStat.py', '03_withCompuStat_SPmodel.py', '04_SNL_prepare.py', '05_checkdata.py', '06_check2019data.py', '06_check2019data_updated_BL.py', '07_filterandcreateAmNonAm.py', '07_filterandcreateAmNonAm_dropsomerating.py', '08_SFA_archived_BvD_preparedata.py', '08_SFA_archived_BvD_somersd.py', '08_SFA_archived_BvD_somersd_quant.py', '09_EXP_archived_BvD_buildratio.py', '09_EXP_archived_BvD_MM_AM+EU_SFAMFA.py', '09_EXP_archived_BvD_MM_am_SFAMFA.py', '09_EXP_archived_BvD_MM_nam_SFAMFA.py', '10_SFAMFA_01_preparedata.py', '10_SFAMFA_02_merge_with_BVD.py', '10_SFAMFA_03_buildratio.py', '10_SFAMFA_04_LC _noGua.py', '10_SFAMFA_04_LC _noGua_nooutliers.py', '10_SFAMFA_04_LC.py', '10_SFAMFA_05_MM.py', '11_relink_01_preparedata.py', '11_relink_02_merge_with_BVD.py', '11_relink_03_buildratio.py', '11_relink_04_CPPD_LC.py', '11_relink_04_CPPD_LC_original.py', '11_relink_04_CPPD_MM.py', '11_relink_04_CPPD_MM_original.py', '11_relink_archived_04_LC_noGua_nooutliers.py', '11_relink_archived_04_LC_noGua_nooutliers_exp.py', '11_relink_archived_04_MM_noGua_nooutliers.py', 'combo.py', 'construct_cty_risk_label.py', 'CPPD_07_filtering.py', 'CPPD_08_SFA_BvD_preparedata.py', 'CPPD_08_SFA_Major.py', 'CPPD_08_SFA_Major_dropped.py', 'CPPD_08_SFA_Major_test.py', 'CPPD_08_SFA_Other.py', 'CPPD_08_SFA_Other_dropped.py', 'CPPD_08_SFA_Other_test.py', 'CPPD_09_MFA_Other.py', 'newfunc.py', 'prdrvw_01_othercorp.py', 'varlib.py', '__investigation_Benford.py', '__investigation_BvDmissImportant.py', '__investigation_BvDmissImportant2.py', '__investigation_CBM.py', '__investigation_checkbreakpoint_GCARS.py', '__investigation_CPPD_O&G_NPPU.py', '__investigation_default.py', '__investigation_GCARS_stability.py', '__investigation_othercorp_replication.py', '__investigation_sample.py', '__investigation_SP.py']
[232, 341, 418, 496, 854, 952, 1069, 1148, 1299, 1465, 1688, 1794, 2106, 2358, 2657, 3000, 3345, 3474, 3648, 4064, 4856, 5600, 6384, 7162, 7316, 7540, 7844, 8484, 8807, 9401, 9740, 10505, 10916, 11682, 29332, 29452, 29591, 29736, 29986, 30234, 30464, 30700, 30936, 31168, 31783, 31880, 32024, 32167, 32476, 32927, 33732, 34067, 34132, 34257, 34564, 34700, 34846, 35052, 35298]
